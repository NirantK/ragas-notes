{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19011569-f522-4ece-a8f6-e5e046d9dda9",
   "metadata": {},
   "source": [
    "## Simple RAG playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50362c0c-0dd1-4583-be57-6b8a141ca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995bf9b-79c2-4cef-850b-e463b538f04a",
   "metadata": {},
   "source": [
    "## Customer support Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61523c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "NOTION_TOKEN = os.environ.get(\"NOTION_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976fb7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ragas_annotator 0.0.1\n",
      "Uninstalling ragas_annotator-0.0.1:\n",
      "  Successfully uninstalled ragas_annotator-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y ragas_annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac26250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ragas_annotator'...\n",
      "remote: Enumerating objects: 650, done.\u001b[K\n",
      "remote: Counting objects: 100% (650/650), done.\u001b[K\n",
      "remote: Compressing objects: 100% (398/398), done.\u001b[K\n",
      "remote: Total 650 (delta 385), reused 487 (delta 234), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (650/650), 662.23 KiB | 4.22 MiB/s, done.\n",
      "Resolving deltas: 100% (385/385), done.\n",
      "Obtaining file:///Users/nirantk/Desktop/scratchpad/ragas/notes/rag-playground/ragas_annotator\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: notion-client in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.3.0)\n",
      "Requirement already satisfied: fastcore in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (1.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (4.67.1)\n",
      "Requirement already satisfied: langfuse in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.60.2)\n",
      "Requirement already satisfied: instructor in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (1.7.9)\n",
      "Requirement already satisfied: pydantic in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.11.2)\n",
      "Requirement already satisfied: numpy in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.2.4)\n",
      "Requirement already satisfied: packaging in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from fastcore->ragas_annotator==0.0.1) (24.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (3.11.16)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (0.16)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (3.1.6)\n",
      "Requirement already satisfied: jiter<0.9,>=0.6.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (0.8.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (1.70.0)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (2.33.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (9.1.2)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (0.15.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from pydantic->ragas_annotator==0.0.1) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from pydantic->ragas_annotator==0.0.1) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from pydantic->ragas_annotator==0.0.1) (0.4.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (4.9.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (0.28.1)\n",
      "Requirement already satisfied: idna<4.0,>=3.7 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (3.10)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (1.19.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from anyio<5.0.0,>=4.4.0->langfuse->ragas_annotator==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse->ragas_annotator==0.0.1) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse->ragas_annotator==0.0.1) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse->ragas_annotator==0.0.1) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas_annotator==0.0.1) (3.0.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor->ragas_annotator==0.0.1) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor->ragas_annotator==0.0.1) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor->ragas_annotator==0.0.1) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor->ragas_annotator==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor->ragas_annotator==0.0.1) (2.19.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.9.0->instructor->ragas_annotator==0.0.1) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.9.0->instructor->ragas_annotator==0.0.1) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->ragas_annotator==0.0.1) (0.1.2)\n",
      "Building wheels for collected packages: ragas_annotator\n",
      "  Building editable for ragas_annotator (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ragas_annotator: filename=ragas_annotator-0.0.1-0.editable-py3-none-any.whl size=7944 sha256=26e3bd9d16b78b5845fb93d56a3883b98b28f0c3da0cb6a1d9630b0bdf2aa40d\n",
      "  Stored in directory: /private/var/folders/dv/m1j20ybn5kz23kskkjpn55cc0000gn/T/pip-ephem-wheel-cache-st_s19px/wheels/74/0a/da/4a8589492f7db2685d43fd42109de51d7561d4a4fa497bed4a\n",
      "Successfully built ragas_annotator\n",
      "Installing collected packages: ragas_annotator\n",
      "Successfully installed ragas_annotator-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# Install ragas_annotator from source\n",
    "!git clone https://github.com/explodinggradients/ragas_annotator\n",
    "!cd ragas_annotator && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f825c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ragas-airline-dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad580dd-4911-4e75-8d9f-77ccd7578da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import AgentAI\n",
    "\n",
    "customer_support_agent = AgentAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d4aaf0-ee19-42e9-b8c0-cb8124f86654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you missed your flight, whether you can get a refund depends on the ticket type you purchased:\n",
      "\n",
      "1. **Refundable Ticket**: You may be eligible for a **full refund**.\n",
      "2. **Non-Refundable Ticket**: You may receive a **partial refund** or travel credit, but cancellation fees will apply.\n",
      "3. **Basic Economy & Promo Fares**: Typically, these tickets are **not refundable**.\n",
      "\n",
      "For more detailed information, you should contact Ragas Airlines customer support or check your ticket conditions in \"Manage My Booking\".\n"
     ]
    }
   ],
   "source": [
    "response = await customer_support_agent.ask(\"Can i get a refund for my missed flight?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4efdf-9068-44c6-9897-07bd95259c23",
   "metadata": {},
   "source": [
    "## Setup sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f70f1a17-688e-4cd0-b449-1400076658a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.project.core import Project\n",
    "from ragas_annotator.project.experiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e41474dc-cc11-4eb1-9473-6686857bc14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(name='Customer support RAG', root_page_id=1b35d9bf94ff801792bfd1824fac0c96)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = Project(\n",
    "    name=\"Customer support RAG\",\n",
    "    notion_api_key=NOTION_TOKEN,\n",
    "    notion_root_page_id=PROJECT_ID,\n",
    ")\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762ea33-4d54-477b-8920-138cc0ec8444",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d26d45b5-697b-47b5-b65e-a82a69c9d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.model.notion_model import NotionModel\n",
    "from ragas_annotator.model import notion_typing as nmt\n",
    "\n",
    "\n",
    "class Dataset(NotionModel):\n",
    "    id: str = nmt.ID()\n",
    "    query: str = nmt.Title()\n",
    "    expected_answer: str = nmt.Text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9989e56e-86d5-44a9-82d4-904977c75e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project.get_dataset(\n",
    "    name=\"RAG Dataset\",\n",
    "    model=Dataset,\n",
    ")\n",
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "374b3b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_create_project_structure',\n",
       " '_notion_backend',\n",
       " 'comparisons_page_id',\n",
       " 'create_dataset',\n",
       " 'create_experiment',\n",
       " 'datasets_page_id',\n",
       " 'experiment',\n",
       " 'experiments_page_id',\n",
       " 'get_dataset',\n",
       " 'get_experiment',\n",
       " 'initialize',\n",
       " 'langfuse_experiment',\n",
       " 'name']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52897619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Project.langfuse_experiment of Project(name='Customer support RAG', root_page_id=1b35d9bf94ff801792bfd1824fac0c96)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.langfuse_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "193d8366-b527-42ee-96d3-b51f70b59060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9163c2-bbd2-4425-a683-d5b5d67b76c6",
   "metadata": {},
   "source": [
    "## LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f78b886-5f87-4428-959d-966c7773b919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fail'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas_annotator.llm import ragas_llm\n",
    "from ragas_annotator.metric import DiscreteMetric\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\", model=\"gpt-4o\", client=AsyncOpenAI())\n",
    "\n",
    "my_metric = DiscreteMetric(\n",
    "    llm=llm,\n",
    "    name=\"correctness\",\n",
    "    prompt=\"Evaluate if given answer {response} is same as expected answer {expected_answer}\",\n",
    "    values=[\"pass\", \"fail\"],\n",
    ")\n",
    "\n",
    "\n",
    "# test LLM as judge\n",
    "result = my_metric.score(\n",
    "    response=\"this is my response\", expected_answer=\"this is not my response\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c45c3e-dd63-43eb-a12c-df08368db561",
   "metadata": {},
   "source": [
    "### Writing custom logic with metric (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "513c8d03-d244-48d8-b638-f8e99aa399f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "# @discrete_metric(llm=llm,\n",
    "#     prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "#     name='new_metric',\n",
    "#     values=[\"low\",\"med\",\"high\"]\n",
    "#     )\n",
    "# def my_metric(llm,prompt,example_store, **kwargs):\n",
    "\n",
    "#         class response_model(BaseModel):\n",
    "#              output: t.List[bool]\n",
    "#              reason: str\n",
    "\n",
    "#         response = llm.generate(\n",
    "# \t        prompt.format(**kwargs),response_model=response_model\n",
    "# \t      )\n",
    "#         total = sum(response.output)\n",
    "#         if total < 1:\n",
    "#             score = 'low'\n",
    "#         else:\n",
    "#             score = 'high'\n",
    "#         return score,\"reason\",\n",
    "\n",
    "# result = my_metric.score(response='my response') # result\n",
    "# print(result)\n",
    "# print(result.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244f410-0292-4384-bce8-bb0358457e40",
   "metadata": {},
   "source": [
    "## Setup an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae42189d-17e5-4dec-8e81-9e1ca53399e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "\n",
    "class Experiment(Dataset):\n",
    "    response: str = nmt.Text()\n",
    "    correctness: t.Literal[\"pass\", \"fail\"] = nmt.Select()\n",
    "    correctness_reason: str = nmt.Text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "146f2227-82b5-44b5-aaf4-4f8b35917002",
   "metadata": {},
   "outputs": [],
   "source": [
    "@project.langfuse_experiment(Experiment, name_prefix=\"Workshop\")\n",
    "async def run_experiment(row: Dataset):\n",
    "    response = await customer_support_agent.ask(row.query)\n",
    "    score = await my_metric.ascore(\n",
    "        response=response, expected_answer=row.expected_answer\n",
    "    )\n",
    "\n",
    "    experiment_view = Experiment(\n",
    "        id=row.id,\n",
    "        query=row.query,\n",
    "        expected_answer=row.expected_answer,\n",
    "        response=response,\n",
    "        correctness=score.result,\n",
    "        correctness_reason=score.reason,\n",
    "    )\n",
    "\n",
    "    return experiment_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c334a",
   "metadata": {},
   "source": [
    "## Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55375f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"setting-up-ragas-annotator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426d7b6-28b3-49ce-92c5-74e468c51165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:09<00:00,  1.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(name=setting-up-ragas-annotator, model=Experiment)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_experiment.run_async(name=experiment_name, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69be73d",
   "metadata": {},
   "source": [
    "You may make any changes to AgentAI class like prompt, model, etc and run any number of experiments. Experiment now would have recorded in the Notion UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da891c-bf76-46c0-abe2-0c17ad638d74",
   "metadata": {},
   "source": [
    "### Train LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f36a1891-fea5-4d36-9fc2-28e0aeef67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.embedding import ragas_embedding\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "embedding = ragas_embedding(\n",
    "    provider=\"openai\", client=OpenAI(), model=\"text-embedding-3-small\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55e1a9bb-c4ec-455f-aa85-758948fcba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 15/15 [00:00<00:00, 145635.56it/s]\n"
     ]
    }
   ],
   "source": [
    "my_metric.train(\n",
    "    project,\n",
    "    experiment_names=[experiment_name],\n",
    "    embedding_model=embedding,\n",
    "    model=Experiment,\n",
    "    method={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6961e10-66c8-4413-9f92-c664275074d8",
   "metadata": {},
   "source": [
    "### Compare experiments\n",
    "Hack to do in notebook, will be done in the UI once we have the UI ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8f44512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83833d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_plot(exp_x: str, exp_y: str, metric):\n",
    "    # Load experiments\n",
    "    exp_x_data = project.get_experiment(exp_x, Experiment)\n",
    "    exp_y_data = project.get_experiment(exp_y, Experiment)\n",
    "    exp_x_data.load()\n",
    "    exp_y_data.load()\n",
    "\n",
    "    # Compare experiments (assuming this is a function that exists)\n",
    "    project.compare_experiments(exp_x_data, exp_y_data)\n",
    "\n",
    "    # Extract metrics from both experiments\n",
    "    results = {\n",
    "        \"exp_x\": [],\n",
    "        \"exp_y\": [],\n",
    "    }\n",
    "    for i in range(len(exp_x_data)):\n",
    "        results[\"exp_x\"].append(getattr(exp_x_data[i], metric.name))\n",
    "        results[\"exp_y\"].append(getattr(exp_y_data[i], metric.name))\n",
    "\n",
    "    # Calculate counts for each category\n",
    "    exp_x_counts = {}\n",
    "    exp_y_counts = {}\n",
    "\n",
    "    # For categorical data like 'pass'/'fail' or 'good'/'okay'/'bad'\n",
    "    # Get unique categories\n",
    "    all_categories = set(results[\"exp_x\"] + results[\"exp_y\"])\n",
    "\n",
    "    # Count occurrences of each category\n",
    "    for category in all_categories:\n",
    "        exp_x_counts[category] = results[\"exp_x\"].count(category)\n",
    "        exp_y_counts[category] = results[\"exp_y\"].count(category)\n",
    "\n",
    "    # Create stacked bar chart\n",
    "\n",
    "\n",
    "    # Set up colors based on categories\n",
    "    if all(cat in [\"pass\", \"fail\"] for cat in all_categories):\n",
    "        colors = {\"pass\": \"#2196F3\", \"fail\": \"#FF5722\"}\n",
    "    elif all(cat in [\"good\", \"okay\", \"bad\"] for cat in all_categories):\n",
    "        colors = {\"good\": \"#4CAF50\", \"okay\": \"#FFC107\", \"bad\": \"#F44336\"}\n",
    "    else:\n",
    "        # Generate colors if categories are unknown\n",
    "        import matplotlib.colors as mcolors\n",
    "\n",
    "        colors = {\n",
    "            cat: list(mcolors.TABLEAU_COLORS.values())[i % len(mcolors.TABLEAU_COLORS)]\n",
    "            for i, cat in enumerate(all_categories)\n",
    "        }\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot stacked bars\n",
    "    experiments = [exp_x, exp_y]\n",
    "    exp_counts = [exp_x_counts, exp_y_counts]\n",
    "\n",
    "    # Calculate totals for percentage\n",
    "    totals = [sum(counts.values()) for counts in exp_counts]\n",
    "\n",
    "    # Sort categories for consistent stacking (e.g., 'pass' always at bottom, then 'fail')\n",
    "    sorted_categories = sorted(all_categories)\n",
    "\n",
    "    # Plot each category as a segment in the stack\n",
    "    bottoms = np.zeros(len(experiments))\n",
    "    for category in sorted_categories:\n",
    "        values = [\n",
    "            counts.get(category, 0) / total * 100\n",
    "            for counts, total in zip(exp_counts, totals)\n",
    "        ]\n",
    "        ax.bar(\n",
    "            experiments,\n",
    "            values,\n",
    "            bottom=bottoms,\n",
    "            label=category.capitalize(),\n",
    "            color=colors[category],\n",
    "        )\n",
    "\n",
    "        # Add text labels inside the bars\n",
    "        for i, v in enumerate(values):\n",
    "            if v > 5:  # Only add label if segment is large enough\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    bottoms[i] + v / 2,\n",
    "                    f\"{int(exp_counts[i].get(category, 0))}\\n({v:.1f}%)\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "        bottoms += values\n",
    "\n",
    "    # Customize the chart\n",
    "    ax.set_title(\n",
    "        f\"Comparison of {metric.name.capitalize()} between Experiments\", fontsize=14\n",
    "    )\n",
    "    ax.set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.legend(title=metric.name.capitalize())\n",
    "\n",
    "    # Add totals on top of each bar\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(i, 101, f\"Total: {total}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f\"comparison_{metric.name}.png\")\n",
    "\n",
    "compare_and_plot(exp_x=\"setting-up-ragas-annotator\", exp_y=\"setting-up-ragas-annotator\", metric=my_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d4a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
