{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19011569-f522-4ece-a8f6-e5e046d9dda9",
   "metadata": {},
   "source": [
    "## Simple RAG playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50362c0c-0dd1-4583-be57-6b8a141ca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995bf9b-79c2-4cef-850b-e463b538f04a",
   "metadata": {},
   "source": [
    "## Customer support Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61523c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "NOTION_TOKEN = os.environ.get(\"NOTION_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976fb7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ragas_annotator 0.0.1\n",
      "Uninstalling ragas_annotator-0.0.1:\n",
      "  Successfully uninstalled ragas_annotator-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y ragas_annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac26250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ragas_annotator' already exists and is not an empty directory.\n",
      "Obtaining file:///Users/nirantk/Desktop/scratchpad/ragas/notes/rag-playground/ragas_annotator\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: notion-client in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.3.0)\n",
      "Requirement already satisfied: fastcore in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (1.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (4.67.1)\n",
      "Requirement already satisfied: langfuse in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.60.2)\n",
      "Requirement already satisfied: instructor in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (1.7.9)\n",
      "Requirement already satisfied: pydantic in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.11.2)\n",
      "Requirement already satisfied: numpy in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from ragas_annotator==0.0.1) (2.2.4)\n",
      "Requirement already satisfied: packaging in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from fastcore->ragas_annotator==0.0.1) (24.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (3.11.16)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (0.16)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (3.1.6)\n",
      "Requirement already satisfied: jiter<0.9,>=0.6.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (0.8.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (1.70.0)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (2.33.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (9.1.2)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from instructor->ragas_annotator==0.0.1) (0.15.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from pydantic->ragas_annotator==0.0.1) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from pydantic->ragas_annotator==0.0.1) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from pydantic->ragas_annotator==0.0.1) (0.4.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (4.9.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (0.28.1)\n",
      "Requirement already satisfied: idna<4.0,>=3.7 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (3.10)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from langfuse->ragas_annotator==0.0.1) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas_annotator==0.0.1) (1.19.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from anyio<5.0.0,>=4.4.0->langfuse->ragas_annotator==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse->ragas_annotator==0.0.1) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse->ragas_annotator==0.0.1) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse->ragas_annotator==0.0.1) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas_annotator==0.0.1) (3.0.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor->ragas_annotator==0.0.1) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor->ragas_annotator==0.0.1) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor->ragas_annotator==0.0.1) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor->ragas_annotator==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor->ragas_annotator==0.0.1) (2.19.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.9.0->instructor->ragas_annotator==0.0.1) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.9.0->instructor->ragas_annotator==0.0.1) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nirantk/Desktop/scratchpad/ragas/notes/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->ragas_annotator==0.0.1) (0.1.2)\n",
      "Building wheels for collected packages: ragas_annotator\n",
      "  Building editable for ragas_annotator (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ragas_annotator: filename=ragas_annotator-0.0.1-0.editable-py3-none-any.whl size=7944 sha256=2c27b8d96e5428a25fa3d43799b8be3893ff2730e3a37df6be2d58c2f7a04d54\n",
      "  Stored in directory: /private/var/folders/dv/m1j20ybn5kz23kskkjpn55cc0000gn/T/pip-ephem-wheel-cache-y7mqx24m/wheels/74/0a/da/4a8589492f7db2685d43fd42109de51d7561d4a4fa497bed4a\n",
      "Successfully built ragas_annotator\n",
      "Installing collected packages: ragas_annotator\n",
      "Successfully installed ragas_annotator-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# Install ragas_annotator from source\n",
    "!git clone https://github.com/explodinggradients/ragas_annotator\n",
    "!cd ragas_annotator && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f825c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ragas-airline-dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad580dd-4911-4e75-8d9f-77ccd7578da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import AgentAI\n",
    "customer_support_agent = AgentAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d4aaf0-ee19-42e9-b8c0-cb8124f86654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your flight was missed due to a delay caused by Ragas Airlines, you can potentially receive compensation or rebooking options at no extra cost. However, if you missed your flight for personal reasons or other non-airline related issues, refunds typically depend on the fare conditions of your ticket:\n",
      "\n",
      "1. **Refundable Tickets**: Eligible for a full refund.\n",
      "2. **Non-Refundable Tickets**: May receive a partial refund minus cancellation fees, or travel credit instead of a full refund.\n",
      "3. **Basic Economy & Promo Fares**: Usually not refundable.\n",
      "\n",
      "You should check your fare rules in \"Manage My Booking\" or contact customer support for more specific guidance.\n"
     ]
    }
   ],
   "source": [
    "response = await customer_support_agent.ask(\"Can i get a refund for my missed flight?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4efdf-9068-44c6-9897-07bd95259c23",
   "metadata": {},
   "source": [
    "## Setup sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70f1a17-688e-4cd0-b449-1400076658a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.project.core import Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41474dc-cc11-4eb1-9473-6686857bc14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(name='Customer support RAG', root_page_id=1b35d9bf94ff801792bfd1824fac0c96)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = Project(\n",
    "    name=\"Customer support RAG\", \n",
    "    notion_api_key=NOTION_TOKEN, \n",
    "    notion_root_page_id=PROJECT_ID,\n",
    ")\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762ea33-4d54-477b-8920-138cc0ec8444",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26d45b5-697b-47b5-b65e-a82a69c9d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.model.notion_model import NotionModel\n",
    "from ragas_annotator.model import notion_typing as nmt\n",
    "\n",
    "class Dataset(NotionModel):\n",
    "    id: str = nmt.ID()\n",
    "    query: str = nmt.Title()\n",
    "    expected_answer: str = nmt.Text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9989e56e-86d5-44a9-82d4-904977c75e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project.get_dataset(\n",
    "    name=\"RAG Dataset\",\n",
    "    model=Dataset,\n",
    ")\n",
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "193d8366-b527-42ee-96d3-b51f70b59060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9163c2-bbd2-4425-a683-d5b5d67b76c6",
   "metadata": {},
   "source": [
    "## LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f78b886-5f87-4428-959d-966c7773b919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fail'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas_annotator.llm import ragas_llm\n",
    "from ragas_annotator.metric import DiscreteMetric\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=AsyncOpenAI())\n",
    "\n",
    "my_metric = DiscreteMetric(\n",
    "    llm=llm,\n",
    "    name='correctness',\n",
    "    prompt=\"Evaluate if given answer {response} is same as expected answer {expected_answer}\",\n",
    "    values=[\"pass\",\"fail\"],\n",
    ")\n",
    "\n",
    "    \n",
    "# test LLM as judge\n",
    "result = my_metric.score(response=\"this is my response\", expected_answer=\"this is not my response\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c45c3e-dd63-43eb-a12c-df08368db561",
   "metadata": {},
   "source": [
    "### Writing custom logic with metric (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "513c8d03-d244-48d8-b638-f8e99aa399f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "# @discrete_metric(llm=llm,\n",
    "#     prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "#     name='new_metric',\n",
    "#     values=[\"low\",\"med\",\"high\"]\n",
    "#     )\n",
    "# def my_metric(llm,prompt,example_store, **kwargs):\n",
    "\n",
    "#         class response_model(BaseModel):\n",
    "#              output: t.List[bool]\n",
    "#              reason: str\n",
    "        \n",
    "#         response = llm.generate(\n",
    "# \t        prompt.format(**kwargs),response_model=response_model\n",
    "# \t      )\n",
    "#         total = sum(response.output)\n",
    "#         if total < 1:\n",
    "#             score = 'low'\n",
    "#         else:\n",
    "#             score = 'high'\n",
    "#         return score,\"reason\",\n",
    "\n",
    "# result = my_metric.score(response='my response') # result\n",
    "# print(result)\n",
    "# print(result.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244f410-0292-4384-bce8-bb0358457e40",
   "metadata": {},
   "source": [
    "## Setup an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae42189d-17e5-4dec-8e81-9e1ca53399e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "class Experiment(Dataset):\n",
    "    response: str = nmt.Text()\n",
    "    correctness: t.Literal['pass','fail'] = nmt.Select()\n",
    "    correctness_reason: str = nmt.Text()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "146f2227-82b5-44b5-aaf4-4f8b35917002",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Project' object has no attribute 'langfuse_experiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;129m@project\u001b[39m\u001b[43m.\u001b[49m\u001b[43mlangfuse_experiment\u001b[49m(Experiment, name_prefix=\u001b[33m\"\u001b[39m\u001b[33mWorkshop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiment\u001b[39m(row: Dataset):\n\u001b[32m      5\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m customer_support_agent.ask(row.query)\n\u001b[32m      6\u001b[39m     score = \u001b[38;5;28;01mawait\u001b[39;00m my_metric.ascore(response=response, expected_answer=row.expected_answer)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Project' object has no attribute 'langfuse_experiment'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "@project.langfuse_experiment(Experiment, name_prefix=\"Workshop\")\n",
    "async def run_experiment(row: Dataset):\n",
    "    response = await customer_support_agent.ask(row.query)\n",
    "    score = await my_metric.ascore(response=response, expected_answer=row.expected_answer)\n",
    "\n",
    "    experiment_view = Experiment(\n",
    "        id=row.id,\n",
    "        query=row.query,\n",
    "        expected_answer=row.expected_answer,\n",
    "        response=response,\n",
    "        correctness=score.result,\n",
    "        correctness_reason=score.reason,\n",
    "    )\n",
    "    \n",
    "    return experiment_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c334a",
   "metadata": {},
   "source": [
    "## Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426d7b6-28b3-49ce-92c5-74e468c51165",
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_experiment.run_async(\n",
    "    name=\"my-experiment-trhee\",\n",
    "    dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69be73d",
   "metadata": {},
   "source": [
    "You may make any changes to AgentAI class like prompt, model, etc and run any number of experiments. Experiment now would have recorded in the Notion UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da891c-bf76-46c0-abe2-0c17ad638d74",
   "metadata": {},
   "source": [
    "### Train LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f36a1891-fea5-4d36-9fc2-28e0aeef67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.embedding import ragas_embedding\n",
    "\n",
    "from openai import OpenAI\n",
    "embedding = ragas_embedding(provider='openai',client=OpenAI(),model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55e1a9bb-c4ec-455f-aa85-758948fcba41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Project' object has no attribute 'get_experiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmy_metric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexperiment_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmy-experiment-one\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/scratchpad/ragas/notes/rag-playground/ragas_annotator/ragas_annotator/metric/base.py:97\u001b[39m, in \u001b[36mMetric.train\u001b[39m\u001b[34m(self, project, experiment_names, model, embedding_model, method)\u001b[39m\n\u001b[32m     95\u001b[39m datasets = []\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m experiment_name \u001b[38;5;129;01min\u001b[39;00m experiment_names:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     experiment_data = \u001b[43mproject\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment\u001b[49m(experiment_name,model)\n\u001b[32m     98\u001b[39m     experiment_data.load()\n\u001b[32m     99\u001b[39m     datasets.append(experiment_data)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Project' object has no attribute 'get_experiment'"
     ]
    }
   ],
   "source": [
    "my_metric.train(project,experiment_names=['my-experiment-one'],embedding_model=embedding,model=Experiment,method={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6961e10-66c8-4413-9f92-c664275074d8",
   "metadata": {},
   "source": [
    "### Compare experiments\n",
    "Hack to do in notebook, will be done in the UI once we have the UI ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83833d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_plot(exp_x: str, exp_y: str, metric):\n",
    "    # Load experiments\n",
    "    exp_x_data = project.get_experiment(exp_x, Experiment)\n",
    "    exp_y_data = project.get_experiment(exp_y, Experiment)\n",
    "    exp_x_data.load()\n",
    "    exp_y_data.load()\n",
    "    \n",
    "    # Compare experiments (assuming this is a function that exists)\n",
    "    project.compare_experiments(exp_x_data, exp_y_data)\n",
    "    \n",
    "    # Extract metrics from both experiments\n",
    "    results = {\n",
    "        \"exp_x\": [],\n",
    "        \"exp_y\": [],\n",
    "    }\n",
    "    for i in range(len(exp_x_data)):\n",
    "        results['exp_x'].append(getattr(exp_x_data[i], metric.name))\n",
    "        results['exp_y'].append(getattr(exp_y_data[i], metric.name))\n",
    "    \n",
    "    # Calculate counts for each category\n",
    "    exp_x_counts = {}\n",
    "    exp_y_counts = {}\n",
    "    \n",
    "    # For categorical data like 'pass'/'fail' or 'good'/'okay'/'bad'\n",
    "    # Get unique categories\n",
    "    all_categories = set(results['exp_x'] + results['exp_y'])\n",
    "    \n",
    "    # Count occurrences of each category\n",
    "    for category in all_categories:\n",
    "        exp_x_counts[category] = results['exp_x'].count(category)\n",
    "        exp_y_counts[category] = results['exp_y'].count(category)\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set up colors based on categories\n",
    "    if all(cat in ['pass', 'fail'] for cat in all_categories):\n",
    "        colors = {'pass': '#2196F3', 'fail': '#FF5722'}\n",
    "    elif all(cat in ['good', 'okay', 'bad'] for cat in all_categories):\n",
    "        colors = {'good': '#4CAF50', 'okay': '#FFC107', 'bad': '#F44336'}\n",
    "    else:\n",
    "        # Generate colors if categories are unknown\n",
    "        import matplotlib.colors as mcolors\n",
    "        colors = {cat: list(mcolors.TABLEAU_COLORS.values())[i % len(mcolors.TABLEAU_COLORS)] \n",
    "                 for i, cat in enumerate(all_categories)}\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot stacked bars\n",
    "    experiments = [exp_x, exp_y]\n",
    "    exp_counts = [exp_x_counts, exp_y_counts]\n",
    "    \n",
    "    # Calculate totals for percentage\n",
    "    totals = [sum(counts.values()) for counts in exp_counts]\n",
    "    \n",
    "    # Sort categories for consistent stacking (e.g., 'pass' always at bottom, then 'fail')\n",
    "    sorted_categories = sorted(all_categories)\n",
    "    \n",
    "    # Plot each category as a segment in the stack\n",
    "    bottoms = np.zeros(len(experiments))\n",
    "    for category in sorted_categories:\n",
    "        values = [counts.get(category, 0) / total * 100 for counts, total in zip(exp_counts, totals)]\n",
    "        ax.bar(experiments, values, bottom=bottoms, label=category.capitalize(), color=colors[category])\n",
    "        \n",
    "        # Add text labels inside the bars\n",
    "        for i, v in enumerate(values):\n",
    "            if v > 5:  # Only add label if segment is large enough\n",
    "                ax.text(i, bottoms[i] + v/2, f\"{int(exp_counts[i].get(category, 0))}\\n({v:.1f}%)\", \n",
    "                        ha='center', va='center', color='white', fontweight='bold')\n",
    "        \n",
    "        bottoms += values\n",
    "    \n",
    "    # Customize the chart\n",
    "    ax.set_title(f'Comparison of {metric.name.capitalize()} between Experiments', fontsize=14)\n",
    "    ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.legend(title=metric.name.capitalize())\n",
    "    \n",
    "    # Add totals on top of each bar\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(i, 101, f\"Total: {total}\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eec0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
