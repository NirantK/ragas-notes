{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19011569-f522-4ece-a8f6-e5e046d9dda9",
   "metadata": {},
   "source": [
    "## Simple RAG playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50362c0c-0dd1-4583-be57-6b8a141ca248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61523c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "PATH = \"/Users/shahules/Myprojects/notes/.envrc\"\n",
    "load_dotenv(PATH)\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "NOTION_TOKEN = os.environ.get(\"NOTION_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ecce8-2f0e-467f-ae3f-77e7fb131735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "PROJECT_ID = \"1b35d9bf94ff801792bfd1824fac0c96\"\n",
    "NOTION_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995bf9b-79c2-4cef-850b-e463b538f04a",
   "metadata": {},
   "source": [
    "## Customer support Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f825c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://huggingface.co/datasets/explodinggradients/ragas-airline-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad580dd-4911-4e75-8d9f-77ccd7578da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import AIAgent\n",
    "customer_support_agent = AIAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d4aaf0-ee19-42e9-b8c0-cb8124f86654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether you can get a refund for a missed flight depends on the reason for missing it and the fare conditions of your ticket:\n",
      "\n",
      "1. **Airline-Initiated Cancellations**: If you missed your flight due to an airline-initiated cancellation (e.g., flight delay or cancellation), you could request a full refund.\n",
      "\n",
      "2. **Passenger-Initiated Reasons**: If you missed your flight due to personal reasons (e.g., arriving late to the airport), the possibility of a refund will depend on the fare type you purchased:\n",
      "   - **Refundable Tickets**: You may be eligible for a full refund or partial refund, depending on the specific circumstances.\n",
      "   - **Non-Refundable Tickets**: Generally, these tickets do not allow refunds; however, you may be offered travel credit, or you may need to purchase a new ticket.\n",
      "\n",
      "3. **Process**: If eligible for a refund, you will need to follow the procedures outlined in the airline's cancellation policy. You may need to provide documentation of your flight and payment details.\n",
      "\n",
      "4. **Claim Timeframe**: Refund processing times can vary, so be sure to check the airline's policy on timeframes for refunds.\n"
     ]
    }
   ],
   "source": [
    "response = await customer_support_agent.ask(\"Can i get a refund for my missed flight?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4efdf-9068-44c6-9897-07bd95259c23",
   "metadata": {},
   "source": [
    "## Setup sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70f1a17-688e-4cd0-b449-1400076658a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.project.core import Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e41474dc-cc11-4eb1-9473-6686857bc14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(name='Customer support RAG', root_page_id=1b35d9bf94ff801792bfd1824fac0c96)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = Project(\n",
    "    name=\"Customer support RAG\", \n",
    "    notion_api_key=NOTION_TOKEN, \n",
    "    notion_root_page_id=PROJECT_ID,\n",
    ")\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762ea33-4d54-477b-8920-138cc0ec8444",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d26d45b5-697b-47b5-b65e-a82a69c9d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.model.notion_model import NotionModel\n",
    "from ragas_annotator import nmt\n",
    "\n",
    "class Dataset(NotionModel):\n",
    "    id: str = nmt.ID()\n",
    "    query: str = nmt.Title()\n",
    "    expected_answer: str = nmt.Text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9989e56e-86d5-44a9-82d4-904977c75e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project.get_dataset(\n",
    "    name=\"RAG Dataset\",\n",
    "    model=Dataset,\n",
    ")\n",
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193d8366-b527-42ee-96d3-b51f70b59060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9163c2-bbd2-4425-a683-d5b5d67b76c6",
   "metadata": {},
   "source": [
    "## LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f78b886-5f87-4428-959d-966c7773b919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fail'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas_annotator.llm import ragas_llm\n",
    "from ragas_annotator.metric import DiscreteMetric\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=AsyncOpenAI())\n",
    "\n",
    "my_metric = DiscreteMetric(\n",
    "    llm=llm,\n",
    "    name='correctness',\n",
    "    prompt=\"Evaluate if given answer {response} is same as expected answer {expected_answer}\",\n",
    "    values=[\"pass\",\"fail\"],\n",
    ")\n",
    "\n",
    "    \n",
    "# test LLM as judge\n",
    "result = my_metric.score(response=\"this is my response\",expected_answer=\"this is not my response\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c45c3e-dd63-43eb-a12c-df08368db561",
   "metadata": {},
   "source": [
    "### Writing custom logic with metric (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c8d03-d244-48d8-b638-f8e99aa399f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "# @discrete_metric(llm=llm,\n",
    "#     prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "#     name='new_metric',\n",
    "#     values=[\"low\",\"med\",\"high\"]\n",
    "#     )\n",
    "# def my_metric(llm,prompt,example_store, **kwargs):\n",
    "\n",
    "#         class response_model(BaseModel):\n",
    "#              output: t.List[bool]\n",
    "#              reason: str\n",
    "        \n",
    "#         response = llm.generate(\n",
    "# \t        prompt.format(**kwargs),response_model=response_model\n",
    "# \t      )\n",
    "#         total = sum(response.output)\n",
    "#         if total < 1:\n",
    "#             score = 'low'\n",
    "#         else:\n",
    "#             score = 'high'\n",
    "#         return score,\"reason\",\n",
    "\n",
    "# result = my_metric.score(response='my response') # result\n",
    "# print(result)\n",
    "# print(result.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244f410-0292-4384-bce8-bb0358457e40",
   "metadata": {},
   "source": [
    "## Setup an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae42189d-17e5-4dec-8e81-9e1ca53399e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "class Experiment(Dataset):\n",
    "    response: str = nmt.Text()\n",
    "    correctness: t.Literal['pass','fail'] = nmt.Select()\n",
    "    correctness_reason: str = nmt.Text()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146f2227-82b5-44b5-aaf4-4f8b35917002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@project.langfuse_experiment(Experiment, name_prefix=\"Workshop\")\n",
    "async def run_experiment(row: Dataset):\n",
    "    response = await agent.ask(row.query)\n",
    "    score = await my_metric.ascore(response=response, expected_answer=row.expected_answer)\n",
    "\n",
    "    experiment_view = Experiment(\n",
    "        id=row.id,\n",
    "        query=row.query,\n",
    "        expected_answer=row.expected_answer,\n",
    "        response=response,\n",
    "        correctness=score.result,\n",
    "        correctness_reason=score.reason,\n",
    "    )\n",
    "    \n",
    "    return experiment_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c334a",
   "metadata": {},
   "source": [
    "## Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426d7b6-28b3-49ce-92c5-74e468c51165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 15/15 [00:13<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(name=my-experiment-1111, model=Experiment)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_experiment.run_async(\n",
    "    name=\"my-experiment-one\",\n",
    "    dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69be73d",
   "metadata": {},
   "source": [
    "You may make any changes to AgentAI class like prompt, model, etc and run any number of experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da891c-bf76-46c0-abe2-0c17ad638d74",
   "metadata": {},
   "source": [
    "### Train LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f36a1891-fea5-4d36-9fc2-28e0aeef67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas_annotator.embedding import ragas_embedding\n",
    "\n",
    "from openai import OpenAI\n",
    "embedding = ragas_embedding(provider='openai',client=OpenAI(),model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1a9bb-c4ec-455f-aa85-758948fcba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|█| 15/15 [00:00<00:00, 115651.7\n"
     ]
    }
   ],
   "source": [
    "my_metric.train(project,experiment_names=['my-experiment-one'],embedding_model=embedding,model=Experiment,method={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6961e10-66c8-4413-9f92-c664275074d8",
   "metadata": {},
   "source": [
    "### Compare experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b470cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_plot(exp_x:str , exp_y:str, metric_name:str):\n",
    "    exp_x = project.get_experiment(exp_x,Experiment)\n",
    "    exp_y = project.get_experiment(exp_y,Experiment)\n",
    "    \n",
    "    exp_x.load()\n",
    "    exp_y.load()\n",
    "    \n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (random)",
   "language": "python",
   "name": "random"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
