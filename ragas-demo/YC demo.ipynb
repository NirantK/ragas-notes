{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414f2aea",
   "metadata": {},
   "source": [
    "- build application\n",
    "- test it manually\n",
    "- use ragas to calculate a score\n",
    "- make an improvement\n",
    "- recompute score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2171b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22507e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext,OpenAIEmbedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_query_engine(embed_model):\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=ServiceContext.from_defaults(chunk_size=512),\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7e41d",
   "metadata": {},
   "source": [
    "# Build an LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a9884cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "SemanticScholarReader = download_loader(\"SemanticScholarReader\")\n",
    "loader = SemanticScholarReader()\n",
    "phrases = [\"embeddings\", \"large language models\",\"BERT\", \"sentiment classification\"]\n",
    "documents = [loader.load_data(query=i, limit=100) for i in phrases]\n",
    "documents = sum(documents, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b649d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a976abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAIEmbedding()\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    service_context=ServiceContext.from_defaults(chunk_size=512),\n",
    "    embed_model=openai_model,\n",
    ")\n",
    "vector_index.storage_context.persist(\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708232b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc5e3474",
   "metadata": {},
   "source": [
    "# Test it Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!llamachat openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead5970",
   "metadata": {},
   "source": [
    "# Test with Ragas\n",
    "\n",
    "1. Build a test dataset\n",
    "2. Evaluate with Ragas Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d01eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "171it [04:21,  1.32it/s]                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occured in eval Please provide the context to be evaluated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1275it [12:22,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "testsetgenerator = TestsetGenerator.from_default()\n",
    "test_size = 50  # Number of samples to generate\n",
    "testset = testsetgenerator.generate(documents, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6d7a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f6e19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How accurate is Chinchilla on the MMLU benchmark?</td>\n",
       "      <td>- We test this hypothesis by training a predic...</td>\n",
       "      <td>Chinchilla is more accurate than Gopher on the...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In comparison, how does Gopher perform on the ...</td>\n",
       "      <td>Chinchilla uniformly and significantly outperf...</td>\n",
       "      <td>In comparison, Gopher performs worse than Chin...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What impact does interacting with an unreliabl...</td>\n",
       "      <td>- On these tasks, we find that human participa...</td>\n",
       "      <td>Interacting with an unreliable large-language-...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the resulting model after ...</td>\n",
       "      <td>We call the resulting model WizardLM.\\nOur fin...</td>\n",
       "      <td>The name of the resulting model after fine-tun...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the impact of the Selection-Inference ...</td>\n",
       "      <td>- \"We show that language models tend to perfor...</td>\n",
       "      <td>The impact of the Selection-Inference framewor...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How accurate is Chinchilla on the MMLU benchmark?   \n",
       "1  In comparison, how does Gopher perform on the ...   \n",
       "2  What impact does interacting with an unreliabl...   \n",
       "3  What is the name of the resulting model after ...   \n",
       "4  What is the impact of the Selection-Inference ...   \n",
       "\n",
       "                                             context  \\\n",
       "0  - We test this hypothesis by training a predic...   \n",
       "1  Chinchilla uniformly and significantly outperf...   \n",
       "2  - On these tasks, we find that human participa...   \n",
       "3  We call the resulting model WizardLM.\\nOur fin...   \n",
       "4  - \"We show that language models tend to perfor...   \n",
       "\n",
       "                                              answer question_type  \\\n",
       "0  Chinchilla is more accurate than Gopher on the...   conditional   \n",
       "1  In comparison, Gopher performs worse than Chin...   conditional   \n",
       "2  Interacting with an unreliable large-language-...   conditional   \n",
       "3  The name of the resulting model after fine-tun...        simple   \n",
       "4  The impact of the Selection-Inference framewor...     reasoning   \n",
       "\n",
       "   episode_done  \n",
       "0         False  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779ee852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_type\n",
       "conditional    22\n",
       "reasoning      22\n",
       "simple         17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.question_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03012518",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"semantic_testdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94609c",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c83765f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 5/5 [03:16<00:00, 39.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 5/5 [04:52<00:00, 58.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate with ragas\n",
    "from ragas.llama_index import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.llama_index import evaluate\n",
    "\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "\n",
    "test_questions = test_df['question'].values.tolist()\n",
    "test_answers = [[item] for item in test_df['answer'].values.tolist()]\n",
    "qe1 = build_query_engine(openai_model)\n",
    "result = evaluate(qe1, metrics, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee6a1253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.3606, 'context_precision': 0.2652, 'context_recall': 0.5634}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_result = result\n",
    "openai_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "434b227d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How accurate is Chinchilla on the MMLU benchmark?</td>\n",
       "      <td>[This also means that Chinchilla uses substant...</td>\n",
       "      <td>Chinchilla achieves a state-of-the-art average...</td>\n",
       "      <td>[Chinchilla is more accurate than Gopher on th...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In comparison, how does Gopher perform on the ...</td>\n",
       "      <td>[This also means that Chinchilla uses substant...</td>\n",
       "      <td>Gopher's performance on the MMLU benchmark is ...</td>\n",
       "      <td>[In comparison, Gopher performs worse than Chi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What impact does interacting with an unreliabl...</td>\n",
       "      <td>[On these tasks, we find that human participan...</td>\n",
       "      <td>Interacting with an unreliable large-language-...</td>\n",
       "      <td>[Interacting with an unreliable large-language...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the resulting model after ...</td>\n",
       "      <td>[Training language models to follow instructio...</td>\n",
       "      <td>The resulting model after fine-tuning with AI-...</td>\n",
       "      <td>[The name of the resulting model after fine-tu...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the impact of the Selection-Inference ...</td>\n",
       "      <td>[Selection-Inference: Exploiting Large Languag...</td>\n",
       "      <td>The Selection-Inference (SI) framework has a p...</td>\n",
       "      <td>[The impact of the Selection-Inference framewo...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How accurate is Chinchilla on the MMLU benchmark?   \n",
       "1  In comparison, how does Gopher perform on the ...   \n",
       "2  What impact does interacting with an unreliabl...   \n",
       "3  What is the name of the resulting model after ...   \n",
       "4  What is the impact of the Selection-Inference ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [This also means that Chinchilla uses substant...   \n",
       "1  [This also means that Chinchilla uses substant...   \n",
       "2  [On these tasks, we find that human participan...   \n",
       "3  [Training language models to follow instructio...   \n",
       "4  [Selection-Inference: Exploiting Large Languag...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Chinchilla achieves a state-of-the-art average...   \n",
       "1  Gopher's performance on the MMLU benchmark is ...   \n",
       "2  Interacting with an unreliable large-language-...   \n",
       "3  The resulting model after fine-tuning with AI-...   \n",
       "4  The Selection-Inference (SI) framework has a p...   \n",
       "\n",
       "                                       ground_truths  context_precision  \\\n",
       "0  [Chinchilla is more accurate than Gopher on th...           0.090909   \n",
       "1  [In comparison, Gopher performs worse than Chi...           0.333333   \n",
       "2  [Interacting with an unreliable large-language...           0.000000   \n",
       "3  [The name of the resulting model after fine-tu...           0.200000   \n",
       "4  [The impact of the Selection-Inference framewo...           0.428571   \n",
       "\n",
       "   context_recall  \n",
       "0             0.0  \n",
       "1             1.0  \n",
       "2             0.5  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_result_df = openai_result.to_pandas()\n",
    "openai_result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e2fb9",
   "metadata": {},
   "source": [
    "# Make Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4478400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 5/5 [03:16<00:00, 39.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 5/5 [04:56<00:00, 59.29s/it]\n"
     ]
    }
   ],
   "source": [
    "flag_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "query_engine2 = build_query_engine(flag_model)\n",
    "result = evaluate(query_engine2, metrics, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1984d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragas_score': 0.3560, 'context_precision': 0.2597, 'context_recall': 0.5655}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result = result\n",
    "hf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3703f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8993c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How accurate is Chinchilla on the MMLU benchmark?</td>\n",
       "      <td>[This also means that Chinchilla uses substant...</td>\n",
       "      <td>Chinchilla achieves a state-of-the-art average...</td>\n",
       "      <td>[Chinchilla is more accurate than Gopher on th...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In comparison, how does Gopher perform on the ...</td>\n",
       "      <td>[This also means that Chinchilla uses substant...</td>\n",
       "      <td>Gopher's performance on the MMLU benchmark is ...</td>\n",
       "      <td>[In comparison, Gopher performs worse than Chi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What impact does interacting with an unreliabl...</td>\n",
       "      <td>[On these tasks, we find that human participan...</td>\n",
       "      <td>Interacting with an unreliable large-language-...</td>\n",
       "      <td>[Interacting with an unreliable large-language...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the resulting model after ...</td>\n",
       "      <td>[Training language models to follow instructio...</td>\n",
       "      <td>The resulting model after fine-tuning with AI-...</td>\n",
       "      <td>[The name of the resulting model after fine-tu...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the impact of the Selection-Inference ...</td>\n",
       "      <td>[Selection-Inference: Exploiting Large Languag...</td>\n",
       "      <td>The Selection-Inference (SI) framework has a p...</td>\n",
       "      <td>[The impact of the Selection-Inference framewo...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How accurate is Chinchilla on the MMLU benchmark?   \n",
       "1  In comparison, how does Gopher perform on the ...   \n",
       "2  What impact does interacting with an unreliabl...   \n",
       "3  What is the name of the resulting model after ...   \n",
       "4  What is the impact of the Selection-Inference ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [This also means that Chinchilla uses substant...   \n",
       "1  [This also means that Chinchilla uses substant...   \n",
       "2  [On these tasks, we find that human participan...   \n",
       "3  [Training language models to follow instructio...   \n",
       "4  [Selection-Inference: Exploiting Large Languag...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Chinchilla achieves a state-of-the-art average...   \n",
       "1  Gopher's performance on the MMLU benchmark is ...   \n",
       "2  Interacting with an unreliable large-language-...   \n",
       "3  The resulting model after fine-tuning with AI-...   \n",
       "4  The Selection-Inference (SI) framework has a p...   \n",
       "\n",
       "                                       ground_truths  context_precision  \\\n",
       "0  [Chinchilla is more accurate than Gopher on th...           0.090909   \n",
       "1  [In comparison, Gopher performs worse than Chi...           0.333333   \n",
       "2  [Interacting with an unreliable large-language...           0.000000   \n",
       "3  [The name of the resulting model after fine-tu...           0.200000   \n",
       "4  [The impact of the Selection-Inference framewo...           0.428571   \n",
       "\n",
       "   context_recall  \n",
       "0             0.0  \n",
       "1             1.0  \n",
       "2             0.5  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result_df = hf_result.to_pandas()\n",
    "hf_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6603ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_result_df.to_csv(\"test_to_debug.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "878b9e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Selection-Inference (SI) framework has a positive impact on the performance of large language models (LLMs) in multi-step logical reasoning problems. It improves the performance of LLMs by over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. Additionally, the SI framework even outperforms a significantly larger baseline model on the same suite of tasks. The SI framework utilizes pre-trained LLMs as general processing modules and alternates between selection and inference to generate a series of interpretable, causal reasoning steps leading to the final answer. This approach allows LLMs to chain together multiple reasoning steps and solve more complex problems effectively.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result_df.iloc[4]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "962972e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.',\n",
       "       'Faithful Reasoning Using Large Language Models Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result_df.iloc[4]['contexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0005a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>How does the LLM-Augmenter system incorporate ...</td>\n",
       "      <td>[Check Your Facts and Try Again: Improving Lar...</td>\n",
       "      <td>The LLM-Augmenter system incorporates external...</td>\n",
       "      <td>[The LLM-Augmenter system incorporates externa...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "50  How does the LLM-Augmenter system incorporate ...   \n",
       "\n",
       "                                             contexts  \\\n",
       "50  [Check Your Facts and Try Again: Improving Lar...   \n",
       "\n",
       "                                               answer  \\\n",
       "50  The LLM-Augmenter system incorporates external...   \n",
       "\n",
       "                                        ground_truths  context_precision  \\\n",
       "50  [The LLM-Augmenter system incorporates externa...           0.266667   \n",
       "\n",
       "    context_recall  \n",
       "50        0.571429  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result_df[(openai_result_df[\"context_recall\"] - hf_result_df[\"context_recall\"]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1822423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question             In what ways does the calibration framework im...\n",
       "contexts             [Assistance with large language models A core ...\n",
       "answer               The calibration framework improves alignment w...\n",
       "ground_truths        [The calibration framework improves alignment ...\n",
       "context_precision                                                  0.1\n",
       "context_recall                                                0.571429\n",
       "Name: 49, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_result_df.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13e3b60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In what ways does the calibration framework improve alignment with human judgments?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_result_df.iloc[49]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c66d801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The calibration framework improves alignment with human judgments by implementing three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration. These strategies ensure that the evaluator model generates multiple evaluation evidence, aggregates results across various orders, and measures the difficulty of each example. Additionally, the framework includes manual annotation of \"win/tie/lose\" outcomes from ChatGPT and Vicuna-13B, which helps mitigate evaluation bias and achieve closer alignment with human judgments.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_result_df.iloc[49]['ground_truths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c5202a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The calibration framework improves alignment with human judgments by implementing three strategies. First, the Multiple Evidence Calibration strategy requires the evaluator model to generate multiple evaluation evidence before assigning ratings. This helps to ensure a more comprehensive and accurate assessment of the candidate responses. Second, the Balanced Position Calibration strategy aggregates results across various orders to determine the final score. By considering different orders of appearance in the context, the evaluation becomes less susceptible to manipulation and provides a more balanced and fair assessment. Finally, the Human-in-the-Loop Calibration strategy introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. This strategy allows for human intervention in cases where the evaluator model may struggle to accurately evaluate certain responses. Overall, these strategies work together to mitigate evaluation bias and improve the alignment of the evaluation results with human judgments.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_result_df.iloc[49]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4ecfd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The calibration framework improves alignment with human judgments by implementing three strategies. First, the Multiple Evidence Calibration strategy requires the evaluator model to generate multiple evaluation evidence before assigning ratings. This helps to gather more information and reduce bias in the evaluation process. Second, the Balanced Position Calibration strategy aggregates results across various orders to determine the final score. By considering different orders of appearance in the context, the evaluation becomes more balanced and less susceptible to manipulation. Finally, the Human-in-the-Loop Calibration strategy introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. This ensures that human judgment is taken into account and helps to address challenging cases where the evaluator model may struggle. Overall, these strategies work together to mitigate evaluation bias and bring the evaluation process closer to human judgments.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result_df.iloc[49]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371db4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
