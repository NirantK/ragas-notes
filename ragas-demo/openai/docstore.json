{"docstore/metadata": {"9d4e69ee-65e2-4301-b7b9-a248a086615a": {"doc_hash": "a2a3847e075b307bb1e0e63b67843747053c2f3a433e95ee1d5367255cb05137"}, "b6dbe332-1721-40e5-931f-9ddf24e63c8c": {"doc_hash": "b522584dbd9497b883f5f21e82307157de6ee43a30ea9d85389ed1c29cc2d2cf"}, "5e964383-21ab-4cdd-ab29-7f4c25ef61e0": {"doc_hash": "320609a4627a0fe0e8a4ffbc9507fb25eb9802f5d437d9ea6554b4f4861f7bc3"}, "225d23c1-c920-47f7-82a5-b067349b8523": {"doc_hash": "9875850ae12d92af4b4b2d11841f4589a85544719af291c419c93dea5b37ec65"}, "2a7e6612-65b2-48e3-84f4-09ef65816354": {"doc_hash": "8a7bea79646fecca0940b6def498e95559371f4c5f8fd789b326a5cb5a075d58"}, "d485337f-9833-41ce-8c75-207c5bb68fbb": {"doc_hash": "45e5a34fef9b4443707860e6caf2a975551f919a98ab430e5dda26a8ffc79ca8"}, "d3893411-0ab4-4320-a1a3-994833771c92": {"doc_hash": "b771790f4cd6f043d067f801758fa6f434fd49b7944fb5f5df68e35d929cb329"}, "2b03a5fa-c92c-45fb-beb7-76322ab9b25d": {"doc_hash": "b84b722367f44b70c8683a19f668957e011c4ba26ba7e5b93ea8d6c49fc9d246"}, "91419461-dd89-4137-950d-27a807e1e774": {"doc_hash": "0fed01bc10efa9a4410e67e4e4fd03401a0a01ece2670a966e9adafb24088d71"}, "24f98f71-2d40-48bc-8c24-589f8a059ec0": {"doc_hash": "56b583f88cb5353aa94e72693e74c9c606681096be52563ade0e45fb9f0f3285"}, "8a62d240-c08a-4ace-86d0-44301e9ae387": {"doc_hash": "03fa1917b909818ac17c84a3c703e5b14b0c5556bd04a1023829f0ceda0f83dd"}, "98161ef1-9562-44b3-90ce-783ed2dcf5ef": {"doc_hash": "6653bdc32ace10da3467984fa5f50ca033412dc819bfeceba75fa8aafb132392"}, "074db97e-555a-4aec-9721-a23a6b55e16f": {"doc_hash": "898304ee0f4ebb3f4c1ca8ab692f9ff113751ed67668fb5a54ce295b31068937"}, "9f27c421-d5ac-439d-a29a-01a3795f6370": {"doc_hash": "8ac15fef532c62591537a8b2c65a56937c47571ad5de3fe75539cdbb8ea76e47"}, "4d07cbcd-7c48-4a24-a1f8-b3f9e227c6dd": {"doc_hash": "0e345b73dab69591b0e0008c258c64da425c2c11e6de6d728f04d411b6f21c74"}, "9f3eaf45-cb13-4d46-bc0d-ab2244ed68c0": {"doc_hash": "cc4c77b0c72c14e66f0864300d847ef64517f9eec08c2eba4b26fff4f1442032"}, "086e27a9-78d9-4d0b-938f-3516774e4760": {"doc_hash": "042b0da682949a8cde94c8fa30c42c582462d7eea04402d3ac0d652d0d98db47"}, "6e6a9122-135d-4446-9dd5-fc2453b6604d": {"doc_hash": "d75a5975257224b5df0acc0ba72c9b341c11303b59fe4644ebf5ce3450412a0e"}, "ad15f097-c2c3-47a1-90be-7e6540b2fcc2": {"doc_hash": "ccc3f016ab1c8cb35b1ccb4ba9262719063dc8bb5760415386f37b037fad50fa"}, "3faa6ae6-1c30-4cfe-98c2-cc03acf9a2aa": {"doc_hash": "7696dc9695b4cb2613eef21b2937d27e53d0d65a60c5f3a495f4d146459f1e4a"}, "3de42693-0990-4d05-95e3-13b8955283c5": {"doc_hash": "d201858e1543bee96d848f68ab63c41e7a5b201566a3d15b50f25d6bb8f3bd39"}, "654edb2d-bbb8-4c26-94c8-2b85c98a14ae": {"doc_hash": "f77d1f92657dbf6b81b4f14b2043c0fffc6d6954bba17a4e7b4fdf862a9e64df"}, "8efdc06f-210f-4381-97ce-5fa91166fc13": {"doc_hash": "ee5547f77297d99b884d9b7a59b8e25530c73a50eebc88d8db63906afe00ea49"}, "c579e6f5-a675-4b34-8915-d831cadb6f61": {"doc_hash": "fc4fe364c0dd003248f8fb2649842b5c2e4ed748426e124cd63225d2302ef81b"}, "af2d4143-fbaa-4af1-8e3a-eddfd114c125": {"doc_hash": "ee746fcfe8a395ffb9da1d9d757d4996e3ff442e0e2239303bc329488d44bd28"}, "e9666ff2-f098-43a5-a5f6-8587f77e9fa6": {"doc_hash": "0ef97244d82fbc3a492933daf6961454d791780293d537a2bfe14081b0557569"}, "6ffd7394-6942-42e7-b01f-ae75612863f6": {"doc_hash": "2b18c4d30d3fa60497e45dbee4f7a7588e02f28f2dfb19f1aa2a1820e5ca4138"}, "ec2c0581-24a7-4545-9c19-0c22fa48181e": {"doc_hash": "2534352667a66689c71d920fcf3c8644d5f62f11a31764b84100a913604d554e"}, "4f12021f-c7b5-4d7e-8d43-658752481687": {"doc_hash": "313e88ce2c18212b4e728fc1edc990d454a087bf6abc82f2dc049c3e22398426"}, "1f4d4c07-16a2-48ed-8087-208a41dcee02": {"doc_hash": "bbf4b8d10c3b036c31750b476bbb9c0d9b05f7f0eaad5a89b1d50c452b0036b0"}, "e8a4883c-2c6d-4ceb-a728-e0a9f4e3f4d6": {"doc_hash": "ab6f76d5e08940ddea3bcb737f44e8d3b85a762e5feee31dde49df45c0c6dcf2"}, "1eef0a61-42eb-420b-90c1-1877dda8c746": {"doc_hash": "de415aafd8e17507d516067ae621721e820c8fa035f2f0fe943b802f221ca383"}, "0f3300ca-b6fc-42ab-8617-078c5ce3566c": {"doc_hash": "87cdbf374301404ab9daf58e6cb3f365e22f94d2c1f3ea00dd5c6d5893d3d8ac"}, "d2dfc7f1-a1a2-4b88-8ae7-75aebeaeb47d": {"doc_hash": "c1f4717f12078cde10c71e50ff266c4e1b964fb8ed58db6eceacaddf29e6da64"}, "156aa766-fabe-4af1-84b9-1c6f379aa6da": {"doc_hash": "e32b915041723eb1af4d96baaba0bd9e8d8df6fdd3664611b851cfaed4a1e4c2"}, "86caca98-8de3-4988-99ae-4be11e028419": {"doc_hash": "6941925e247a0c06738fbb5d4b45e6c3812d25a55da681f16d4f94d35a48712b"}, "1e078f33-1bcf-4262-8754-6e344d200c90": {"doc_hash": "6c56257832358b39cef0c7d9c55d6f22c263db1e7ed56f406b796e35d2e55fa8"}, "4b3f6b2c-a193-4d8a-93e0-371072339959": {"doc_hash": "eda8213a40e1bcfc0f0c28a5528d7e7284e4ba351cd02191938bd2a257eb2dfb"}, "4dad8280-c405-4ef5-b229-6d47ff7480af": {"doc_hash": "c586c44df2005b4e16a1b5de9cc9101cc1d1e78a17d07fda4014854563af2841"}, "3b4da62b-3fd8-44c7-9c88-d589ff00f28c": {"doc_hash": "0bd77eae9d148a9522824a9c45fb6e0f3b4bad478828d52290068899720e9066"}, "72cd9b3f-a353-4133-b1e5-fe98326c95d8": {"doc_hash": "f717280d3b579648f915e2e2e3bd01a557b79c74892e047b5c195554e9f78f1c"}, "9cf0347a-2685-4c0f-b46e-51517d2b82a5": {"doc_hash": "8ecc98606c375f73cac0759dc2d30f3601e0fcc2afd739442fafb31544026612"}, "971c9ea2-0ff3-4d54-8971-a2c3dbe03caa": {"doc_hash": "3c118348fef64501188a5b278f8a965fbf811206f4c20bf99e550e0ac38f3f81"}, "cdf9b041-fd18-49af-9797-61ea9bc8af67": {"doc_hash": "9adb080aeb8cdb24e0f4acead18debd33403cb0b91fd7c24505e22b50a28ed20"}, "226c7a90-ff29-42bd-8d04-9f3bd74d1ea4": {"doc_hash": "8b38a91476ebd3d4d1ea8a844bd141f8b89418939bbb4d626bf98de714ac5395"}, "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5": {"doc_hash": "cb19ddb7e0f44d55e2ed9bacc5ba9d429d3941e20e7a6ae4ffeda8f95779b9d6"}, "bcc2ee4a-a361-4d0e-90f6-481b167db6d7": {"doc_hash": "0ea14251b3211fc52800013601b0d3c7f64d4a59a7c06be0d189a8973cb1a993"}, "0c87d542-9d83-4107-9294-e3f7606fc9ab": {"doc_hash": "8de13bd4f2787cb7053dbc98c45cea18a27bcc9babb8aa6775edb1ac6ed0c65d"}, "9612f788-1e71-4259-9bb3-8cbfffe0238e": {"doc_hash": "ffdb67c4beb11c397a1da77145e8fe190d4258e86940b70531a418754a6a7d77"}, "a28b6264-bab4-42a5-be83-1ca8a5d9dda7": {"doc_hash": "970c0d15e8ed4d8feed02782bd9e3676359e3a8a41aec7b196c46993f33175a3"}, "df9c9841-0cb0-417c-8738-1a527502e9f6": {"doc_hash": "23d81c93e46cf261a8bdf1f0e1fa5b22885aa0c9f13b1ba92ed902b8e1d3e7be"}, "9ade8cfe-c14b-4766-b7b9-7da722b55530": {"doc_hash": "c32a975e80382927b2c668efa0710bb840cb094b60a04bb0253532f9a079414b"}, "0a2f9da6-ebbf-47d0-9f32-21325ea11075": {"doc_hash": "a3b8226bb10ec2f14456a6e8c4ff9642002e6c903a8f1b43223da5e0ad66c4de"}, "4c0fc807-c715-426f-9647-b320bdeda630": {"doc_hash": "640631aed6ebf90a07e9b4c22a1c7a18b6e79c38927cdecd84b00b83c5814e39"}, "24deb67b-4795-4a2a-9203-89b7fefd583e": {"doc_hash": "cf0824a081550d6e829b85e445f88e4c561d248d0e08e37f849b24004a974940"}, "dd9ab116-ba7f-4b46-b567-115a805d403d": {"doc_hash": "85cbec6cf844ae4fac60fa716505bbb3b0aad637875f45370a63f21e98d62635"}, "900f35db-fc9d-4fd1-bf21-8f466e18d253": {"doc_hash": "1e4cf898fdc6a5f545ec3e3290dc0c29c384c5fb0116eb54be3f920cdde890c3"}, "33555ccf-3b2c-4463-a1ba-dbf354d2b7d5": {"doc_hash": "6fec11ce3bd90f1d4ff4f59291cd6d9af9ffb0b066eff9a3ae22321c9f8c3390"}, "15740998-e241-46ad-a84e-948b02bf809b": {"doc_hash": "193c1d933e02a8c329b05bbe261cf2715022bb7647246c241b23db1e6670b71f"}, "1162a76f-1d11-4771-8655-e8c108f4cf1c": {"doc_hash": "4b633899e2590e17058123c1260020c92ce29c23dd10e66014301e73ec5559da"}, "2d698339-0250-40db-b79d-28caddeab50d": {"doc_hash": "0166d2c30f0f6abf3fbd399b3db934552cd78b2d0fab15d9db5b0f0b8d3e27c5"}, "a595946b-5120-4796-a7db-4ec3b76aece2": {"doc_hash": "310cb9c9700bad039fc899761cf8b523328ac7a981f5a96e4daa2b61756fdb8c"}, "59318c71-e193-4755-b4c9-a266d0c7c5a8": {"doc_hash": "475d0c0a3172b9d6415e1f0667acedffde96f22c19d3333a920343bb23e58559"}, "5ddb59b9-8a4c-4a1a-8ffb-9834a8a136d4": {"doc_hash": "47f086a4470682fc64f93351f5ceaa188fefa778e71e4248a45fd2416e19dc53"}, "c0a4f260-a296-4bbd-bd6d-4e4df8b3bee0": {"doc_hash": "ca89d7618136fbd8c358f00a37e68ffdc901cf06d5831076b7d9b5f101d4fe8d"}, "a7991df4-901e-4302-bb9d-11fdadb86650": {"doc_hash": "aab52cbf8104a02530393f4dda8e2ac44de463ad126549e818e58cb9398fd6b3"}, "e74b3d2e-2beb-40fe-9ef0-3fc75e9daaa0": {"doc_hash": "3a5633e06cddf25e3a7dd8222c15d02ee3b1cf22fec427ce4eb66ef5f147b5e2"}, "cb6a87f4-e6f2-45e0-82d2-2066ec7c2771": {"doc_hash": "ba11b8822dd9894d21761443a74f85b59a147fe8445eb0f5ac25702247a72bbe"}, "abbb03a1-5cbe-423e-8b58-9374443c1584": {"doc_hash": "d85d28543f254e832ea8508a1921253e8d25d109d376a149e038a3f7db2a52de"}, "549c635a-fff8-47fa-8d18-b71cb0b716c9": {"doc_hash": "038b1982bfc132210e949f65df14f81885092b44929bab14feb17753c0aae4c8"}, "3809e1ba-81f3-4c4b-8903-9447067af8ff": {"doc_hash": "3ff0f1e46b3be82804cb3c1392a33d816b9c66b58489b84a446743c92c47bb76"}, "1c132b5b-24ec-47c6-a405-10017b6c0274": {"doc_hash": "a17577537cdf41df0c576b75099f449cc689c467e04893b963fc64b9672cde54"}, "8c1c9973-9d55-471b-9863-a6cea419fef8": {"doc_hash": "667e1bc5d06a4ecc94360ced63b0f0ec3a9f1b5547474cb94e7550e80acff60c"}, "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9": {"doc_hash": "b38bdb362c7cb6b2d1abfb557e29fc0ae5e328bcd50a0980bf62118fe892ef8f"}, "cd11a346-f5e8-4319-b698-af78eb861136": {"doc_hash": "16d7bf4cf435eaf5d3d5f598f0691a855a36892ae614b6c1620d5179a11b3daa"}, "0130dfa3-a254-4f8f-adfa-d021ec30798c": {"doc_hash": "f3c02688653dbc497abd0720eb522a5179ea75ec1354ec0f698ff59b95804b18"}, "dfb1163f-e732-40ed-9253-f227f0dff022": {"doc_hash": "96eb80d55de28f0c17a7cf4db1b66c012fec334731abb53a4ab8649826b4eb80"}, "6925cf83-6ee8-4cc9-96cb-b69c650af276": {"doc_hash": "57a08dff5ecc800add2f3a1cfce6c8ba836dafe3a6f8d7488be8f368f2a0b6da"}, "a36974b6-77f6-4c00-b9af-e56ea621bfb3": {"doc_hash": "c5826400fea90516f3b1cc06c6c18c38c6400f046fdc92d1d6cb621b28ca6ec0"}, "dbeb5079-4654-4901-98e2-a47c333d1066": {"doc_hash": "a216ee3750a95d770941b83334ff5a00e96131071d56f03ad03923a4bdb29904"}, "cc7cdf4d-72a7-40e4-a387-416a320e7a77": {"doc_hash": "e05b2c8da0d2a7d616e7454d36a76b365ac3b74a1ca1189f7350353079185108"}, "fc10c197-ca0c-454c-9715-847fc89f5753": {"doc_hash": "e2c9acbfdc0384496a3c470044a8f6a7326379ade368a14dad998a9f11973205"}, "92196516-0ccb-43f6-9292-15a8044b5b4b": {"doc_hash": "5d69d950303c81576e9621d34c1c96bf79d88c5f93774689db30b660e448f61c"}, "3234fd39-7653-4a79-a5c1-53a2c59f3b13": {"doc_hash": "8e0db000e5d40d1da8b1d70060d105716ccc44494e178486aa78693560b8adc4"}, "5dc919c1-7b45-4d76-a07b-b4fed3e88eb4": {"doc_hash": "86a70af8058503d149d4efc718b592a13e957362b2f5eb33c9eef764672a741b"}, "b879d8cd-e884-4787-b79d-18819ec7477d": {"doc_hash": "6dcc06742cd26418bce0469b8799b095856f1b98ba2496ea173fceaa8370da5f"}, "da5f1c01-d813-43ae-9217-2d0346f39ca4": {"doc_hash": "077919e3b1a75e81196e05e497fd26da888de190fef56e6b80973bdb9dc26c75"}, "55e6e486-bc6e-4073-bba5-89639e708e8c": {"doc_hash": "55774d45084da3aa4b6ac801729df7d2c961155affbc806fc9db602ccf6b586d"}, "c5c97630-d99a-4175-bc31-b30aa39d8535": {"doc_hash": "329958cb7028c1f8c16187de17ca589abab1edaad98bb01b1bbc0c137a10cb51"}, "a73bcb04-a9cd-4978-b5f1-1fc1fe019048": {"doc_hash": "6d089637c26fadd9a8073b512b8e5fbec7ae560ee3e86c5a254941469a7bdde1"}, "67b87faa-cc33-4f08-863a-92ed7b442b87": {"doc_hash": "aea3993e4b9e4d23da2a60c9bae949e1d1ae2a71f06a6267dc42d588e436b0b5"}, "ffc24877-e43c-40fc-893c-aa6f9a1c917f": {"doc_hash": "2b848de29c6bf7b7d02f71e1022ba7bcb4c37b76d8cffac7a8574ad7ad04bd48"}, "037cd110-ee3a-4926-8674-4817122f5327": {"doc_hash": "39ef86a0356dbe4e8df049597a04b4e9e390ed1f83758a1f656379b3268e55ef"}, "1e3e953f-e419-4782-8eae-dc40d3bb8f26": {"doc_hash": "7a8d6e1cd3fc82394d8cf282c813a37114ea7228cacf42e811fd7fcbfc3d530b"}, "26316f62-a24c-46fb-afff-b354925bc1c8": {"doc_hash": "bae8eee3af9ea0c36f909ae05f5896fb0a9810b65ef70b96c8e0f78b6a8d016f"}, "12b644db-be9e-45c7-91f3-d6a67efa7afa": {"doc_hash": "f6726bb1e34c7fb9ebba5a98b8566396dfdd2bfa7a0831c75cb2c3763025eb59"}, "af725450-3369-4ed4-8f24-a8c80adb25dd": {"doc_hash": "be83c926ff988c4d06e89942d881de2a4b795cbf8cbbf0c7d24b13dec8126a0c"}, "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb": {"doc_hash": "8187ad14bdcc3b6ccefd779afc604bfe61c0754ee94c9ad74624197993063aee"}, "a023287c-0fb4-4b10-a1c4-9a94b7d4ba4f": {"doc_hash": "42beaeaed2cdeb3b2dcc33f06cb1f33f7ddc036d5971eaecee31b80cf1f55e31"}, "ba222a86-8474-4ab0-b55f-65ef9ef4acfb": {"doc_hash": "b87aa0090cd01e8746a5f71b154b0e1dfe2f7997336309f1d2cd2f719afbacea"}, "6f3cbbc8-d0b2-47cb-bd1f-d1cc0cff6fb2": {"doc_hash": "a2a3847e075b307bb1e0e63b67843747053c2f3a433e95ee1d5367255cb05137", "ref_doc_id": "9d4e69ee-65e2-4301-b7b9-a248a086615a"}, "e0cbdf8b-55a1-4c36-ae9b-3bb12616f62f": {"doc_hash": "22ee726759b2c60b60c0a1f8d3b9537aa856383470e7e9e34200352d44f89a24", "ref_doc_id": "b6dbe332-1721-40e5-931f-9ddf24e63c8c"}, "0be527d9-32a9-4b5c-ae26-d0e0f10b0824": {"doc_hash": "b0db24963103cf971893069225d076cf7a8a4cc889c98e84bc76d9462feedec6", "ref_doc_id": "b6dbe332-1721-40e5-931f-9ddf24e63c8c"}, "0a8aefbc-7226-4adc-a9a8-76fdb2621bfd": {"doc_hash": "65a5ec6d7b31f9903d6d77e6a3c87ba0650d784d33d9001e3e9ba1ffc02fdb7e", "ref_doc_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0"}, "fff9973f-f7ce-46c6-87a4-6b2486cc976b": {"doc_hash": "8b63ff0114e0729e8bc6d32cdc934c0e1995c1674397a397b41c3be1d9593b91", "ref_doc_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0"}, "5de093c7-cd0f-4c81-b60e-082fde23e8a9": {"doc_hash": "6037a2e1255b16220c25cb43813443ee66dbd7edef141eafeda1ea52008ecb59", "ref_doc_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0"}, "69a84425-eb34-4c70-933d-f272791884bb": {"doc_hash": "64ed9869c2a09e1f64cf3ce24d0ddce8306b2fd2a418439ca86c443ed1e87701", "ref_doc_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0"}, "102420fa-1de2-49c5-bec9-e11dbe9754b9": {"doc_hash": "4a8ef4b9516a44874c875bd7ba83fbb55ca417da86bc0d6e258d24d2102e8975", "ref_doc_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0"}, "54a5d4ba-ea89-4ea8-8f4b-edae0669a2b9": {"doc_hash": "9875850ae12d92af4b4b2d11841f4589a85544719af291c419c93dea5b37ec65", "ref_doc_id": "225d23c1-c920-47f7-82a5-b067349b8523"}, "7eb6de56-76ee-4ec1-8ecb-9e18ed8c01e4": {"doc_hash": "4ec6b7c391a0f247978b1e355bf08648177476e047c9b1e256ef0db8ff3ef655", "ref_doc_id": "2a7e6612-65b2-48e3-84f4-09ef65816354"}, "65f57cbe-5504-4bc8-a463-49c5bf40351d": {"doc_hash": "2bcc9a54c3447535f57e10bed5d2190882b34fededcfc2c7e0f9138921ecb806", "ref_doc_id": "2a7e6612-65b2-48e3-84f4-09ef65816354"}, "698ea218-5b80-4962-9760-6a9e446e532c": {"doc_hash": "b950f2ef13fb46fa4d756f3d36a062887e6696d22ed907158c166b57a64a1dd1", "ref_doc_id": "d485337f-9833-41ce-8c75-207c5bb68fbb"}, "dc38a3b7-a08a-4349-8e7b-c5e82af532f8": {"doc_hash": "e8a85b69369c44585b3b71552424d19ca28a56faa0355ff0314450f1a3105640", "ref_doc_id": "d485337f-9833-41ce-8c75-207c5bb68fbb"}, "f04ff5eb-f20a-43e3-a5f1-bca6b660d27f": {"doc_hash": "b771790f4cd6f043d067f801758fa6f434fd49b7944fb5f5df68e35d929cb329", "ref_doc_id": "d3893411-0ab4-4320-a1a3-994833771c92"}, "c96cc3bf-4133-4d21-93db-e9cd883a77c0": {"doc_hash": "b84b722367f44b70c8683a19f668957e011c4ba26ba7e5b93ea8d6c49fc9d246", "ref_doc_id": "2b03a5fa-c92c-45fb-beb7-76322ab9b25d"}, "39e9db19-cebf-496d-bb7b-35270bde9b97": {"doc_hash": "79918385257d62fdd9ab4dc1a97ff54b9a28bcc2ed9ced60e18f415905ceba90", "ref_doc_id": "91419461-dd89-4137-950d-27a807e1e774"}, "195fdb5b-3cc0-4ea7-98b7-78d650698154": {"doc_hash": "d3adcb1c0394bdeb24f97dad6c57c8ac8e3ee72a9b4d78f7273aa182583dde67", "ref_doc_id": "91419461-dd89-4137-950d-27a807e1e774"}, "97b84ea5-6bdd-4026-aeb6-465747f6a59f": {"doc_hash": "f0f0743d00df50433247ae37178df7fe6efb5174f18aff4a7f811868e4d39799", "ref_doc_id": "24f98f71-2d40-48bc-8c24-589f8a059ec0"}, "9232ed54-292c-4300-b938-5d8a70849947": {"doc_hash": "335e97bcee757aa99aa4e70aaa5fbf6611afdc582a7788f8d706c31838d01d07", "ref_doc_id": "24f98f71-2d40-48bc-8c24-589f8a059ec0"}, "9d5a5c37-6e9b-4a09-bb76-f803eaae1d0c": {"doc_hash": "03fa1917b909818ac17c84a3c703e5b14b0c5556bd04a1023829f0ceda0f83dd", "ref_doc_id": "8a62d240-c08a-4ace-86d0-44301e9ae387"}, "1de38d24-26cf-4ad9-bb1b-3b732a34e878": {"doc_hash": "1047e71ec9c7792d7330e15fa41d934e88627c7d92e03f155bbeab5f895b22d3", "ref_doc_id": "98161ef1-9562-44b3-90ce-783ed2dcf5ef"}, "44513a67-2ea3-4b6e-8dc1-78120fb2782a": {"doc_hash": "cfd92659f678c21c5925d374253eb78ae3d8d6cb06bf6d0e64c5cf9f226fc6be", "ref_doc_id": "98161ef1-9562-44b3-90ce-783ed2dcf5ef"}, "7a8f40b2-e0b6-4c4b-a8d0-727daf6c0465": {"doc_hash": "898304ee0f4ebb3f4c1ca8ab692f9ff113751ed67668fb5a54ce295b31068937", "ref_doc_id": "074db97e-555a-4aec-9721-a23a6b55e16f"}, "31723c28-0a50-4554-ada8-ffa913cc5e39": {"doc_hash": "970b752ea44a632759c8d374c9442f496638646b0389ef1e803d6f06a318a9ee", "ref_doc_id": "9f27c421-d5ac-439d-a29a-01a3795f6370"}, "38c5224b-9be4-4e41-8558-039157de37a3": {"doc_hash": "818cbd8252c9e1362195cdddc9f66c5666a0e7f35e3e7bb78873fbff61e7d47e", "ref_doc_id": "9f27c421-d5ac-439d-a29a-01a3795f6370"}, "8e8deaab-8a86-44bf-b3d6-2ecf20b28c18": {"doc_hash": "0e345b73dab69591b0e0008c258c64da425c2c11e6de6d728f04d411b6f21c74", "ref_doc_id": "4d07cbcd-7c48-4a24-a1f8-b3f9e227c6dd"}, "c6dcc55f-5ece-4543-b00d-7f28f0ccbf53": {"doc_hash": "cc4c77b0c72c14e66f0864300d847ef64517f9eec08c2eba4b26fff4f1442032", "ref_doc_id": "9f3eaf45-cb13-4d46-bc0d-ab2244ed68c0"}, "12256cf7-a7ce-432d-9b90-23d5ebb351a5": {"doc_hash": "042b0da682949a8cde94c8fa30c42c582462d7eea04402d3ac0d652d0d98db47", "ref_doc_id": "086e27a9-78d9-4d0b-938f-3516774e4760"}, "032e038d-a860-4e32-94fe-80e1db629093": {"doc_hash": "d75a5975257224b5df0acc0ba72c9b341c11303b59fe4644ebf5ce3450412a0e", "ref_doc_id": "6e6a9122-135d-4446-9dd5-fc2453b6604d"}, "8a71ecf5-e9b3-43d5-973b-00530135c89b": {"doc_hash": "d2b651a4fe498dd92474d62518e46a82f3ce0244e1fbb04332af73770ebd6b62", "ref_doc_id": "ad15f097-c2c3-47a1-90be-7e6540b2fcc2"}, "0c39c7c2-5fc3-4982-92d1-1daaf213d51a": {"doc_hash": "a45dff4757472c9047ee1ee3dc28e0af629a401bb8439e38d3f90228f7673131", "ref_doc_id": "ad15f097-c2c3-47a1-90be-7e6540b2fcc2"}, "0eac94a7-dc16-437a-81d1-975297d031c1": {"doc_hash": "7696dc9695b4cb2613eef21b2937d27e53d0d65a60c5f3a495f4d146459f1e4a", "ref_doc_id": "3faa6ae6-1c30-4cfe-98c2-cc03acf9a2aa"}, "df62aff7-b4e1-409c-9a81-e3c497188423": {"doc_hash": "d201858e1543bee96d848f68ab63c41e7a5b201566a3d15b50f25d6bb8f3bd39", "ref_doc_id": "3de42693-0990-4d05-95e3-13b8955283c5"}, "d107e6e2-4aac-410a-b730-87b570742df9": {"doc_hash": "3f68977580c4fdc06a454313fd682c725d36d496d3a4c1267747a5eb211e9755", "ref_doc_id": "654edb2d-bbb8-4c26-94c8-2b85c98a14ae"}, "691358af-db41-4a08-ac41-9e6031fd6c12": {"doc_hash": "93e7d0c2170592411ecfa9dd27096167c73d4432f1d09655d86a1730b901859a", "ref_doc_id": "654edb2d-bbb8-4c26-94c8-2b85c98a14ae"}, "8cf2dc98-f5aa-45a6-836a-8267aa6909de": {"doc_hash": "eed2a9362e7f5b0bfe30cf042d81f4b30fc84a655440f5707d175ecab3078674", "ref_doc_id": "8efdc06f-210f-4381-97ce-5fa91166fc13"}, "06369f7d-fb14-4f91-834d-9f296b153f00": {"doc_hash": "a15745dc7a21ab1a93efa9d34f5f36f8b366ee97da11c95b0577d4041995ff90", "ref_doc_id": "8efdc06f-210f-4381-97ce-5fa91166fc13"}, "818b3416-c03d-47ed-9483-b5be82d8e179": {"doc_hash": "33f464051af0a800964a916372384027160b246291102f8a11a76ded3f0fb220", "ref_doc_id": "c579e6f5-a675-4b34-8915-d831cadb6f61"}, "1fe355a9-395e-4006-946b-0e7073ce9c96": {"doc_hash": "05bd6a1e6f574c7cc140cf01a0b0728cebaeb3beab2125a9bfd9762cb0beb33f", "ref_doc_id": "c579e6f5-a675-4b34-8915-d831cadb6f61"}, "ba252561-ac1e-48a3-a28b-ffca8d1fb2cb": {"doc_hash": "ee746fcfe8a395ffb9da1d9d757d4996e3ff442e0e2239303bc329488d44bd28", "ref_doc_id": "af2d4143-fbaa-4af1-8e3a-eddfd114c125"}, "73b3f92b-56f4-43e3-b69b-24147d2b3c19": {"doc_hash": "0ef97244d82fbc3a492933daf6961454d791780293d537a2bfe14081b0557569", "ref_doc_id": "e9666ff2-f098-43a5-a5f6-8587f77e9fa6"}, "793ac17d-6bc7-4b4c-bec7-7bca0600ddec": {"doc_hash": "2b18c4d30d3fa60497e45dbee4f7a7588e02f28f2dfb19f1aa2a1820e5ca4138", "ref_doc_id": "6ffd7394-6942-42e7-b01f-ae75612863f6"}, "8cc6e23b-27c7-4162-a2f5-93273dbb0cb9": {"doc_hash": "2534352667a66689c71d920fcf3c8644d5f62f11a31764b84100a913604d554e", "ref_doc_id": "ec2c0581-24a7-4545-9c19-0c22fa48181e"}, "3d4ab8cf-d1c2-4058-b695-cd54ee4cbdfc": {"doc_hash": "313e88ce2c18212b4e728fc1edc990d454a087bf6abc82f2dc049c3e22398426", "ref_doc_id": "4f12021f-c7b5-4d7e-8d43-658752481687"}, "a1e8d600-2713-44df-89fa-8eeb4d807fd6": {"doc_hash": "6212c83691004e1d1455d6115eea7ba84fb4a27f194e0e08b3f94940a573981e", "ref_doc_id": "1f4d4c07-16a2-48ed-8087-208a41dcee02"}, "6401cbba-3834-4075-8fea-74a0fd56f11d": {"doc_hash": "ffb335d2cfcbc596f09e2ef3b333b44f695bcf483e08ffe2a02529ea5e44ff75", "ref_doc_id": "1f4d4c07-16a2-48ed-8087-208a41dcee02"}, "5d4630c4-4b9a-45c8-a9a0-525d5e2ec4f7": {"doc_hash": "ab6f76d5e08940ddea3bcb737f44e8d3b85a762e5feee31dde49df45c0c6dcf2", "ref_doc_id": "e8a4883c-2c6d-4ceb-a728-e0a9f4e3f4d6"}, "623d4355-e3ff-4a3e-ae35-dde9d422de86": {"doc_hash": "5c711276631332e3bf8046c072efb4b0407a65f42150b3cea514ad063320d51b", "ref_doc_id": "1eef0a61-42eb-420b-90c1-1877dda8c746"}, "33242c4d-5a7b-4b72-80f1-a158dbd38e39": {"doc_hash": "05d884c110ab059b9b138ac258bd803296f270753064698d741a1b6970f17fd2", "ref_doc_id": "1eef0a61-42eb-420b-90c1-1877dda8c746"}, "0518ab85-5af8-4f0c-b153-e0d076829f6e": {"doc_hash": "87cdbf374301404ab9daf58e6cb3f365e22f94d2c1f3ea00dd5c6d5893d3d8ac", "ref_doc_id": "0f3300ca-b6fc-42ab-8617-078c5ce3566c"}, "2ef9155a-0808-4444-b2a5-d750a2715654": {"doc_hash": "c1f4717f12078cde10c71e50ff266c4e1b964fb8ed58db6eceacaddf29e6da64", "ref_doc_id": "d2dfc7f1-a1a2-4b88-8ae7-75aebeaeb47d"}, "b8a6ff02-102b-4c71-8503-9d8858564732": {"doc_hash": "e32b915041723eb1af4d96baaba0bd9e8d8df6fdd3664611b851cfaed4a1e4c2", "ref_doc_id": "156aa766-fabe-4af1-84b9-1c6f379aa6da"}, "52409e45-5230-4e5e-9999-509af585a51a": {"doc_hash": "6941925e247a0c06738fbb5d4b45e6c3812d25a55da681f16d4f94d35a48712b", "ref_doc_id": "86caca98-8de3-4988-99ae-4be11e028419"}, "f39152ab-9a48-4bca-98d9-215297cfb494": {"doc_hash": "6c56257832358b39cef0c7d9c55d6f22c263db1e7ed56f406b796e35d2e55fa8", "ref_doc_id": "1e078f33-1bcf-4262-8754-6e344d200c90"}, "be061a19-abb3-4887-a7c5-fcc96fd623ab": {"doc_hash": "5032bc2e9352938e42d923832a13a9cd9502b66efa04d4347d7f314c7f3e558c", "ref_doc_id": "4b3f6b2c-a193-4d8a-93e0-371072339959"}, "67800253-f967-4692-bba9-64867bcbb3e7": {"doc_hash": "31fcd6b487ec5acb7aba0d646afcbc9c2993149cb92388ae7f0de5b2e69de228", "ref_doc_id": "4b3f6b2c-a193-4d8a-93e0-371072339959"}, "e1b070a8-3dd3-4f0c-8cca-597592d1b1df": {"doc_hash": "c586c44df2005b4e16a1b5de9cc9101cc1d1e78a17d07fda4014854563af2841", "ref_doc_id": "4dad8280-c405-4ef5-b229-6d47ff7480af"}, "98dc8d84-7e4c-46bf-a3cf-e55e51ef8907": {"doc_hash": "0bd77eae9d148a9522824a9c45fb6e0f3b4bad478828d52290068899720e9066", "ref_doc_id": "3b4da62b-3fd8-44c7-9c88-d589ff00f28c"}, "b059fe23-a328-4005-a04b-371267840166": {"doc_hash": "f717280d3b579648f915e2e2e3bd01a557b79c74892e047b5c195554e9f78f1c", "ref_doc_id": "72cd9b3f-a353-4133-b1e5-fe98326c95d8"}, "e214ebbe-a4a2-4577-a1fa-6529171c905d": {"doc_hash": "8ecc98606c375f73cac0759dc2d30f3601e0fcc2afd739442fafb31544026612", "ref_doc_id": "9cf0347a-2685-4c0f-b46e-51517d2b82a5"}, "4e335f4c-1af9-48fe-95da-4351af74a1aa": {"doc_hash": "3c118348fef64501188a5b278f8a965fbf811206f4c20bf99e550e0ac38f3f81", "ref_doc_id": "971c9ea2-0ff3-4d54-8971-a2c3dbe03caa"}, "fb7fb34c-8b57-421b-99b4-09017b849864": {"doc_hash": "9adb080aeb8cdb24e0f4acead18debd33403cb0b91fd7c24505e22b50a28ed20", "ref_doc_id": "cdf9b041-fd18-49af-9797-61ea9bc8af67"}, "3300d303-99a4-4525-a60d-d1752aeebd7a": {"doc_hash": "8b38a91476ebd3d4d1ea8a844bd141f8b89418939bbb4d626bf98de714ac5395", "ref_doc_id": "226c7a90-ff29-42bd-8d04-9f3bd74d1ea4"}, "95bd1c65-a23b-4c37-b038-e94823a5f0a4": {"doc_hash": "5bc55f8bf4b8e51246ad826930b138fd98ddcf43b86ebfe696f0483e1521d0f4", "ref_doc_id": "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5"}, "836e5ae1-10df-4b38-958e-4f88496998f1": {"doc_hash": "64c24835d888bd0372e15336f812275f806d771f9e1f5fab9bc1462de77022c3", "ref_doc_id": "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5"}, "4e80bd3b-2ecd-46cd-838a-292278252d37": {"doc_hash": "bf77aac8fa1cc651246d0af58021e46ff0e409eab39551e37c262991da926a38", "ref_doc_id": "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5"}, "42988a74-efa9-4291-bde2-f2e52df2387f": {"doc_hash": "0ea14251b3211fc52800013601b0d3c7f64d4a59a7c06be0d189a8973cb1a993", "ref_doc_id": "bcc2ee4a-a361-4d0e-90f6-481b167db6d7"}, "fe365372-01e6-489b-a56f-7dc9bc921877": {"doc_hash": "8de13bd4f2787cb7053dbc98c45cea18a27bcc9babb8aa6775edb1ac6ed0c65d", "ref_doc_id": "0c87d542-9d83-4107-9294-e3f7606fc9ab"}, "3bdd2053-dd18-429e-afb5-8da4ceec0311": {"doc_hash": "ffdb67c4beb11c397a1da77145e8fe190d4258e86940b70531a418754a6a7d77", "ref_doc_id": "9612f788-1e71-4259-9bb3-8cbfffe0238e"}, "89636cfc-5d09-437b-9b35-c8f89bbd3b4b": {"doc_hash": "970c0d15e8ed4d8feed02782bd9e3676359e3a8a41aec7b196c46993f33175a3", "ref_doc_id": "a28b6264-bab4-42a5-be83-1ca8a5d9dda7"}, "d53ace1a-ac06-4d66-9474-ffc7a89f4aca": {"doc_hash": "23d81c93e46cf261a8bdf1f0e1fa5b22885aa0c9f13b1ba92ed902b8e1d3e7be", "ref_doc_id": "df9c9841-0cb0-417c-8738-1a527502e9f6"}, "645024ce-9b96-45ff-b5c9-3c509c1e32cf": {"doc_hash": "c32a975e80382927b2c668efa0710bb840cb094b60a04bb0253532f9a079414b", "ref_doc_id": "9ade8cfe-c14b-4766-b7b9-7da722b55530"}, "4810a3a5-ffb3-4f38-ab8e-9a45a5df5d7e": {"doc_hash": "a3b8226bb10ec2f14456a6e8c4ff9642002e6c903a8f1b43223da5e0ad66c4de", "ref_doc_id": "0a2f9da6-ebbf-47d0-9f32-21325ea11075"}, "d4eb84e9-227d-4603-9d86-56ffbde4e943": {"doc_hash": "35011ff6073cf894e1c80be1c5e74f06c7fa9d9e58565062ee7d15e207ffc940", "ref_doc_id": "4c0fc807-c715-426f-9647-b320bdeda630"}, "0e858d36-dd07-4ecd-9906-9fde5dd3c8d2": {"doc_hash": "4fa54ac4c7d64cc6f0c3d14b07af488eb3b56e705cc280b211756501600bd4c2", "ref_doc_id": "4c0fc807-c715-426f-9647-b320bdeda630"}, "0501062b-74f1-4602-9fd4-c970c0cff831": {"doc_hash": "cf0824a081550d6e829b85e445f88e4c561d248d0e08e37f849b24004a974940", "ref_doc_id": "24deb67b-4795-4a2a-9203-89b7fefd583e"}, "5382996a-1111-4639-a45a-20d037308d79": {"doc_hash": "e0df3046bfd3d9765ecc8fe57afaa9c5105aeae13004fd716b04c84fd0cb70f9", "ref_doc_id": "dd9ab116-ba7f-4b46-b567-115a805d403d"}, "f7fdca32-024d-436c-ab9d-87a0c8f56200": {"doc_hash": "f1ad3c517051ac5c8d07c924ee098988c890fe54a146dbc4049a14be7b69c734", "ref_doc_id": "dd9ab116-ba7f-4b46-b567-115a805d403d"}, "cf0a99d4-e2a4-4989-8f57-b3cf3d48286b": {"doc_hash": "1e4cf898fdc6a5f545ec3e3290dc0c29c384c5fb0116eb54be3f920cdde890c3", "ref_doc_id": "900f35db-fc9d-4fd1-bf21-8f466e18d253"}, "0aec158f-2af8-4b02-8188-33bb287bba3e": {"doc_hash": "c93249fb0fffb7a6ed51e35d801dee8d72322be9a1741b2f85a66ba93f2f53c2", "ref_doc_id": "33555ccf-3b2c-4463-a1ba-dbf354d2b7d5"}, "ae9138c8-f473-4bc8-aa3d-0b84845c0bd5": {"doc_hash": "5162f2fe3f58a0fbc892b2aac969b638850ed968bb44f7fb41b625694cf378a7", "ref_doc_id": "33555ccf-3b2c-4463-a1ba-dbf354d2b7d5"}, "de84f76a-2bfb-4f8c-a018-25270138caaa": {"doc_hash": "193c1d933e02a8c329b05bbe261cf2715022bb7647246c241b23db1e6670b71f", "ref_doc_id": "15740998-e241-46ad-a84e-948b02bf809b"}, "094946a2-d061-41ee-ab11-a29b2e6c2f68": {"doc_hash": "4b633899e2590e17058123c1260020c92ce29c23dd10e66014301e73ec5559da", "ref_doc_id": "1162a76f-1d11-4771-8655-e8c108f4cf1c"}, "8780dc0c-b52d-483f-bb05-a37c44ab8431": {"doc_hash": "9ab4f5fa13de43aa01e8e98cf97881be4740d2da39e7a77b3e92c8f98529626f", "ref_doc_id": "2d698339-0250-40db-b79d-28caddeab50d"}, "4e26d892-407e-4cad-b511-79efd956a7f5": {"doc_hash": "cdfa1832819688714bb22b7460c5044ac4d6fd81c9cd5c84a8d7d196865be247", "ref_doc_id": "2d698339-0250-40db-b79d-28caddeab50d"}, "901c1fca-489b-4f9a-b0aa-c8ac24b67a31": {"doc_hash": "310cb9c9700bad039fc899761cf8b523328ac7a981f5a96e4daa2b61756fdb8c", "ref_doc_id": "a595946b-5120-4796-a7db-4ec3b76aece2"}, "e9a94b93-dc0d-4a09-9e18-fe7888463686": {"doc_hash": "c6edd6117ce604538f5b90cf91a561fd40047dff060893cb2c4b775a4e1b0b23", "ref_doc_id": "59318c71-e193-4755-b4c9-a266d0c7c5a8"}, "520d449b-f0a4-409c-9a1b-9153f4148ab6": {"doc_hash": "eecb1bf58f2692a0954b1244c012d744991a18782705a033b75c268ef6572f9c", "ref_doc_id": "59318c71-e193-4755-b4c9-a266d0c7c5a8"}, "2d4380c8-a715-4864-ab4c-d5527b17ed6e": {"doc_hash": "0c91dff23226ac8da6f2528e734e9ae11ebd20f17a22f6f7a2a66fda0a105be6", "ref_doc_id": "5ddb59b9-8a4c-4a1a-8ffb-9834a8a136d4"}, "0555ff75-23f0-4fa0-b4cc-d44c5b7d14b9": {"doc_hash": "57b5252df349e515b993d5f9f2bc4d52be045cc1cdc8d93f25d16db17d630f7b", "ref_doc_id": "5ddb59b9-8a4c-4a1a-8ffb-9834a8a136d4"}, "108dee8e-0588-4e37-8379-1b1ac801fdfa": {"doc_hash": "ba7361dfb63d5603de713023bfdffd0a22bb90ebd87e56a755eeaec42887a589", "ref_doc_id": "c0a4f260-a296-4bbd-bd6d-4e4df8b3bee0"}, "9083decc-bb9a-4c29-bba7-0d11ead008f6": {"doc_hash": "5008d9ef46588873c34b147d67c40d0914d112fcd98bda22994f2a3698f5cb45", "ref_doc_id": "c0a4f260-a296-4bbd-bd6d-4e4df8b3bee0"}, "0d10250a-8c80-4a6f-bf9a-0a0dc4cfc7c3": {"doc_hash": "aab52cbf8104a02530393f4dda8e2ac44de463ad126549e818e58cb9398fd6b3", "ref_doc_id": "a7991df4-901e-4302-bb9d-11fdadb86650"}, "d86640df-d07c-423c-ae35-8ebf441d4cd2": {"doc_hash": "3a5633e06cddf25e3a7dd8222c15d02ee3b1cf22fec427ce4eb66ef5f147b5e2", "ref_doc_id": "e74b3d2e-2beb-40fe-9ef0-3fc75e9daaa0"}, "be66680d-0829-4aee-948f-cd0c54c959d5": {"doc_hash": "ba11b8822dd9894d21761443a74f85b59a147fe8445eb0f5ac25702247a72bbe", "ref_doc_id": "cb6a87f4-e6f2-45e0-82d2-2066ec7c2771"}, "80cd099c-ed94-4564-ae7d-6a2987abf609": {"doc_hash": "4838b932eb7a3b9085be5d0ed73492e85007709ee2b51a50e0c9b325326943a0", "ref_doc_id": "abbb03a1-5cbe-423e-8b58-9374443c1584"}, "8e5ddb88-7d7f-455a-a1ef-81ff31069171": {"doc_hash": "37eb898dd5332c4a717f58f2eb12c9a4241b1bf2ea358cbdd6d3bc477d215cdc", "ref_doc_id": "abbb03a1-5cbe-423e-8b58-9374443c1584"}, "ac4c69b5-04e8-45de-a61b-f07ea0b89e1c": {"doc_hash": "038b1982bfc132210e949f65df14f81885092b44929bab14feb17753c0aae4c8", "ref_doc_id": "549c635a-fff8-47fa-8d18-b71cb0b716c9"}, "8be8af7a-2e77-4b7d-bdd8-0ba9e9712dba": {"doc_hash": "3ff0f1e46b3be82804cb3c1392a33d816b9c66b58489b84a446743c92c47bb76", "ref_doc_id": "3809e1ba-81f3-4c4b-8903-9447067af8ff"}, "23ea0f97-2a4b-4130-9df7-6a01b247612b": {"doc_hash": "a17577537cdf41df0c576b75099f449cc689c467e04893b963fc64b9672cde54", "ref_doc_id": "1c132b5b-24ec-47c6-a405-10017b6c0274"}, "2711496c-7b6c-401d-8066-4c0f0f0630ec": {"doc_hash": "667e1bc5d06a4ecc94360ced63b0f0ec3a9f1b5547474cb94e7550e80acff60c", "ref_doc_id": "8c1c9973-9d55-471b-9863-a6cea419fef8"}, "f4cb3e47-1364-40ed-8dc2-bbe11b000d79": {"doc_hash": "6a10e7d2fe0c37128d6a97e896cf392811f4263c3794289b193ff650ca6aee60", "ref_doc_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9"}, "5cd979f0-c9af-43ba-a3c6-c30cb89e6a19": {"doc_hash": "7f1aebee73cc8bf25e7764692c7be875235f867e83e37509d0c424565a76a89e", "ref_doc_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9"}, "3e6f12d6-3cc1-4de8-afed-1404ff93e418": {"doc_hash": "712474781b4e5962142c9d36d27a7bcdb32dc03947487916abdee53b93c6e2d6", "ref_doc_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9"}, "0f8026a5-5204-4eb9-9ff4-46e8870ca444": {"doc_hash": "8a99c7ebc46d5bb93580f4fc0fed78921cefc85ea944ed12a352e1491427a353", "ref_doc_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9"}, "8c1374ab-fec3-4d6c-9f9a-2aa55e724243": {"doc_hash": "829517a8b9ab22a06255c3b67f30beb88355d0454aa773dbbc9eb0d42c6e0ea9", "ref_doc_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9"}, "6adbd397-4b40-4df8-b866-f3c46272092a": {"doc_hash": "16d7bf4cf435eaf5d3d5f598f0691a855a36892ae614b6c1620d5179a11b3daa", "ref_doc_id": "cd11a346-f5e8-4319-b698-af78eb861136"}, "bd735b8d-8ddc-470b-bacd-6a6630252f05": {"doc_hash": "f3c02688653dbc497abd0720eb522a5179ea75ec1354ec0f698ff59b95804b18", "ref_doc_id": "0130dfa3-a254-4f8f-adfa-d021ec30798c"}, "001ccbe4-a7ba-4511-a35f-0aeadae961a5": {"doc_hash": "96eb80d55de28f0c17a7cf4db1b66c012fec334731abb53a4ab8649826b4eb80", "ref_doc_id": "dfb1163f-e732-40ed-9253-f227f0dff022"}, "1c70b554-a085-491d-a67b-14ef84bd4425": {"doc_hash": "57a08dff5ecc800add2f3a1cfce6c8ba836dafe3a6f8d7488be8f368f2a0b6da", "ref_doc_id": "6925cf83-6ee8-4cc9-96cb-b69c650af276"}, "8ba77af6-6aff-4b64-9206-8895a271a929": {"doc_hash": "89c76b59aefd28837fe3e3c2583962b9f64b272b2656074ce86ae854f708f923", "ref_doc_id": "a36974b6-77f6-4c00-b9af-e56ea621bfb3"}, "b37a6fc9-c135-44b1-8c3c-df0bac34ad8b": {"doc_hash": "155943d193e51562f35841760a98cf941a06b61768bae1902677e901f68a6fb3", "ref_doc_id": "a36974b6-77f6-4c00-b9af-e56ea621bfb3"}, "342e0ad6-4569-49c5-906e-1fe303e5f668": {"doc_hash": "56ab0b6ebad03c6fce5b1c4739616fbfd34beac1e2fa80b521b7431a2866798c", "ref_doc_id": "a36974b6-77f6-4c00-b9af-e56ea621bfb3"}, "6ef7a3d8-859e-4536-b0cb-87da8a690803": {"doc_hash": "a216ee3750a95d770941b83334ff5a00e96131071d56f03ad03923a4bdb29904", "ref_doc_id": "dbeb5079-4654-4901-98e2-a47c333d1066"}, "ccba7ccf-a07c-4c2a-b833-d714a143b17b": {"doc_hash": "5fb241d2bf189706ccf33cf5c40c402a34d0f36a015b367484c7c551b7d5b8dc", "ref_doc_id": "cc7cdf4d-72a7-40e4-a387-416a320e7a77"}, "8cc7c4ee-2c73-4664-9226-71600afb3e2a": {"doc_hash": "a7f5819aedba3827819b3897fd3fa422bcd7dd38259fe15280097a3a703f4595", "ref_doc_id": "cc7cdf4d-72a7-40e4-a387-416a320e7a77"}, "af3a62d1-20ab-41c3-86fe-b6a3859b448a": {"doc_hash": "e2c9acbfdc0384496a3c470044a8f6a7326379ade368a14dad998a9f11973205", "ref_doc_id": "fc10c197-ca0c-454c-9715-847fc89f5753"}, "d9620c25-abf4-4b19-8c60-e2be6bd035f2": {"doc_hash": "26db22516b5b6453ad0810c26eed465fedc01146a7a302c834e7a35df2837b50", "ref_doc_id": "92196516-0ccb-43f6-9292-15a8044b5b4b"}, "87bcc8c2-7705-4f82-83ee-83c7400b0a60": {"doc_hash": "ef3c814cacc60c63d88987e3387687029b747cf8e6866f8d6aa48485d8fa52e9", "ref_doc_id": "92196516-0ccb-43f6-9292-15a8044b5b4b"}, "7eea075b-2dd2-44de-9f85-ad7e42a6a039": {"doc_hash": "4366ee6c864bcd7b0e72a90ef3e9a96cbf21b28f967709a586a3d37ab9af9468", "ref_doc_id": "3234fd39-7653-4a79-a5c1-53a2c59f3b13"}, "a0259824-9d8b-45eb-80c2-5e501c61e451": {"doc_hash": "f52e14002640c72e531b9e309699d35117207fc39272d6542b4afcc30d3043bc", "ref_doc_id": "3234fd39-7653-4a79-a5c1-53a2c59f3b13"}, "21de50ed-52ad-4757-811b-a011348864bb": {"doc_hash": "fd257c0298745164c70e819d56e18ba54d53852e7a2346fdccb02e3cf5bedb73", "ref_doc_id": "5dc919c1-7b45-4d76-a07b-b4fed3e88eb4"}, "8eb10aee-8654-4adf-9274-f9e6bc2c095d": {"doc_hash": "bfc7d8305c5df532681a5f209c86b11df4c960f60e3d8187bb7128adf3dd22f4", "ref_doc_id": "5dc919c1-7b45-4d76-a07b-b4fed3e88eb4"}, "c40f5dad-f3b7-40e3-8a77-797bb40e0fb2": {"doc_hash": "6dcc06742cd26418bce0469b8799b095856f1b98ba2496ea173fceaa8370da5f", "ref_doc_id": "b879d8cd-e884-4787-b79d-18819ec7477d"}, "09861dcc-9906-4806-b515-15c80ef781b6": {"doc_hash": "bea505a7b6391a0fe95c17246b188037c11d7606da5af5b91a06834738c1bee3", "ref_doc_id": "da5f1c01-d813-43ae-9217-2d0346f39ca4"}, "36e58545-d8a8-43df-90e4-ffb3c6e6457b": {"doc_hash": "23dace1c8d9f389a1087415d9d8570c15d877e86e18f2f9dc7db3636e985f30d", "ref_doc_id": "da5f1c01-d813-43ae-9217-2d0346f39ca4"}, "f8150398-a7b7-46b3-bcf5-8a2783bb228d": {"doc_hash": "c24ac578322123ba75fdc464a852228a4f42ca6e2c55cf5a7d0417c7d97f9caf", "ref_doc_id": "55e6e486-bc6e-4073-bba5-89639e708e8c"}, "236bba97-317c-442d-b369-ddb085356bee": {"doc_hash": "258aa52cd6000aedff5068caebf0779d7a96f1a54e289c8ae404fda042c703bb", "ref_doc_id": "55e6e486-bc6e-4073-bba5-89639e708e8c"}, "8f1c72ae-ed18-466f-ae6d-55600385b60e": {"doc_hash": "2e0e6e31b93d1e9fa7679a9ab4c966c2ab037086979fe0e883007808d11e047b", "ref_doc_id": "c5c97630-d99a-4175-bc31-b30aa39d8535"}, "b595587e-a0a9-4374-bfd0-0fd465e80743": {"doc_hash": "86671d8f9170bc34104c15e8093327a165a4f4a4db3130924482d4a639f3fca7", "ref_doc_id": "c5c97630-d99a-4175-bc31-b30aa39d8535"}, "1ef81d84-76c3-49b6-a697-0f1aacb7b67a": {"doc_hash": "0b712a610351980e1092e7de01428e7beddb1158f551c2f46d39d6d36b5a84b8", "ref_doc_id": "a73bcb04-a9cd-4978-b5f1-1fc1fe019048"}, "9ac10e17-e2d0-4606-9d34-d55dd28e076d": {"doc_hash": "4681b37f8ae24808cceba4aa2410c44e074ccfd1ea478e0a2468a7fd243b9655", "ref_doc_id": "a73bcb04-a9cd-4978-b5f1-1fc1fe019048"}, "b030a5f8-ac32-4683-a6f6-d1442e6f1aa5": {"doc_hash": "aea3993e4b9e4d23da2a60c9bae949e1d1ae2a71f06a6267dc42d588e436b0b5", "ref_doc_id": "67b87faa-cc33-4f08-863a-92ed7b442b87"}, "c73b9439-1f3e-42d3-8b50-f4958199bfde": {"doc_hash": "a328ab9579c6152e473e5d6746a4a9f793cfbf8729d5abb0b62e788a6193f7b3", "ref_doc_id": "ffc24877-e43c-40fc-893c-aa6f9a1c917f"}, "9c9e7d52-0514-4f79-b5ab-586cbde1f5d3": {"doc_hash": "b0b509a6e97f668c72f1cb812ee5767c716fce75389d2224afb2256faad57dc7", "ref_doc_id": "ffc24877-e43c-40fc-893c-aa6f9a1c917f"}, "8c2d0994-4bd0-41f7-ad53-31422c268a09": {"doc_hash": "39ef86a0356dbe4e8df049597a04b4e9e390ed1f83758a1f656379b3268e55ef", "ref_doc_id": "037cd110-ee3a-4926-8674-4817122f5327"}, "bb5ea7d5-a2af-45cc-83d0-ea2ac11f1fe7": {"doc_hash": "7a8d6e1cd3fc82394d8cf282c813a37114ea7228cacf42e811fd7fcbfc3d530b", "ref_doc_id": "1e3e953f-e419-4782-8eae-dc40d3bb8f26"}, "0497ec8e-ad55-4ba1-a0e2-8c8fe4429b1f": {"doc_hash": "bae8eee3af9ea0c36f909ae05f5896fb0a9810b65ef70b96c8e0f78b6a8d016f", "ref_doc_id": "26316f62-a24c-46fb-afff-b354925bc1c8"}, "7129cae3-93a6-45be-a1ab-0104a6c46987": {"doc_hash": "8dc2bd98400074cc7f3c77c38f95c220e4b444b21114c5840d88d1d95833ac7d", "ref_doc_id": "12b644db-be9e-45c7-91f3-d6a67efa7afa"}, "c92b16b4-8633-4dd4-bce0-f8eceef1ebad": {"doc_hash": "356da5d945539156d020a22bf7cba63dd3f319115d48e53349e20cdce644cb52", "ref_doc_id": "12b644db-be9e-45c7-91f3-d6a67efa7afa"}, "0ced9a6f-0b73-4247-9619-dfc9de229cc0": {"doc_hash": "db1ea487b08ed89230eaedc14d65f5cd0d1a0c4af1f5936b6dbf189c29cca21b", "ref_doc_id": "af725450-3369-4ed4-8f24-a8c80adb25dd"}, "9187459a-bfae-4caa-b74e-815145561e7b": {"doc_hash": "c228dbc11f330b589308de19456f9c94534c223555d4ee9abd0338448382ce04", "ref_doc_id": "af725450-3369-4ed4-8f24-a8c80adb25dd"}, "832ae626-c45b-49f1-80bb-218120a7f0d9": {"doc_hash": "d87ee38eda03453f97e3c0d7844103d068eceaba2127d5b89d0e2a86a764613f", "ref_doc_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb"}, "c0b99e15-2625-4f6d-84cd-1c60de929e27": {"doc_hash": "ceddc0641a72e15a4913739325394fa9641e8dea04321f0fef6580f10ce33cfe", "ref_doc_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb"}, "a10f5859-717a-4f94-a2be-5771da77b16e": {"doc_hash": "efad2e4da3391513f1114298c42008fbd33ee9ebd840e24a38cb4bfd1348b533", "ref_doc_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb"}, "6b21224f-cfc0-4f90-a1c0-8076867d148e": {"doc_hash": "af2dd6a12448c57e9bea7adc2fc4e72e3195ed578ad13582669db26c222845bc", "ref_doc_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb"}, "6ec5bc0c-2754-49b8-b738-fc4edcb04697": {"doc_hash": "42beaeaed2cdeb3b2dcc33f06cb1f33f7ddc036d5971eaecee31b80cf1f55e31", "ref_doc_id": "a023287c-0fb4-4b10-a1c4-9a94b7d4ba4f"}, "6b259adb-188d-48a3-98ca-8689991857c2": {"doc_hash": "4596d7d975941ee4861fa2c66fb1197a37bdeacef5e283174462de91e1f35621", "ref_doc_id": "ba222a86-8474-4ab0-b55f-65ef9ef4acfb"}, "83ca4193-6149-4f16-9344-1f144888073e": {"doc_hash": "7952680742b7d5f9a205224db0d4cb4a8072856007a47db78969c34eed532fd2", "ref_doc_id": "ba222a86-8474-4ab0-b55f-65ef9ef4acfb"}}, "docstore/data": {"6f3cbbc8-d0b2-47cb-bd1f-d1cc0cff6fb2": {"__data__": {"id_": "6f3cbbc8-d0b2-47cb-bd1f-d1cc0cff6fb2", "embedding": null, "metadata": {"title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "citationCount": 2015, "openAccessPdf": null, "authors": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "E. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "externalIds": {"DBLP": "conf/nips/Wei0SBIXCLZ22", "ArXiv": "2201.11903", "CorpusId": 246411621}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d4e69ee-65e2-4301-b7b9-a248a086615a", "node_type": null, "metadata": {"title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "citationCount": 2015, "openAccessPdf": null, "authors": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "E. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "externalIds": {"DBLP": "conf/nips/Wei0SBIXCLZ22", "ArXiv": "2201.11903", "CorpusId": 246411621}}, "hash": "a2a3847e075b307bb1e0e63b67843747053c2f3a433e95ee1d5367255cb05137"}}, "hash": "a2a3847e075b307bb1e0e63b67843747053c2f3a433e95ee1d5367255cb05137", "text": "Chain of Thought Prompting Elicits Reasoning in Large Language Models We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "start_char_idx": 0, "end_char_idx": 933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0cbdf8b-55a1-4c36-ae9b-3bb12616f62f": {"__data__": {"id_": "e0cbdf8b-55a1-4c36-ae9b-3bb12616f62f", "embedding": null, "metadata": {"title": "Large Language Models are Zero-Shot Reasoners", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "citationCount": 841, "openAccessPdf": null, "authors": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "externalIds": {"DBLP": "journals/corr/abs-2205-11916", "ArXiv": "2205.11916", "CorpusId": 249017743}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6dbe332-1721-40e5-931f-9ddf24e63c8c", "node_type": null, "metadata": {"title": "Large Language Models are Zero-Shot Reasoners", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "citationCount": 841, "openAccessPdf": null, "authors": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "externalIds": {"DBLP": "journals/corr/abs-2205-11916", "ArXiv": "2205.11916", "CorpusId": 249017743}}, "hash": "b522584dbd9497b883f5f21e82307157de6ee43a30ea9d85389ed1c29cc2d2cf"}, "3": {"node_id": "0be527d9-32a9-4b5c-ae26-d0e0f10b0824", "node_type": null, "metadata": {"title": "Large Language Models are Zero-Shot Reasoners", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "citationCount": 841, "openAccessPdf": null, "authors": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "externalIds": {"DBLP": "journals/corr/abs-2205-11916", "ArXiv": "2205.11916", "CorpusId": 249017743}}, "hash": "b0db24963103cf971893069225d076cf7a8a4cc889c98e84bc76d9462feedec6"}}, "hash": "22ee726759b2c60b60c0a1f8d3b9537aa856383470e7e9e34200352d44f89a24", "text": "Large Language Models are Zero-Shot Reasoners Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements", "start_char_idx": 0, "end_char_idx": 1343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0be527d9-32a9-4b5c-ae26-d0e0f10b0824": {"__data__": {"id_": "0be527d9-32a9-4b5c-ae26-d0e0f10b0824", "embedding": null, "metadata": {"title": "Large Language Models are Zero-Shot Reasoners", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "citationCount": 841, "openAccessPdf": null, "authors": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "externalIds": {"DBLP": "journals/corr/abs-2205-11916", "ArXiv": "2205.11916", "CorpusId": 249017743}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6dbe332-1721-40e5-931f-9ddf24e63c8c", "node_type": null, "metadata": {"title": "Large Language Models are Zero-Shot Reasoners", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "citationCount": 841, "openAccessPdf": null, "authors": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "externalIds": {"DBLP": "journals/corr/abs-2205-11916", "ArXiv": "2205.11916", "CorpusId": 249017743}}, "hash": "b522584dbd9497b883f5f21e82307157de6ee43a30ea9d85389ed1c29cc2d2cf"}, "2": {"node_id": "e0cbdf8b-55a1-4c36-ae9b-3bb12616f62f", "node_type": null, "metadata": {"title": "Large Language Models are Zero-Shot Reasoners", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "citationCount": 841, "openAccessPdf": null, "authors": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "externalIds": {"DBLP": "journals/corr/abs-2205-11916", "ArXiv": "2205.11916", "CorpusId": 249017743}}, "hash": "22ee726759b2c60b60c0a1f8d3b9537aa856383470e7e9e34200352d44f89a24"}}, "hash": "b0db24963103cf971893069225d076cf7a8a4cc889c98e84bc76d9462feedec6", "text": "model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.", "start_char_idx": 1272, "end_char_idx": 1960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a8aefbc-7226-4adc-a9a8-76fdb2621bfd": {"__data__": {"id_": "0a8aefbc-7226-4adc-a9a8-76fdb2621bfd", "embedding": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "320609a4627a0fe0e8a4ffbc9507fb25eb9802f5d437d9ea6554b4f4861f7bc3"}, "3": {"node_id": "fff9973f-f7ce-46c6-87a4-6b2486cc976b", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "8b63ff0114e0729e8bc6d32cdc934c0e1995c1674397a397b41c3be1d9593b91"}}, "hash": "65a5ec6d7b31f9903d6d77e6a3c87ba0650d784d33d9001e3e9ba1ffc02fdb7e", "text": "Evaluating Large Language Models Trained on Code We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new", "start_char_idx": 0, "end_char_idx": 270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fff9973f-f7ce-46c6-87a4-6b2486cc976b": {"__data__": {"id_": "fff9973f-f7ce-46c6-87a4-6b2486cc976b", "embedding": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "320609a4627a0fe0e8a4ffbc9507fb25eb9802f5d437d9ea6554b4f4861f7bc3"}, "2": {"node_id": "0a8aefbc-7226-4adc-a9a8-76fdb2621bfd", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "65a5ec6d7b31f9903d6d77e6a3c87ba0650d784d33d9001e3e9ba1ffc02fdb7e"}, "3": {"node_id": "5de093c7-cd0f-4c81-b60e-082fde23e8a9", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "6037a2e1255b16220c25cb43813443ee66dbd7edef141eafeda1ea52008ecb59"}}, "hash": "8b63ff0114e0729e8bc6d32cdc934c0e1995c1674397a397b41c3be1d9593b91", "text": "production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J", "start_char_idx": 211, "end_char_idx": 456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5de093c7-cd0f-4c81-b60e-082fde23e8a9": {"__data__": {"id_": "5de093c7-cd0f-4c81-b60e-082fde23e8a9", "embedding": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "320609a4627a0fe0e8a4ffbc9507fb25eb9802f5d437d9ea6554b4f4861f7bc3"}, "2": {"node_id": "fff9973f-f7ce-46c6-87a4-6b2486cc976b", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "8b63ff0114e0729e8bc6d32cdc934c0e1995c1674397a397b41c3be1d9593b91"}, "3": {"node_id": "69a84425-eb34-4c70-933d-f272791884bb", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "64ed9869c2a09e1f64cf3ce24d0ddce8306b2fd2a418439ca86c443ed1e87701"}}, "hash": "6037a2e1255b16220c25cb43813443ee66dbd7edef141eafeda1ea52008ecb59", "text": "of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our", "start_char_idx": 477, "end_char_idx": 730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "69a84425-eb34-4c70-933d-f272791884bb": {"__data__": {"id_": "69a84425-eb34-4c70-933d-f272791884bb", "embedding": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "320609a4627a0fe0e8a4ffbc9507fb25eb9802f5d437d9ea6554b4f4861f7bc3"}, "2": {"node_id": "5de093c7-cd0f-4c81-b60e-082fde23e8a9", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "6037a2e1255b16220c25cb43813443ee66dbd7edef141eafeda1ea52008ecb59"}, "3": {"node_id": "102420fa-1de2-49c5-bec9-e11dbe9754b9", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "4a8ef4b9516a44874c875bd7ba83fbb55ca417da86bc0d6e258d24d2102e8975"}}, "hash": "64ed9869c2a09e1f64cf3ce24d0ddce8306b2fd2a418439ca86c443ed1e87701", "text": "to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss", "start_char_idx": 719, "end_char_idx": 1016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "102420fa-1de2-49c5-bec9-e11dbe9754b9": {"__data__": {"id_": "102420fa-1de2-49c5-bec9-e11dbe9754b9", "embedding": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e964383-21ab-4cdd-ab29-7f4c25ef61e0", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "320609a4627a0fe0e8a4ffbc9507fb25eb9802f5d437d9ea6554b4f4861f7bc3"}, "2": {"node_id": "69a84425-eb34-4c70-933d-f272791884bb", "node_type": null, "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}, "hash": "64ed9869c2a09e1f64cf3ce24d0ddce8306b2fd2a418439ca86c443ed1e87701"}}, "hash": "4a8ef4b9516a44874c875bd7ba83fbb55ca417da86bc0d6e258d24d2102e8975", "text": "long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "start_char_idx": 981, "end_char_idx": 1192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "54a5d4ba-ea89-4ea8-8f4b-edae0669a2b9": {"__data__": {"id_": "54a5d4ba-ea89-4ea8-8f4b-edae0669a2b9", "embedding": null, "metadata": {"title": "Emergent Abilities of Large Language Models", "venue": "Trans. Mach. Learn. Res.", "year": 2022, "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "citationCount": 809, "openAccessPdf": null, "authors": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "E. Chi", "Tatsunori Hashimoto", "Oriol Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "externalIds": {"DBLP": "journals/corr/abs-2206-07682", "ArXiv": "2206.07682", "DOI": "10.48550/arXiv.2206.07682", "CorpusId": 249674500}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "225d23c1-c920-47f7-82a5-b067349b8523", "node_type": null, "metadata": {"title": "Emergent Abilities of Large Language Models", "venue": "Trans. Mach. Learn. Res.", "year": 2022, "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "citationCount": 809, "openAccessPdf": null, "authors": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "E. Chi", "Tatsunori Hashimoto", "Oriol Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "externalIds": {"DBLP": "journals/corr/abs-2206-07682", "ArXiv": "2206.07682", "DOI": "10.48550/arXiv.2206.07682", "CorpusId": 249674500}}, "hash": "9875850ae12d92af4b4b2d11841f4589a85544719af291c419c93dea5b37ec65"}}, "hash": "9875850ae12d92af4b4b2d11841f4589a85544719af291c419c93dea5b37ec65", "text": "Emergent Abilities of Large Language Models Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "start_char_idx": 0, "end_char_idx": 646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7eb6de56-76ee-4ec1-8ecb-9e18ed8c01e4": {"__data__": {"id_": "7eb6de56-76ee-4ec1-8ecb-9e18ed8c01e4", "embedding": null, "metadata": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "citationCount": 1291, "openAccessPdf": null, "authors": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "externalIds": {"DBLP": "conf/iclr/HuSWALWWC22", "ArXiv": "2106.09685", "CorpusId": 235458009}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a7e6612-65b2-48e3-84f4-09ef65816354", "node_type": null, "metadata": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "citationCount": 1291, "openAccessPdf": null, "authors": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "externalIds": {"DBLP": "conf/iclr/HuSWALWWC22", "ArXiv": "2106.09685", "CorpusId": 235458009}}, "hash": "8a7bea79646fecca0940b6def498e95559371f4c5f8fd789b326a5cb5a075d58"}, "3": {"node_id": "65f57cbe-5504-4bc8-a463-49c5bf40351d", "node_type": null, "metadata": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "citationCount": 1291, "openAccessPdf": null, "authors": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "externalIds": {"DBLP": "conf/iclr/HuSWALWWC22", "ArXiv": "2106.09685", "CorpusId": 235458009}}, "hash": "2bcc9a54c3447535f57e10bed5d2190882b34fededcfc2c7e0f9138921ecb806"}}, "hash": "4ec6b7c391a0f247978b1e355bf08648177476e047c9b1e256ef0db8ff3ef655", "text": "LoRA: Low-Rank Adaptation of Large Language Models An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model", "start_char_idx": 0, "end_char_idx": 1368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "65f57cbe-5504-4bc8-a463-49c5bf40351d": {"__data__": {"id_": "65f57cbe-5504-4bc8-a463-49c5bf40351d", "embedding": null, "metadata": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "citationCount": 1291, "openAccessPdf": null, "authors": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "externalIds": {"DBLP": "conf/iclr/HuSWALWWC22", "ArXiv": "2106.09685", "CorpusId": 235458009}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a7e6612-65b2-48e3-84f4-09ef65816354", "node_type": null, "metadata": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "citationCount": 1291, "openAccessPdf": null, "authors": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "externalIds": {"DBLP": "conf/iclr/HuSWALWWC22", "ArXiv": "2106.09685", "CorpusId": 235458009}}, "hash": "8a7bea79646fecca0940b6def498e95559371f4c5f8fd789b326a5cb5a075d58"}, "2": {"node_id": "7eb6de56-76ee-4ec1-8ecb-9e18ed8c01e4", "node_type": null, "metadata": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "citationCount": 1291, "openAccessPdf": null, "authors": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "externalIds": {"DBLP": "conf/iclr/HuSWALWWC22", "ArXiv": "2106.09685", "CorpusId": 235458009}}, "hash": "4ec6b7c391a0f247978b1e355bf08648177476e047c9b1e256ef0db8ff3ef655"}}, "hash": "2bcc9a54c3447535f57e10bed5d2190882b34fededcfc2c7e0f9138921ecb806", "text": "the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "start_char_idx": 1283, "end_char_idx": 1450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "698ea218-5b80-4962-9760-6a9e446e532c": {"__data__": {"id_": "698ea218-5b80-4962-9760-6a9e446e532c", "embedding": null, "metadata": {"title": "Training Compute-Optimal Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "citationCount": 725, "openAccessPdf": null, "authors": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "Oriol Vinyals", "L. Sifre"], "externalIds": {"ArXiv": "2203.15556", "DBLP": "journals/corr/abs-2203-15556", "DOI": "10.48550/arXiv.2203.15556", "CorpusId": 247778764}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d485337f-9833-41ce-8c75-207c5bb68fbb", "node_type": null, "metadata": {"title": "Training Compute-Optimal Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "citationCount": 725, "openAccessPdf": null, "authors": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "Oriol Vinyals", "L. Sifre"], "externalIds": {"ArXiv": "2203.15556", "DBLP": "journals/corr/abs-2203-15556", "DOI": "10.48550/arXiv.2203.15556", "CorpusId": 247778764}}, "hash": "45e5a34fef9b4443707860e6caf2a975551f919a98ab430e5dda26a8ffc79ca8"}, "3": {"node_id": "dc38a3b7-a08a-4349-8e7b-c5e82af532f8", "node_type": null, "metadata": {"title": "Training Compute-Optimal Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "citationCount": 725, "openAccessPdf": null, "authors": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "Oriol Vinyals", "L. Sifre"], "externalIds": {"ArXiv": "2203.15556", "DBLP": "journals/corr/abs-2203-15556", "DOI": "10.48550/arXiv.2203.15556", "CorpusId": 247778764}}, "hash": "e8a85b69369c44585b3b71552424d19ca28a56faa0355ff0314450f1a3105640"}}, "hash": "b950f2ef13fb46fa4d756f3d36a062887e6696d22ed907158c166b57a64a1dd1", "text": "Training Compute-Optimal Large Language Models We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B),", "start_char_idx": 0, "end_char_idx": 970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc38a3b7-a08a-4349-8e7b-c5e82af532f8": {"__data__": {"id_": "dc38a3b7-a08a-4349-8e7b-c5e82af532f8", "embedding": null, "metadata": {"title": "Training Compute-Optimal Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "citationCount": 725, "openAccessPdf": null, "authors": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "Oriol Vinyals", "L. Sifre"], "externalIds": {"ArXiv": "2203.15556", "DBLP": "journals/corr/abs-2203-15556", "DOI": "10.48550/arXiv.2203.15556", "CorpusId": 247778764}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d485337f-9833-41ce-8c75-207c5bb68fbb", "node_type": null, "metadata": {"title": "Training Compute-Optimal Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "citationCount": 725, "openAccessPdf": null, "authors": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "Oriol Vinyals", "L. Sifre"], "externalIds": {"ArXiv": "2203.15556", "DBLP": "journals/corr/abs-2203-15556", "DOI": "10.48550/arXiv.2203.15556", "CorpusId": 247778764}}, "hash": "45e5a34fef9b4443707860e6caf2a975551f919a98ab430e5dda26a8ffc79ca8"}, "2": {"node_id": "698ea218-5b80-4962-9760-6a9e446e532c", "node_type": null, "metadata": {"title": "Training Compute-Optimal Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "citationCount": 725, "openAccessPdf": null, "authors": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "Oriol Vinyals", "L. Sifre"], "externalIds": {"ArXiv": "2203.15556", "DBLP": "journals/corr/abs-2203-15556", "DOI": "10.48550/arXiv.2203.15556", "CorpusId": 247778764}}, "hash": "b950f2ef13fb46fa4d756f3d36a062887e6696d22ed907158c166b57a64a1dd1"}}, "hash": "e8a85b69369c44585b3b71552424d19ca28a56faa0355ff0314450f1a3105640", "text": "GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.", "start_char_idx": 938, "end_char_idx": 1330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f04ff5eb-f20a-43e3-a5f1-bca6b660d27f": {"__data__": {"id_": "f04ff5eb-f20a-43e3-a5f1-bca6b660d27f", "embedding": null, "metadata": {"title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models", "venue": "medRxiv", "year": 2022, "paperId": "cf1f26e7cbed3958b3c2870656568c299fece6e3", "citationCount": 693, "openAccessPdf": "https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000198&type=printable", "authors": ["Tiffany H. Kung", "Morgan Cheatham", "Arielle Medenilla", "Czarina Sillos", "Lorie De Leon", "Camille Elepa\u00f1o", "Maria Madriaga", "Rimel Aggabao", "Giezel Diaz-Candido", "James Maningo", "Victor Tseng"], "externalIds": {"PubMedCentral": "9931230", "DOI": "10.1371/journal.pdig.0000198", "CorpusId": 254876189, "PubMed": "36812645"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3893411-0ab4-4320-a1a3-994833771c92", "node_type": null, "metadata": {"title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models", "venue": "medRxiv", "year": 2022, "paperId": "cf1f26e7cbed3958b3c2870656568c299fece6e3", "citationCount": 693, "openAccessPdf": "https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000198&type=printable", "authors": ["Tiffany H. Kung", "Morgan Cheatham", "Arielle Medenilla", "Czarina Sillos", "Lorie De Leon", "Camille Elepa\u00f1o", "Maria Madriaga", "Rimel Aggabao", "Giezel Diaz-Candido", "James Maningo", "Victor Tseng"], "externalIds": {"PubMedCentral": "9931230", "DOI": "10.1371/journal.pdig.0000198", "CorpusId": 254876189, "PubMed": "36812645"}}, "hash": "b771790f4cd6f043d067f801758fa6f434fd49b7944fb5f5df68e35d929cb329"}}, "hash": "b771790f4cd6f043d067f801758fa6f434fd49b7944fb5f5df68e35d929cb329", "text": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.", "start_char_idx": 0, "end_char_idx": 660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c96cc3bf-4133-4d21-93db-e9cd883a77c0": {"__data__": {"id_": "c96cc3bf-4133-4d21-93db-e9cd883a77c0", "embedding": null, "metadata": {"title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "citationCount": 576, "openAccessPdf": null, "authors": ["Junnan Li", "Dongxu Li", "S. Savarese", "Steven C. H. Hoi"], "externalIds": {"DBLP": "conf/icml/0008LSH23", "ArXiv": "2301.12597", "DOI": "10.48550/arXiv.2301.12597", "CorpusId": 256390509}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b03a5fa-c92c-45fb-beb7-76322ab9b25d", "node_type": null, "metadata": {"title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "citationCount": 576, "openAccessPdf": null, "authors": ["Junnan Li", "Dongxu Li", "S. Savarese", "Steven C. H. Hoi"], "externalIds": {"DBLP": "conf/icml/0008LSH23", "ArXiv": "2301.12597", "DOI": "10.48550/arXiv.2301.12597", "CorpusId": 256390509}}, "hash": "b84b722367f44b70c8683a19f668957e011c4ba26ba7e5b93ea8d6c49fc9d246"}}, "hash": "b84b722367f44b70c8683a19f668957e011c4ba26ba7e5b93ea8d6c49fc9d246", "text": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.", "start_char_idx": 0, "end_char_idx": 1149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "39e9db19-cebf-496d-bb7b-35270bde9b97": {"__data__": {"id_": "39e9db19-cebf-496d-bb7b-35270bde9b97", "embedding": null, "metadata": {"title": "Program Synthesis with Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "citationCount": 406, "openAccessPdf": null, "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "H. Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton"], "externalIds": {"DBLP": "journals/corr/abs-2108-07732", "ArXiv": "2108.07732", "CorpusId": 237142385}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91419461-dd89-4137-950d-27a807e1e774", "node_type": null, "metadata": {"title": "Program Synthesis with Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "citationCount": 406, "openAccessPdf": null, "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "H. Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton"], "externalIds": {"DBLP": "journals/corr/abs-2108-07732", "ArXiv": "2108.07732", "CorpusId": 237142385}}, "hash": "0fed01bc10efa9a4410e67e4e4fd03401a0a01ece2670a966e9adafb24088d71"}, "3": {"node_id": "195fdb5b-3cc0-4ea7-98b7-78d650698154", "node_type": null, "metadata": {"title": "Program Synthesis with Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "citationCount": 406, "openAccessPdf": null, "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "H. Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton"], "externalIds": {"DBLP": "journals/corr/abs-2108-07732", "ArXiv": "2108.07732", "CorpusId": 237142385}}, "hash": "d3adcb1c0394bdeb24f97dad6c57c8ac8e3ee72a9b4d78f7273aa182583dde67"}}, "hash": "79918385257d62fdd9ab4dc1a97ff54b9a28bcc2ed9ced60e18f415905ceba90", "text": "Program Synthesis with Large Language Models This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's", "start_char_idx": 0, "end_char_idx": 1340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "195fdb5b-3cc0-4ea7-98b7-78d650698154": {"__data__": {"id_": "195fdb5b-3cc0-4ea7-98b7-78d650698154", "embedding": null, "metadata": {"title": "Program Synthesis with Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "citationCount": 406, "openAccessPdf": null, "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "H. Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton"], "externalIds": {"DBLP": "journals/corr/abs-2108-07732", "ArXiv": "2108.07732", "CorpusId": 237142385}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91419461-dd89-4137-950d-27a807e1e774", "node_type": null, "metadata": {"title": "Program Synthesis with Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "citationCount": 406, "openAccessPdf": null, "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "H. Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton"], "externalIds": {"DBLP": "journals/corr/abs-2108-07732", "ArXiv": "2108.07732", "CorpusId": 237142385}}, "hash": "0fed01bc10efa9a4410e67e4e4fd03401a0a01ece2670a966e9adafb24088d71"}, "2": {"node_id": "39e9db19-cebf-496d-bb7b-35270bde9b97", "node_type": null, "metadata": {"title": "Program Synthesis with Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "citationCount": 406, "openAccessPdf": null, "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "H. Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton"], "externalIds": {"DBLP": "journals/corr/abs-2108-07732", "ArXiv": "2108.07732", "CorpusId": 237142385}}, "hash": "79918385257d62fdd9ab4dc1a97ff54b9a28bcc2ed9ced60e18f415905ceba90"}}, "hash": "d3adcb1c0394bdeb24f97dad6c57c8ac8e3ee72a9b4d78f7273aa182583dde67", "text": "model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.", "start_char_idx": 1267, "end_char_idx": 1940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "97b84ea5-6bdd-4026-aeb6-465747f6a59f": {"__data__": {"id_": "97b84ea5-6bdd-4026-aeb6-465747f6a59f", "embedding": null, "metadata": {"title": "Automatic Chain of Thought Prompting in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "citationCount": 192, "openAccessPdf": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "externalIds": {"DBLP": "journals/corr/abs-2210-03493", "ArXiv": "2210.03493", "DOI": "10.48550/arXiv.2210.03493", "CorpusId": 252762275}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24f98f71-2d40-48bc-8c24-589f8a059ec0", "node_type": null, "metadata": {"title": "Automatic Chain of Thought Prompting in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "citationCount": 192, "openAccessPdf": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "externalIds": {"DBLP": "journals/corr/abs-2210-03493", "ArXiv": "2210.03493", "DOI": "10.48550/arXiv.2210.03493", "CorpusId": 252762275}}, "hash": "56b583f88cb5353aa94e72693e74c9c606681096be52563ade0e45fb9f0f3285"}, "3": {"node_id": "9232ed54-292c-4300-b938-5d8a70849947", "node_type": null, "metadata": {"title": "Automatic Chain of Thought Prompting in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "citationCount": 192, "openAccessPdf": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "externalIds": {"DBLP": "journals/corr/abs-2210-03493", "ArXiv": "2210.03493", "DOI": "10.48550/arXiv.2210.03493", "CorpusId": 252762275}}, "hash": "335e97bcee757aa99aa4e70aaa5fbf6611afdc582a7788f8d706c31838d01d07"}}, "hash": "f0f0743d00df50433247ae37178df7fe6efb5174f18aff4a7f811868e4d39799", "text": "Automatic Chain of Thought Prompting in Large Language Models Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like\"Let's think step by step\"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the\"Let's think step by step\"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual", "start_char_idx": 0, "end_char_idx": 1385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9232ed54-292c-4300-b938-5d8a70849947": {"__data__": {"id_": "9232ed54-292c-4300-b938-5d8a70849947", "embedding": null, "metadata": {"title": "Automatic Chain of Thought Prompting in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "citationCount": 192, "openAccessPdf": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "externalIds": {"DBLP": "journals/corr/abs-2210-03493", "ArXiv": "2210.03493", "DOI": "10.48550/arXiv.2210.03493", "CorpusId": 252762275}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24f98f71-2d40-48bc-8c24-589f8a059ec0", "node_type": null, "metadata": {"title": "Automatic Chain of Thought Prompting in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "citationCount": 192, "openAccessPdf": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "externalIds": {"DBLP": "journals/corr/abs-2210-03493", "ArXiv": "2210.03493", "DOI": "10.48550/arXiv.2210.03493", "CorpusId": 252762275}}, "hash": "56b583f88cb5353aa94e72693e74c9c606681096be52563ade0e45fb9f0f3285"}, "2": {"node_id": "97b84ea5-6bdd-4026-aeb6-465747f6a59f", "node_type": null, "metadata": {"title": "Automatic Chain of Thought Prompting in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "citationCount": 192, "openAccessPdf": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "externalIds": {"DBLP": "journals/corr/abs-2210-03493", "ArXiv": "2210.03493", "DOI": "10.48550/arXiv.2210.03493", "CorpusId": 252762275}}, "hash": "f0f0743d00df50433247ae37178df7fe6efb5174f18aff4a7f811868e4d39799"}}, "hash": "335e97bcee757aa99aa4e70aaa5fbf6611afdc582a7788f8d706c31838d01d07", "text": "matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot", "start_char_idx": 1310, "end_char_idx": 1477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9d5a5c37-6e9b-4a09-bb76-f803eaae1d0c": {"__data__": {"id_": "9d5a5c37-6e9b-4a09-bb76-f803eaae1d0c", "embedding": null, "metadata": {"title": "Extracting Training Data from Large Language Models", "venue": "USENIX Security Symposium", "year": 2020, "paperId": "62d1a3137b01a69443bebf4d92c1990ec512a6a1", "citationCount": 770, "openAccessPdf": null, "authors": ["Nicholas Carlini", "Florian Tram\u00e8r", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom B. Brown", "D. Song", "\u00da. Erlingsson", "Alina Oprea", "Colin Raffel"], "externalIds": {"DBLP": "journals/corr/abs-2012-07805", "MAG": "3112689365", "ArXiv": "2012.07805", "CorpusId": 229156229}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a62d240-c08a-4ace-86d0-44301e9ae387", "node_type": null, "metadata": {"title": "Extracting Training Data from Large Language Models", "venue": "USENIX Security Symposium", "year": 2020, "paperId": "62d1a3137b01a69443bebf4d92c1990ec512a6a1", "citationCount": 770, "openAccessPdf": null, "authors": ["Nicholas Carlini", "Florian Tram\u00e8r", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom B. Brown", "D. Song", "\u00da. Erlingsson", "Alina Oprea", "Colin Raffel"], "externalIds": {"DBLP": "journals/corr/abs-2012-07805", "MAG": "3112689365", "ArXiv": "2012.07805", "CorpusId": 229156229}}, "hash": "03fa1917b909818ac17c84a3c703e5b14b0c5556bd04a1023829f0ceda0f83dd"}}, "hash": "03fa1917b909818ac17c84a3c703e5b14b0c5556bd04a1023829f0ceda0f83dd", "text": "Extracting Training Data from Large Language Models It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.", "start_char_idx": 0, "end_char_idx": 1114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1de38d24-26cf-4ad9-bb1b-3b732a34e878": {"__data__": {"id_": "1de38d24-26cf-4ad9-bb1b-3b732a34e878", "embedding": null, "metadata": {"title": "A systematic evaluation of large language models of code", "venue": "MAPS@PLDI", "year": 2022, "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "citationCount": 188, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"], "externalIds": {"DBLP": "conf/pldi/Xu0NH22", "ArXiv": "2202.13169", "DOI": "10.1145/3520312.3534862", "CorpusId": 247158549}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "98161ef1-9562-44b3-90ce-783ed2dcf5ef", "node_type": null, "metadata": {"title": "A systematic evaluation of large language models of code", "venue": "MAPS@PLDI", "year": 2022, "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "citationCount": 188, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"], "externalIds": {"DBLP": "conf/pldi/Xu0NH22", "ArXiv": "2202.13169", "DOI": "10.1145/3520312.3534862", "CorpusId": 247158549}}, "hash": "6653bdc32ace10da3467984fa5f50ca033412dc819bfeceba75fa8aafb132392"}, "3": {"node_id": "44513a67-2ea3-4b6e-8dc1-78120fb2782a", "node_type": null, "metadata": {"title": "A systematic evaluation of large language models of code", "venue": "MAPS@PLDI", "year": 2022, "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "citationCount": 188, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"], "externalIds": {"DBLP": "conf/pldi/Xu0NH22", "ArXiv": "2202.13169", "DOI": "10.1145/3520312.3534862", "CorpusId": 247158549}}, "hash": "cfd92659f678c21c5925d374253eb78ae3d8d6cb06bf6d0e64c5cf9f226fc6be"}}, "hash": "1047e71ec9c7792d7330e15fa41d934e88627c7d92e03f155bbeab5f895b22d3", "text": "A systematic evaluation of large language models of code Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application", "start_char_idx": 0, "end_char_idx": 1302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "44513a67-2ea3-4b6e-8dc1-78120fb2782a": {"__data__": {"id_": "44513a67-2ea3-4b6e-8dc1-78120fb2782a", "embedding": null, "metadata": {"title": "A systematic evaluation of large language models of code", "venue": "MAPS@PLDI", "year": 2022, "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "citationCount": 188, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"], "externalIds": {"DBLP": "conf/pldi/Xu0NH22", "ArXiv": "2202.13169", "DOI": "10.1145/3520312.3534862", "CorpusId": 247158549}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "98161ef1-9562-44b3-90ce-783ed2dcf5ef", "node_type": null, "metadata": {"title": "A systematic evaluation of large language models of code", "venue": "MAPS@PLDI", "year": 2022, "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "citationCount": 188, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"], "externalIds": {"DBLP": "conf/pldi/Xu0NH22", "ArXiv": "2202.13169", "DOI": "10.1145/3520312.3534862", "CorpusId": 247158549}}, "hash": "6653bdc32ace10da3467984fa5f50ca033412dc819bfeceba75fa8aafb132392"}, "2": {"node_id": "1de38d24-26cf-4ad9-bb1b-3b732a34e878", "node_type": null, "metadata": {"title": "A systematic evaluation of large language models of code", "venue": "MAPS@PLDI", "year": 2022, "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "citationCount": 188, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"], "externalIds": {"DBLP": "conf/pldi/Xu0NH22", "ArXiv": "2202.13169", "DOI": "10.1145/3520312.3534862", "CorpusId": 247158549}}, "hash": "1047e71ec9c7792d7330e15fa41d934e88627c7d92e03f155bbeab5f895b22d3"}}, "hash": "cfd92659f678c21c5925d374253eb78ae3d8d6cb06bf6d0e64c5cf9f226fc6be", "text": "which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.", "start_char_idx": 1257, "end_char_idx": 1380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7a8f40b2-e0b6-4c4b-a8d0-727daf6c0465": {"__data__": {"id_": "7a8f40b2-e0b6-4c4b-a8d0-727daf6c0465", "embedding": null, "metadata": {"title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models", "venue": "International Conference on Machine Learning", "year": 2022, "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "citationCount": 100, "openAccessPdf": null, "authors": ["Guangxuan Xiao", "Ji Lin", "Mickael Seznec", "Julien Demouth", "Song Han"], "externalIds": {"DBLP": "journals/corr/abs-2211-10438", "ArXiv": "2211.10438", "DOI": "10.48550/arXiv.2211.10438", "CorpusId": 253708271}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "074db97e-555a-4aec-9721-a23a6b55e16f", "node_type": null, "metadata": {"title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models", "venue": "International Conference on Machine Learning", "year": 2022, "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "citationCount": 100, "openAccessPdf": null, "authors": ["Guangxuan Xiao", "Ji Lin", "Mickael Seznec", "Julien Demouth", "Song Han"], "externalIds": {"DBLP": "journals/corr/abs-2211-10438", "ArXiv": "2211.10438", "DOI": "10.48550/arXiv.2211.10438", "CorpusId": 253708271}}, "hash": "898304ee0f4ebb3f4c1ca8ab692f9ff113751ed67668fb5a54ce295b31068937"}}, "hash": "898304ee0f4ebb3f4c1ca8ab692f9ff113751ed67668fb5a54ce295b31068937", "text": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.", "start_char_idx": 0, "end_char_idx": 1260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "31723c28-0a50-4554-ada8-ffa913cc5e39": {"__data__": {"id_": "31723c28-0a50-4554-ada8-ffa913cc5e39", "embedding": null, "metadata": {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "5437e8adab596d7294124c0e798708e050e25321", "citationCount": 358, "openAccessPdf": null, "authors": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "E. Chi"], "externalIds": {"DBLP": "journals/corr/abs-2205-10625", "ArXiv": "2205.10625", "DOI": "10.48550/arXiv.2205.10625", "CorpusId": 248986239}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f27c421-d5ac-439d-a29a-01a3795f6370", "node_type": null, "metadata": {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "5437e8adab596d7294124c0e798708e050e25321", "citationCount": 358, "openAccessPdf": null, "authors": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "E. Chi"], "externalIds": {"DBLP": "journals/corr/abs-2205-10625", "ArXiv": "2205.10625", "DOI": "10.48550/arXiv.2205.10625", "CorpusId": 248986239}}, "hash": "8ac15fef532c62591537a8b2c65a56937c47571ad5de3fe75539cdbb8ea76e47"}, "3": {"node_id": "38c5224b-9be4-4e41-8558-039157de37a3", "node_type": null, "metadata": {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "5437e8adab596d7294124c0e798708e050e25321", "citationCount": 358, "openAccessPdf": null, "authors": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "E. Chi"], "externalIds": {"DBLP": "journals/corr/abs-2205-10625", "ArXiv": "2205.10625", "DOI": "10.48550/arXiv.2205.10625", "CorpusId": 248986239}}, "hash": "818cbd8252c9e1362195cdddc9f66c5666a0e7f35e3e7bb78873fbff61e7d47e"}}, "hash": "970b752ea44a632759c8d374c9442f496638646b0389ef1e803d6f06a318a9ee", "text": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with", "start_char_idx": 0, "end_char_idx": 1190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38c5224b-9be4-4e41-8558-039157de37a3": {"__data__": {"id_": "38c5224b-9be4-4e41-8558-039157de37a3", "embedding": null, "metadata": {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "5437e8adab596d7294124c0e798708e050e25321", "citationCount": 358, "openAccessPdf": null, "authors": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "E. Chi"], "externalIds": {"DBLP": "journals/corr/abs-2205-10625", "ArXiv": "2205.10625", "DOI": "10.48550/arXiv.2205.10625", "CorpusId": 248986239}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f27c421-d5ac-439d-a29a-01a3795f6370", "node_type": null, "metadata": {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "5437e8adab596d7294124c0e798708e050e25321", "citationCount": 358, "openAccessPdf": null, "authors": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "E. Chi"], "externalIds": {"DBLP": "journals/corr/abs-2205-10625", "ArXiv": "2205.10625", "DOI": "10.48550/arXiv.2205.10625", "CorpusId": 248986239}}, "hash": "8ac15fef532c62591537a8b2c65a56937c47571ad5de3fe75539cdbb8ea76e47"}, "2": {"node_id": "31723c28-0a50-4554-ada8-ffa913cc5e39", "node_type": null, "metadata": {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "5437e8adab596d7294124c0e798708e050e25321", "citationCount": 358, "openAccessPdf": null, "authors": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "E. Chi"], "externalIds": {"DBLP": "journals/corr/abs-2205-10625", "ArXiv": "2205.10625", "DOI": "10.48550/arXiv.2205.10625", "CorpusId": 248986239}}, "hash": "970b752ea44a632759c8d374c9442f496638646b0389ef1e803d6f06a318a9ee"}}, "hash": "818cbd8252c9e1362195cdddc9f66c5666a0e7f35e3e7bb78873fbff61e7d47e", "text": "least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.", "start_char_idx": 1121, "end_char_idx": 1463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e8deaab-8a86-44bf-b3d6-2ecf20b28c18": {"__data__": {"id_": "8e8deaab-8a86-44bf-b3d6-2ecf20b28c18", "embedding": null, "metadata": {"title": "Large language models encode clinical knowledge", "venue": "Nature", "year": 2022, "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "citationCount": 277, "openAccessPdf": "https://www.nature.com/articles/s41586-023-06291-2.pdf", "authors": ["K. Singhal", "Shekoofeh Azizi", "T. Tu", "S. Mahdavi", "Jason Wei", "Hyung Won Chung", "Nathan Scales", "A. Tanwani", "H. Cole-Lewis", "S. Pfohl", "P. Payne", "Martin G. Seneviratne", "P. Gamble", "C. Kelly", "Nathaneal Scharli", "Aakanksha Chowdhery", "P. A. Mansfield", "B. A. Y. Arcas", "D. Webster", "Greg S. Corrado", "Y. Matias", "K. Chou", "Juraj Gottweis", "Nenad Toma\u0161ev", "Yun Liu", "A. Rajkomar", "J. Barral", "Christopher Semturs", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"PubMedCentral": "10396962", "DBLP": "journals/corr/abs-2212-13138", "ArXiv": "2212.13138", "DOI": "10.1038/s41586-023-06291-2", "CorpusId": 255124952, "PubMed": "37438534"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d07cbcd-7c48-4a24-a1f8-b3f9e227c6dd", "node_type": null, "metadata": {"title": "Large language models encode clinical knowledge", "venue": "Nature", "year": 2022, "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "citationCount": 277, "openAccessPdf": "https://www.nature.com/articles/s41586-023-06291-2.pdf", "authors": ["K. Singhal", "Shekoofeh Azizi", "T. Tu", "S. Mahdavi", "Jason Wei", "Hyung Won Chung", "Nathan Scales", "A. Tanwani", "H. Cole-Lewis", "S. Pfohl", "P. Payne", "Martin G. Seneviratne", "P. Gamble", "C. Kelly", "Nathaneal Scharli", "Aakanksha Chowdhery", "P. A. Mansfield", "B. A. Y. Arcas", "D. Webster", "Greg S. Corrado", "Y. Matias", "K. Chou", "Juraj Gottweis", "Nenad Toma\u0161ev", "Yun Liu", "A. Rajkomar", "J. Barral", "Christopher Semturs", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"PubMedCentral": "10396962", "DBLP": "journals/corr/abs-2212-13138", "ArXiv": "2212.13138", "DOI": "10.1038/s41586-023-06291-2", "CorpusId": 255124952, "PubMed": "37438534"}}, "hash": "0e345b73dab69591b0e0008c258c64da425c2c11e6de6d728f04d411b6f21c74"}}, "hash": "0e345b73dab69591b0e0008c258c64da425c2c11e6de6d728f04d411b6f21c74", "text": "Large language models encode clinical knowledge", "start_char_idx": 0, "end_char_idx": 47, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6dcc55f-5ece-4543-b00d-7f28f0ccbf53": {"__data__": {"id_": "c6dcc55f-5ece-4543-b00d-7f28f0ccbf53", "embedding": null, "metadata": {"title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd", "citationCount": 133, "openAccessPdf": null, "authors": ["Antonia Creswell", "M. Shanahan", "I. Higgins"], "externalIds": {"DBLP": "journals/corr/abs-2205-09712", "ArXiv": "2205.09712", "DOI": "10.48550/arXiv.2205.09712", "CorpusId": 248887351}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f3eaf45-cb13-4d46-bc0d-ab2244ed68c0", "node_type": null, "metadata": {"title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd", "citationCount": 133, "openAccessPdf": null, "authors": ["Antonia Creswell", "M. Shanahan", "I. Higgins"], "externalIds": {"DBLP": "journals/corr/abs-2205-09712", "ArXiv": "2205.09712", "DOI": "10.48550/arXiv.2205.09712", "CorpusId": 248887351}}, "hash": "cc4c77b0c72c14e66f0864300d847ef64517f9eec08c2eba4b26fff4f1442032"}}, "hash": "cc4c77b0c72c14e66f0864300d847ef64517f9eec08c2eba4b26fff4f1442032", "text": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.", "start_char_idx": 0, "end_char_idx": 1432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "12256cf7-a7ce-432d-9b90-23d5ebb351a5": {"__data__": {"id_": "12256cf7-a7ce-432d-9b90-23d5ebb351a5", "embedding": null, "metadata": {"title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models", "venue": "IEEE International Conference on Robotics and Automation", "year": 2022, "paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "citationCount": 134, "openAccessPdf": "https://arxiv.org/pdf/2209.11302", "authors": ["Ishika Singh", "Valts Blukis", "Arsalan Mousavian", "Ankit Goyal", "Danfei Xu", "Jonathan Tremblay", "D. Fox", "Jesse Thomason", "Animesh Garg"], "externalIds": {"DBLP": "journals/corr/abs-2209-11302", "ArXiv": "2209.11302", "DOI": "10.1109/ICRA48891.2023.10161317", "CorpusId": 252519594}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "086e27a9-78d9-4d0b-938f-3516774e4760", "node_type": null, "metadata": {"title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models", "venue": "IEEE International Conference on Robotics and Automation", "year": 2022, "paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "citationCount": 134, "openAccessPdf": "https://arxiv.org/pdf/2209.11302", "authors": ["Ishika Singh", "Valts Blukis", "Arsalan Mousavian", "Ankit Goyal", "Danfei Xu", "Jonathan Tremblay", "D. Fox", "Jesse Thomason", "Animesh Garg"], "externalIds": {"DBLP": "journals/corr/abs-2209-11302", "ArXiv": "2209.11302", "DOI": "10.1109/ICRA48891.2023.10161317", "CorpusId": 252519594}}, "hash": "042b0da682949a8cde94c8fa30c42c582462d7eea04402d3ac0d652d0d98db47"}}, "hash": "042b0da682949a8cde94c8fa30c42c582462d7eea04402d3ac0d652d0d98db47", "text": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io", "start_char_idx": 0, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "032e038d-a860-4e32-94fe-80e1db629093": {"__data__": {"id_": "032e038d-a860-4e32-94fe-80e1db629093", "embedding": null, "metadata": {"title": "Exploring Length Generalization in Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "citationCount": 66, "openAccessPdf": null, "authors": ["Cem Anil", "Yuhuai Wu", "Anders Andreassen", "Aitor Lewkowycz", "Vedant Misra", "V. Ramasesh", "Ambrose Slone", "Guy Gur-Ari", "Ethan Dyer", "Behnam Neyshabur"], "externalIds": {"ArXiv": "2207.04901", "DBLP": "conf/nips/AnilWALMRSGDN22", "DOI": "10.48550/arXiv.2207.04901", "CorpusId": 250425737}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e6a9122-135d-4446-9dd5-fc2453b6604d", "node_type": null, "metadata": {"title": "Exploring Length Generalization in Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "citationCount": 66, "openAccessPdf": null, "authors": ["Cem Anil", "Yuhuai Wu", "Anders Andreassen", "Aitor Lewkowycz", "Vedant Misra", "V. Ramasesh", "Ambrose Slone", "Guy Gur-Ari", "Ethan Dyer", "Behnam Neyshabur"], "externalIds": {"ArXiv": "2207.04901", "DBLP": "conf/nips/AnilWALMRSGDN22", "DOI": "10.48550/arXiv.2207.04901", "CorpusId": 250425737}}, "hash": "d75a5975257224b5df0acc0ba72c9b341c11303b59fe4644ebf5ce3450412a0e"}}, "hash": "d75a5975257224b5df0acc0ba72c9b341c11303b59fe4644ebf5ce3450412a0e", "text": "Exploring Length Generalization in Large Language Models The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.", "start_char_idx": 0, "end_char_idx": 1157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a71ecf5-e9b3-43d5-973b-00530135c89b": {"__data__": {"id_": "8a71ecf5-e9b3-43d5-973b-00530135c89b", "embedding": null, "metadata": {"title": "Large Language Models Are Human-Level Prompt Engineers", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "citationCount": 181, "openAccessPdf": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "externalIds": {"ArXiv": "2211.01910", "DBLP": "conf/iclr/ZhouMHPPCB23", "DOI": "10.48550/arXiv.2211.01910", "CorpusId": 253265328}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad15f097-c2c3-47a1-90be-7e6540b2fcc2", "node_type": null, "metadata": {"title": "Large Language Models Are Human-Level Prompt Engineers", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "citationCount": 181, "openAccessPdf": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "externalIds": {"ArXiv": "2211.01910", "DBLP": "conf/iclr/ZhouMHPPCB23", "DOI": "10.48550/arXiv.2211.01910", "CorpusId": 253265328}}, "hash": "ccc3f016ab1c8cb35b1ccb4ba9262719063dc8bb5760415386f37b037fad50fa"}, "3": {"node_id": "0c39c7c2-5fc3-4982-92d1-1daaf213d51a", "node_type": null, "metadata": {"title": "Large Language Models Are Human-Level Prompt Engineers", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "citationCount": 181, "openAccessPdf": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "externalIds": {"ArXiv": "2211.01910", "DBLP": "conf/iclr/ZhouMHPPCB23", "DOI": "10.48550/arXiv.2211.01910", "CorpusId": 253265328}}, "hash": "a45dff4757472c9047ee1ee3dc28e0af629a401bb8439e38d3f90228f7673131"}}, "hash": "d2b651a4fe498dd92474d62518e46a82f3ce0244e1fbb04332af73770ebd6b62", "text": "Large Language Models Are Human-Level Prompt Engineers By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at", "start_char_idx": 0, "end_char_idx": 1467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c39c7c2-5fc3-4982-92d1-1daaf213d51a": {"__data__": {"id_": "0c39c7c2-5fc3-4982-92d1-1daaf213d51a", "embedding": null, "metadata": {"title": "Large Language Models Are Human-Level Prompt Engineers", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "citationCount": 181, "openAccessPdf": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "externalIds": {"ArXiv": "2211.01910", "DBLP": "conf/iclr/ZhouMHPPCB23", "DOI": "10.48550/arXiv.2211.01910", "CorpusId": 253265328}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad15f097-c2c3-47a1-90be-7e6540b2fcc2", "node_type": null, "metadata": {"title": "Large Language Models Are Human-Level Prompt Engineers", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "citationCount": 181, "openAccessPdf": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "externalIds": {"ArXiv": "2211.01910", "DBLP": "conf/iclr/ZhouMHPPCB23", "DOI": "10.48550/arXiv.2211.01910", "CorpusId": 253265328}}, "hash": "ccc3f016ab1c8cb35b1ccb4ba9262719063dc8bb5760415386f37b037fad50fa"}, "2": {"node_id": "8a71ecf5-e9b3-43d5-973b-00530135c89b", "node_type": null, "metadata": {"title": "Large Language Models Are Human-Level Prompt Engineers", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "citationCount": 181, "openAccessPdf": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "externalIds": {"ArXiv": "2211.01910", "DBLP": "conf/iclr/ZhouMHPPCB23", "DOI": "10.48550/arXiv.2211.01910", "CorpusId": 253265328}}, "hash": "d2b651a4fe498dd92474d62518e46a82f3ce0244e1fbb04332af73770ebd6b62"}}, "hash": "a45dff4757472c9047ee1ee3dc28e0af629a401bb8439e38d3f90228f7673131", "text": "prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.", "start_char_idx": 1379, "end_char_idx": 1524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0eac94a7-dc16-437a-81d1-975297d031c1": {"__data__": {"id_": "0eac94a7-dc16-437a-81d1-975297d031c1", "embedding": null, "metadata": {"title": "Expectation vs.\u00a0Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models", "venue": "CHI Extended Abstracts", "year": 2022, "paperId": "4054fc9e8776dc0324cfc215462d606eb75916c0", "citationCount": 134, "openAccessPdf": null, "authors": ["Priyan Vaithilingam", "Tianyi Zhang", "Elena L. Glassman"], "externalIds": {"DBLP": "conf/chi/Vaithilingam0G22", "DOI": "10.1145/3491101.3519665", "CorpusId": 247255943}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3faa6ae6-1c30-4cfe-98c2-cc03acf9a2aa", "node_type": null, "metadata": {"title": "Expectation vs.\u00a0Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models", "venue": "CHI Extended Abstracts", "year": 2022, "paperId": "4054fc9e8776dc0324cfc215462d606eb75916c0", "citationCount": 134, "openAccessPdf": null, "authors": ["Priyan Vaithilingam", "Tianyi Zhang", "Elena L. Glassman"], "externalIds": {"DBLP": "conf/chi/Vaithilingam0G22", "DOI": "10.1145/3491101.3519665", "CorpusId": 247255943}}, "hash": "7696dc9695b4cb2613eef21b2937d27e53d0d65a60c5f3a495f4d146459f1e4a"}}, "hash": "7696dc9695b4cb2613eef21b2937d27e53d0d65a60c5f3a495f4d146459f1e4a", "text": "Expectation vs.\u00a0Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants\u2019 feedback.", "start_char_idx": 0, "end_char_idx": 1163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "df62aff7-b4e1-409c-9a81-e3c497188423": {"__data__": {"id_": "df62aff7-b4e1-409c-9a81-e3c497188423", "embedding": null, "metadata": {"title": "Large Language Models Can Self-Improve", "venue": "arXiv.org", "year": 2022, "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd", "citationCount": 133, "openAccessPdf": null, "authors": ["Jiaxin Huang", "S. Gu", "Le Hou", "Yuexin Wu", "Xuezhi Wang", "Hongkun Yu", "Jiawei Han"], "externalIds": {"DBLP": "journals/corr/abs-2210-11610", "ArXiv": "2210.11610", "DOI": "10.48550/arXiv.2210.11610", "CorpusId": 253080328}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3de42693-0990-4d05-95e3-13b8955283c5", "node_type": null, "metadata": {"title": "Large Language Models Can Self-Improve", "venue": "arXiv.org", "year": 2022, "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd", "citationCount": 133, "openAccessPdf": null, "authors": ["Jiaxin Huang", "S. Gu", "Le Hou", "Yuexin Wu", "Xuezhi Wang", "Hongkun Yu", "Jiawei Han"], "externalIds": {"DBLP": "journals/corr/abs-2210-11610", "ArXiv": "2210.11610", "DOI": "10.48550/arXiv.2210.11610", "CorpusId": 253080328}}, "hash": "d201858e1543bee96d848f68ab63c41e7a5b201566a3d15b50f25d6bb8f3bd39"}}, "hash": "d201858e1543bee96d848f68ab63c41e7a5b201566a3d15b50f25d6bb8f3bd39", "text": "Large Language Models Can Self-Improve Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate\"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.", "start_char_idx": 0, "end_char_idx": 1005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d107e6e2-4aac-410a-b730-87b570742df9": {"__data__": {"id_": "d107e6e2-4aac-410a-b730-87b570742df9", "embedding": null, "metadata": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "citationCount": 159, "openAccessPdf": null, "authors": ["Xuechen Li", "Florian Tram\u00e8r", "Percy Liang", "Tatsunori B. Hashimoto"], "externalIds": {"DBLP": "conf/iclr/LiTLH22", "ArXiv": "2110.05679", "CorpusId": 238634219}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "654edb2d-bbb8-4c26-94c8-2b85c98a14ae", "node_type": null, "metadata": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "citationCount": 159, "openAccessPdf": null, "authors": ["Xuechen Li", "Florian Tram\u00e8r", "Percy Liang", "Tatsunori B. Hashimoto"], "externalIds": {"DBLP": "conf/iclr/LiTLH22", "ArXiv": "2110.05679", "CorpusId": 238634219}}, "hash": "f77d1f92657dbf6b81b4f14b2043c0fffc6d6954bba17a4e7b4fdf862a9e64df"}, "3": {"node_id": "691358af-db41-4a08-ac41-9e6031fd6c12", "node_type": null, "metadata": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "citationCount": 159, "openAccessPdf": null, "authors": ["Xuechen Li", "Florian Tram\u00e8r", "Percy Liang", "Tatsunori B. Hashimoto"], "externalIds": {"DBLP": "conf/iclr/LiTLH22", "ArXiv": "2110.05679", "CorpusId": 238634219}}, "hash": "93e7d0c2170592411ecfa9dd27096167c73d4432f1d09655d86a1730b901859a"}}, "hash": "3f68977580c4fdc06a454313fd682c725d36d496d3a4c1267747a5eb211e9755", "text": "Large Language Models Can Be Strong Differentially Private Learners Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines -- by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models doesn't tend to suffer from dimension-dependent performance degradation. Code to reproduce results can be found at", "start_char_idx": 0, "end_char_idx": 1568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "691358af-db41-4a08-ac41-9e6031fd6c12": {"__data__": {"id_": "691358af-db41-4a08-ac41-9e6031fd6c12", "embedding": null, "metadata": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "citationCount": 159, "openAccessPdf": null, "authors": ["Xuechen Li", "Florian Tram\u00e8r", "Percy Liang", "Tatsunori B. Hashimoto"], "externalIds": {"DBLP": "conf/iclr/LiTLH22", "ArXiv": "2110.05679", "CorpusId": 238634219}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "654edb2d-bbb8-4c26-94c8-2b85c98a14ae", "node_type": null, "metadata": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "citationCount": 159, "openAccessPdf": null, "authors": ["Xuechen Li", "Florian Tram\u00e8r", "Percy Liang", "Tatsunori B. Hashimoto"], "externalIds": {"DBLP": "conf/iclr/LiTLH22", "ArXiv": "2110.05679", "CorpusId": 238634219}}, "hash": "f77d1f92657dbf6b81b4f14b2043c0fffc6d6954bba17a4e7b4fdf862a9e64df"}, "2": {"node_id": "d107e6e2-4aac-410a-b730-87b570742df9", "node_type": null, "metadata": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "citationCount": 159, "openAccessPdf": null, "authors": ["Xuechen Li", "Florian Tram\u00e8r", "Percy Liang", "Tatsunori B. Hashimoto"], "externalIds": {"DBLP": "conf/iclr/LiTLH22", "ArXiv": "2110.05679", "CorpusId": 238634219}}, "hash": "3f68977580c4fdc06a454313fd682c725d36d496d3a4c1267747a5eb211e9755"}}, "hash": "93e7d0c2170592411ecfa9dd27096167c73d4432f1d09655d86a1730b901859a", "text": "from dimension-dependent performance degradation. Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.", "start_char_idx": 1477, "end_char_idx": 1618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8cf2dc98-f5aa-45a6-836a-8267aa6909de": {"__data__": {"id_": "8cf2dc98-f5aa-45a6-836a-8267aa6909de", "embedding": null, "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "citationCount": 253, "openAccessPdf": null, "authors": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "externalIds": {"ArXiv": "2304.10592", "DBLP": "journals/corr/abs-2304-10592", "DOI": "10.48550/arXiv.2304.10592", "CorpusId": 258291930}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8efdc06f-210f-4381-97ce-5fa91166fc13", "node_type": null, "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "citationCount": 253, "openAccessPdf": null, "authors": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "externalIds": {"ArXiv": "2304.10592", "DBLP": "journals/corr/abs-2304-10592", "DOI": "10.48550/arXiv.2304.10592", "CorpusId": 258291930}}, "hash": "ee5547f77297d99b884d9b7a59b8e25530c73a50eebc88d8db63906afe00ea49"}, "3": {"node_id": "06369f7d-fb14-4f91-834d-9f296b153f00", "node_type": null, "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "citationCount": 253, "openAccessPdf": null, "authors": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "externalIds": {"ArXiv": "2304.10592", "DBLP": "journals/corr/abs-2304-10592", "DOI": "10.48550/arXiv.2304.10592", "CorpusId": 258291930}}, "hash": "a15745dc7a21ab1a93efa9d34f5f36f8b366ee97da11c95b0577d4041995ff90"}}, "hash": "eed2a9362e7f5b0bfe30cf042d81f4b30fc84a655440f5707d175ecab3078674", "text": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description", "start_char_idx": 0, "end_char_idx": 1411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "06369f7d-fb14-4f91-834d-9f296b153f00": {"__data__": {"id_": "06369f7d-fb14-4f91-834d-9f296b153f00", "embedding": null, "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "citationCount": 253, "openAccessPdf": null, "authors": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "externalIds": {"ArXiv": "2304.10592", "DBLP": "journals/corr/abs-2304-10592", "DOI": "10.48550/arXiv.2304.10592", "CorpusId": 258291930}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8efdc06f-210f-4381-97ce-5fa91166fc13", "node_type": null, "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "citationCount": 253, "openAccessPdf": null, "authors": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "externalIds": {"ArXiv": "2304.10592", "DBLP": "journals/corr/abs-2304-10592", "DOI": "10.48550/arXiv.2304.10592", "CorpusId": 258291930}}, "hash": "ee5547f77297d99b884d9b7a59b8e25530c73a50eebc88d8db63906afe00ea49"}, "2": {"node_id": "8cf2dc98-f5aa-45a6-836a-8267aa6909de", "node_type": null, "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "citationCount": 253, "openAccessPdf": null, "authors": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "externalIds": {"ArXiv": "2304.10592", "DBLP": "journals/corr/abs-2304-10592", "DOI": "10.48550/arXiv.2304.10592", "CorpusId": 258291930}}, "hash": "eed2a9362e7f5b0bfe30cf042d81f4b30fc84a655440f5707d175ecab3078674"}}, "hash": "a15745dc7a21ab1a93efa9d34f5f36f8b366ee97da11c95b0577d4041995ff90", "text": "and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.", "start_char_idx": 1328, "end_char_idx": 1646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "818b3416-c03d-47ed-9483-b5be82d8e179": {"__data__": {"id_": "818b3416-c03d-47ed-9483-b5be82d8e179", "embedding": null, "metadata": {"title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "venue": "arXiv.org", "year": 2022, "paperId": "6d147d1b7a283252052cda28a98ee6cc6379f932", "citationCount": 77, "openAccessPdf": null, "authors": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "externalIds": {"ArXiv": "2206.10498", "DBLP": "journals/corr/abs-2206-10498", "DOI": "10.48550/arXiv.2206.10498", "CorpusId": 249889477}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c579e6f5-a675-4b34-8915-d831cadb6f61", "node_type": null, "metadata": {"title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "venue": "arXiv.org", "year": 2022, "paperId": "6d147d1b7a283252052cda28a98ee6cc6379f932", "citationCount": 77, "openAccessPdf": null, "authors": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "externalIds": {"ArXiv": "2206.10498", "DBLP": "journals/corr/abs-2206-10498", "DOI": "10.48550/arXiv.2206.10498", "CorpusId": 249889477}}, "hash": "fc4fe364c0dd003248f8fb2649842b5c2e4ed748426e124cd63225d2302ef81b"}, "3": {"node_id": "1fe355a9-395e-4006-946b-0e7073ce9c96", "node_type": null, "metadata": {"title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "venue": "arXiv.org", "year": 2022, "paperId": "6d147d1b7a283252052cda28a98ee6cc6379f932", "citationCount": 77, "openAccessPdf": null, "authors": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "externalIds": {"ArXiv": "2206.10498", "DBLP": "journals/corr/abs-2206-10498", "DOI": "10.48550/arXiv.2206.10498", "CorpusId": 249889477}}, "hash": "05bd6a1e6f574c7cc140cf01a0b0728cebaeb3beab2125a9bfd9762cb0beb33f"}}, "hash": "33f464051af0a800964a916372384027160b246291102f8a11a76ded3f0fb220", "text": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence. We provide multiple test cases that are more involved than any of the previously established benchmarks and each test case evaluates a different aspect of reasoning about actions and change. Results on GPT-3 (davinci), Instruct-GPT3 (text-davinci-002) and BLOOM", "start_char_idx": 0, "end_char_idx": 1444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1fe355a9-395e-4006-946b-0e7073ce9c96": {"__data__": {"id_": "1fe355a9-395e-4006-946b-0e7073ce9c96", "embedding": null, "metadata": {"title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "venue": "arXiv.org", "year": 2022, "paperId": "6d147d1b7a283252052cda28a98ee6cc6379f932", "citationCount": 77, "openAccessPdf": null, "authors": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "externalIds": {"ArXiv": "2206.10498", "DBLP": "journals/corr/abs-2206-10498", "DOI": "10.48550/arXiv.2206.10498", "CorpusId": 249889477}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c579e6f5-a675-4b34-8915-d831cadb6f61", "node_type": null, "metadata": {"title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "venue": "arXiv.org", "year": 2022, "paperId": "6d147d1b7a283252052cda28a98ee6cc6379f932", "citationCount": 77, "openAccessPdf": null, "authors": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "externalIds": {"ArXiv": "2206.10498", "DBLP": "journals/corr/abs-2206-10498", "DOI": "10.48550/arXiv.2206.10498", "CorpusId": 249889477}}, "hash": "fc4fe364c0dd003248f8fb2649842b5c2e4ed748426e124cd63225d2302ef81b"}, "2": {"node_id": "818b3416-c03d-47ed-9483-b5be82d8e179", "node_type": null, "metadata": {"title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "venue": "arXiv.org", "year": 2022, "paperId": "6d147d1b7a283252052cda28a98ee6cc6379f932", "citationCount": 77, "openAccessPdf": null, "authors": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "externalIds": {"ArXiv": "2206.10498", "DBLP": "journals/corr/abs-2206-10498", "DOI": "10.48550/arXiv.2206.10498", "CorpusId": 249889477}}, "hash": "33f464051af0a800964a916372384027160b246291102f8a11a76ded3f0fb220"}}, "hash": "05bd6a1e6f574c7cc140cf01a0b0728cebaeb3beab2125a9bfd9762cb0beb33f", "text": "Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance on such reasoning tasks.", "start_char_idx": 1402, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ba252561-ac1e-48a3-a28b-ffca8d1fb2cb": {"__data__": {"id_": "ba252561-ac1e-48a3-a28b-ffca8d1fb2cb", "embedding": null, "metadata": {"title": "Large language models are few-shot clinical information extractors", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "686d9ee744fa013cc21cdd86acd864c936e9e456", "citationCount": 92, "openAccessPdf": "https://aclanthology.org/2022.emnlp-main.130.pdf", "authors": ["Monica Agrawal", "S. Hegselmann", "Hunter Lang", "Yoon Kim", "D. Sontag"], "externalIds": {"ArXiv": "2205.12689", "DBLP": "conf/emnlp/AgrawalHLKS22", "ACL": "2022.emnlp-main.130", "DOI": "10.18653/v1/2022.emnlp-main.130", "CorpusId": 249062918}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af2d4143-fbaa-4af1-8e3a-eddfd114c125", "node_type": null, "metadata": {"title": "Large language models are few-shot clinical information extractors", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "686d9ee744fa013cc21cdd86acd864c936e9e456", "citationCount": 92, "openAccessPdf": "https://aclanthology.org/2022.emnlp-main.130.pdf", "authors": ["Monica Agrawal", "S. Hegselmann", "Hunter Lang", "Yoon Kim", "D. Sontag"], "externalIds": {"ArXiv": "2205.12689", "DBLP": "conf/emnlp/AgrawalHLKS22", "ACL": "2022.emnlp-main.130", "DOI": "10.18653/v1/2022.emnlp-main.130", "CorpusId": 249062918}}, "hash": "ee746fcfe8a395ffb9da1d9d757d4996e3ff442e0e2239303bc329488d44bd28"}}, "hash": "ee746fcfe8a395ffb9da1d9d757d4996e3ff442e0e2239303bc329488d44bd28", "text": "Large language models are few-shot clinical information extractors A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.", "start_char_idx": 0, "end_char_idx": 1247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "73b3f92b-56f4-43e3-b69b-24147d2b3c19": {"__data__": {"id_": "73b3f92b-56f4-43e3-b69b-24147d2b3c19", "embedding": null, "metadata": {"title": "Towards Reasoning in Large Language Models: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "citationCount": 108, "openAccessPdf": null, "authors": ["Jie Huang", "K. Chang"], "externalIds": {"ArXiv": "2212.10403", "DBLP": "journals/corr/abs-2212-10403", "DOI": "10.48550/arXiv.2212.10403", "CorpusId": 254877753}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9666ff2-f098-43a5-a5f6-8587f77e9fa6", "node_type": null, "metadata": {"title": "Towards Reasoning in Large Language Models: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "citationCount": 108, "openAccessPdf": null, "authors": ["Jie Huang", "K. Chang"], "externalIds": {"ArXiv": "2212.10403", "DBLP": "journals/corr/abs-2212-10403", "DOI": "10.48550/arXiv.2212.10403", "CorpusId": 254877753}}, "hash": "0ef97244d82fbc3a492933daf6961454d791780293d537a2bfe14081b0557569"}}, "hash": "0ef97244d82fbc3a492933daf6961454d791780293d537a2bfe14081b0557569", "text": "Towards Reasoning in Large Language Models: A Survey Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.", "start_char_idx": 0, "end_char_idx": 964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "793ac17d-6bc7-4b4c-bec7-7bca0600ddec": {"__data__": {"id_": "793ac17d-6bc7-4b4c-bec7-7bca0600ddec", "embedding": null, "metadata": {"title": "Wordcraft: Story Writing With Large Language Models", "venue": "International Conference on Intelligent User Interfaces", "year": 2022, "paperId": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20", "citationCount": 86, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3490099.3511105", "authors": ["Ann Yuan", "Andy Coenen", "Emily Reif", "Daphne Ippolito"], "externalIds": {"DBLP": "conf/iui/YuanCRI22", "DOI": "10.1145/3490099.3511105", "CorpusId": 247585187}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ffd7394-6942-42e7-b01f-ae75612863f6", "node_type": null, "metadata": {"title": "Wordcraft: Story Writing With Large Language Models", "venue": "International Conference on Intelligent User Interfaces", "year": 2022, "paperId": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20", "citationCount": 86, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3490099.3511105", "authors": ["Ann Yuan", "Andy Coenen", "Emily Reif", "Daphne Ippolito"], "externalIds": {"DBLP": "conf/iui/YuanCRI22", "DOI": "10.1145/3490099.3511105", "CorpusId": 247585187}}, "hash": "2b18c4d30d3fa60497e45dbee4f7a7588e02f28f2dfb19f1aa2a1820e5ca4138"}}, "hash": "2b18c4d30d3fa60497e45dbee4f7a7588e02f28f2dfb19f1aa2a1820e5ca4138", "text": "Wordcraft: Story Writing With Large Language Models The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers\u2019 custom requests expressed in natural language (such as \u201drewrite this text to be more Dickensian\u201d), and generate suggestions that serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.", "start_char_idx": 0, "end_char_idx": 1110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8cc6e23b-27c7-4162-a2f5-93273dbb0cb9": {"__data__": {"id_": "8cc6e23b-27c7-4162-a2f5-93273dbb0cb9", "embedding": null, "metadata": {"title": "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models", "venue": "International Computing Education Research Workshop", "year": 2022, "paperId": "0d08ffccc982781e310bb184397bbe64b9aef157", "citationCount": 89, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3501385.3543957", "authors": ["Sami Sarsa", "Paul Denny", "Arto Hellas", "Juho Leinonen"], "externalIds": {"DBLP": "journals/corr/abs-2206-11861", "ArXiv": "2206.11861", "DOI": "10.1145/3501385.3543957", "CorpusId": 249954011}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec2c0581-24a7-4545-9c19-0c22fa48181e", "node_type": null, "metadata": {"title": "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models", "venue": "International Computing Education Research Workshop", "year": 2022, "paperId": "0d08ffccc982781e310bb184397bbe64b9aef157", "citationCount": 89, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3501385.3543957", "authors": ["Sami Sarsa", "Paul Denny", "Arto Hellas", "Juho Leinonen"], "externalIds": {"DBLP": "journals/corr/abs-2206-11861", "ArXiv": "2206.11861", "DOI": "10.1145/3501385.3543957", "CorpusId": 249954011}}, "hash": "2534352667a66689c71d920fcf3c8644d5f62f11a31764b84100a913604d554e"}}, "hash": "2534352667a66689c71d920fcf3c8644d5f62f11a31764b84100a913604d554e", "text": "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.", "start_char_idx": 0, "end_char_idx": 1343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3d4ab8cf-d1c2-4058-b695-cd54ee4cbdfc": {"__data__": {"id_": "3d4ab8cf-d1c2-4058-b695-cd54ee4cbdfc", "embedding": null, "metadata": {"title": "Faithful Reasoning Using Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "f0a0e8b6e84207f50db4d24cc4016e40601214ef", "citationCount": 64, "openAccessPdf": null, "authors": ["Antonia Creswell", "M. Shanahan"], "externalIds": {"ArXiv": "2208.14271", "DBLP": "journals/corr/abs-2208-14271", "DOI": "10.48550/arXiv.2208.14271", "CorpusId": 251929296}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f12021f-c7b5-4d7e-8d43-658752481687", "node_type": null, "metadata": {"title": "Faithful Reasoning Using Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "f0a0e8b6e84207f50db4d24cc4016e40601214ef", "citationCount": 64, "openAccessPdf": null, "authors": ["Antonia Creswell", "M. Shanahan"], "externalIds": {"ArXiv": "2208.14271", "DBLP": "journals/corr/abs-2208-14271", "DOI": "10.48550/arXiv.2208.14271", "CorpusId": 251929296}}, "hash": "313e88ce2c18212b4e728fc1edc990d454a087bf6abc82f2dc049c3e22398426"}}, "hash": "313e88ce2c18212b4e728fc1edc990d454a087bf6abc82f2dc049c3e22398426", "text": "Faithful Reasoning Using Large Language Models Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.", "start_char_idx": 0, "end_char_idx": 1114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a1e8d600-2713-44df-89fa-8eeb4d807fd6": {"__data__": {"id_": "a1e8d600-2713-44df-89fa-8eeb4d807fd6", "embedding": null, "metadata": {"title": "Can large language models reason about medical questions?", "venue": "arXiv.org", "year": 2022, "paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "citationCount": 82, "openAccessPdf": null, "authors": ["Valentin Li'evin", "C. Hother", "O. Winther"], "externalIds": {"DBLP": "journals/corr/abs-2207-08143", "ArXiv": "2207.08143", "DOI": "10.48550/arXiv.2207.08143", "CorpusId": 250627547}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f4d4c07-16a2-48ed-8087-208a41dcee02", "node_type": null, "metadata": {"title": "Can large language models reason about medical questions?", "venue": "arXiv.org", "year": 2022, "paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "citationCount": 82, "openAccessPdf": null, "authors": ["Valentin Li'evin", "C. Hother", "O. Winther"], "externalIds": {"DBLP": "journals/corr/abs-2207-08143", "ArXiv": "2207.08143", "DOI": "10.48550/arXiv.2207.08143", "CorpusId": 250627547}}, "hash": "bbf4b8d10c3b036c31750b476bbb9c0d9b05f7f0eaad5a89b1d50c452b0036b0"}, "3": {"node_id": "6401cbba-3834-4075-8fea-74a0fd56f11d", "node_type": null, "metadata": {"title": "Can large language models reason about medical questions?", "venue": "arXiv.org", "year": 2022, "paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "citationCount": 82, "openAccessPdf": null, "authors": ["Valentin Li'evin", "C. Hother", "O. Winther"], "externalIds": {"DBLP": "journals/corr/abs-2207-08143", "ArXiv": "2207.08143", "DOI": "10.48550/arXiv.2207.08143", "CorpusId": 250627547}}, "hash": "ffb335d2cfcbc596f09e2ef3b333b44f695bcf483e08ffe2a02529ea5e44ff75"}}, "hash": "6212c83691004e1d1455d6115eea7ba84fb4a27f194e0e08b3f94940a573981e", "text": "Can large language models reason about medical questions? Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\\ too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets.", "start_char_idx": 0, "end_char_idx": 1347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6401cbba-3834-4075-8fea-74a0fd56f11d": {"__data__": {"id_": "6401cbba-3834-4075-8fea-74a0fd56f11d", "embedding": null, "metadata": {"title": "Can large language models reason about medical questions?", "venue": "arXiv.org", "year": 2022, "paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "citationCount": 82, "openAccessPdf": null, "authors": ["Valentin Li'evin", "C. Hother", "O. Winther"], "externalIds": {"DBLP": "journals/corr/abs-2207-08143", "ArXiv": "2207.08143", "DOI": "10.48550/arXiv.2207.08143", "CorpusId": 250627547}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f4d4c07-16a2-48ed-8087-208a41dcee02", "node_type": null, "metadata": {"title": "Can large language models reason about medical questions?", "venue": "arXiv.org", "year": 2022, "paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "citationCount": 82, "openAccessPdf": null, "authors": ["Valentin Li'evin", "C. Hother", "O. Winther"], "externalIds": {"DBLP": "journals/corr/abs-2207-08143", "ArXiv": "2207.08143", "DOI": "10.48550/arXiv.2207.08143", "CorpusId": 250627547}}, "hash": "bbf4b8d10c3b036c31750b476bbb9c0d9b05f7f0eaad5a89b1d50c452b0036b0"}, "2": {"node_id": "a1e8d600-2713-44df-89fa-8eeb4d807fd6", "node_type": null, "metadata": {"title": "Can large language models reason about medical questions?", "venue": "arXiv.org", "year": 2022, "paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "citationCount": 82, "openAccessPdf": null, "authors": ["Valentin Li'evin", "C. Hother", "O. Winther"], "externalIds": {"DBLP": "journals/corr/abs-2207-08143", "ArXiv": "2207.08143", "DOI": "10.48550/arXiv.2207.08143", "CorpusId": 250627547}}, "hash": "6212c83691004e1d1455d6115eea7ba84fb4a27f194e0e08b3f94940a573981e"}}, "hash": "ffb335d2cfcbc596f09e2ef3b333b44f695bcf483e08ffe2a02529ea5e44ff75", "text": "probability but also achieves human-level performances on the three datasets. USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.", "start_char_idx": 1270, "end_char_idx": 1397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5d4630c4-4b9a-45c8-a9a0-525d5e2ec4f7": {"__data__": {"id_": "5d4630c4-4b9a-45c8-a9a0-525d5e2ec4f7", "embedding": null, "metadata": {"title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "8b293973061026d9d0eed90e71e30928e029171e", "citationCount": 67, "openAccessPdf": null, "authors": ["Kushal Tirumala", "Aram H. Markosyan", "Luke Zettlemoyer", "Armen Aghajanyan"], "externalIds": {"DBLP": "journals/corr/abs-2205-10770", "ArXiv": "2205.10770", "DOI": "10.48550/arXiv.2205.10770", "CorpusId": 248986465}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8a4883c-2c6d-4ceb-a728-e0a9f4e3f4d6", "node_type": null, "metadata": {"title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "8b293973061026d9d0eed90e71e30928e029171e", "citationCount": 67, "openAccessPdf": null, "authors": ["Kushal Tirumala", "Aram H. Markosyan", "Luke Zettlemoyer", "Armen Aghajanyan"], "externalIds": {"DBLP": "journals/corr/abs-2205-10770", "ArXiv": "2205.10770", "DOI": "10.48550/arXiv.2205.10770", "CorpusId": 248986465}}, "hash": "ab6f76d5e08940ddea3bcb737f44e8d3b85a762e5feee31dde49df45c0c6dcf2"}}, "hash": "ab6f76d5e08940ddea3bcb737f44e8d3b85a762e5feee31dde49df45c0c6dcf2", "text": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.", "start_char_idx": 0, "end_char_idx": 1098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "623d4355-e3ff-4a3e-ae35-dde9d422de86": {"__data__": {"id_": "623d4355-e3ff-4a3e-ae35-dde9d422de86", "embedding": null, "metadata": {"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "citationCount": 59, "openAccessPdf": null, "authors": ["Archiki Prasad", "Peter Hase", "Xiang Zhou", "Mohit Bansal"], "externalIds": {"ArXiv": "2203.07281", "ACL": "2023.eacl-main.277", "DBLP": "conf/eacl/PrasadHZB23", "DOI": "10.48550/arXiv.2203.07281", "CorpusId": 247447170}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1eef0a61-42eb-420b-90c1-1877dda8c746", "node_type": null, "metadata": {"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "citationCount": 59, "openAccessPdf": null, "authors": ["Archiki Prasad", "Peter Hase", "Xiang Zhou", "Mohit Bansal"], "externalIds": {"ArXiv": "2203.07281", "ACL": "2023.eacl-main.277", "DBLP": "conf/eacl/PrasadHZB23", "DOI": "10.48550/arXiv.2203.07281", "CorpusId": 247447170}}, "hash": "de415aafd8e17507d516067ae621721e820c8fa035f2f0fe943b802f221ca383"}, "3": {"node_id": "33242c4d-5a7b-4b72-80f1-a158dbd38e39", "node_type": null, "metadata": {"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "citationCount": 59, "openAccessPdf": null, "authors": ["Archiki Prasad", "Peter Hase", "Xiang Zhou", "Mohit Bansal"], "externalIds": {"ArXiv": "2203.07281", "ACL": "2023.eacl-main.277", "DBLP": "conf/eacl/PrasadHZB23", "DOI": "10.48550/arXiv.2203.07281", "CorpusId": 247447170}}, "hash": "05d884c110ab059b9b138ac258bd803296f270753064698d741a1b6970f17fd2"}}, "hash": "5c711276631332e3bf8046c072efb4b0407a65f42150b3cea514ad063320d51b", "text": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for", "start_char_idx": 0, "end_char_idx": 1296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "33242c4d-5a7b-4b72-80f1-a158dbd38e39": {"__data__": {"id_": "33242c4d-5a7b-4b72-80f1-a158dbd38e39", "embedding": null, "metadata": {"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "citationCount": 59, "openAccessPdf": null, "authors": ["Archiki Prasad", "Peter Hase", "Xiang Zhou", "Mohit Bansal"], "externalIds": {"ArXiv": "2203.07281", "ACL": "2023.eacl-main.277", "DBLP": "conf/eacl/PrasadHZB23", "DOI": "10.48550/arXiv.2203.07281", "CorpusId": 247447170}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1eef0a61-42eb-420b-90c1-1877dda8c746", "node_type": null, "metadata": {"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "citationCount": 59, "openAccessPdf": null, "authors": ["Archiki Prasad", "Peter Hase", "Xiang Zhou", "Mohit Bansal"], "externalIds": {"ArXiv": "2203.07281", "ACL": "2023.eacl-main.277", "DBLP": "conf/eacl/PrasadHZB23", "DOI": "10.48550/arXiv.2203.07281", "CorpusId": 247447170}}, "hash": "de415aafd8e17507d516067ae621721e820c8fa035f2f0fe943b802f221ca383"}, "2": {"node_id": "623d4355-e3ff-4a3e-ae35-dde9d422de86", "node_type": null, "metadata": {"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "citationCount": 59, "openAccessPdf": null, "authors": ["Archiki Prasad", "Peter Hase", "Xiang Zhou", "Mohit Bansal"], "externalIds": {"ArXiv": "2203.07281", "ACL": "2023.eacl-main.277", "DBLP": "conf/eacl/PrasadHZB23", "DOI": "10.48550/arXiv.2203.07281", "CorpusId": 247447170}}, "hash": "5c711276631332e3bf8046c072efb4b0407a65f42150b3cea514ad063320d51b"}}, "hash": "05d884c110ab059b9b138ac258bd803296f270753064698d741a1b6970f17fd2", "text": "manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy.", "start_char_idx": 1225, "end_char_idx": 1550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0518ab85-5af8-4f0c-b153-e0d076829f6e": {"__data__": {"id_": "0518ab85-5af8-4f0c-b153-e0d076829f6e", "embedding": null, "metadata": {"title": "Emergent analogical reasoning in large language models", "venue": "Nature Human Behaviour", "year": 2022, "paperId": "3cbffab9d7981da6662d474aaa056dcbd3c1701e", "citationCount": 57, "openAccessPdf": "https://arxiv.org/pdf/2212.09196", "authors": ["Taylor W. Webb", "K. Holyoak", "Hongjing Lu"], "externalIds": {"DBLP": "journals/corr/abs-2212-09196", "ArXiv": "2212.09196", "DOI": "10.1038/s41562-023-01659-w", "CorpusId": 254854575, "PubMed": "37524930"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f3300ca-b6fc-42ab-8617-078c5ce3566c", "node_type": null, "metadata": {"title": "Emergent analogical reasoning in large language models", "venue": "Nature Human Behaviour", "year": 2022, "paperId": "3cbffab9d7981da6662d474aaa056dcbd3c1701e", "citationCount": 57, "openAccessPdf": "https://arxiv.org/pdf/2212.09196", "authors": ["Taylor W. Webb", "K. Holyoak", "Hongjing Lu"], "externalIds": {"DBLP": "journals/corr/abs-2212-09196", "ArXiv": "2212.09196", "DOI": "10.1038/s41562-023-01659-w", "CorpusId": 254854575, "PubMed": "37524930"}}, "hash": "87cdbf374301404ab9daf58e6cb3f365e22f94d2c1f3ea00dd5c6d5893d3d8ac"}}, "hash": "87cdbf374301404ab9daf58e6cb3f365e22f94d2c1f3ea00dd5c6d5893d3d8ac", "text": "Emergent analogical reasoning in large language models", "start_char_idx": 0, "end_char_idx": 54, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2ef9155a-0808-4444-b2a5-d750a2715654": {"__data__": {"id_": "2ef9155a-0808-4444-b2a5-d750a2715654", "embedding": null, "metadata": {"title": "Autoformalization with Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5", "citationCount": 56, "openAccessPdf": null, "authors": ["Yuhuai Wu", "Albert Qiaochu Jiang", "Wenda Li", "Markus N. Rabe", "Charles Staats", "M. Jamnik", "Christian Szegedy"], "externalIds": {"DBLP": "journals/corr/abs-2205-12615", "ArXiv": "2205.12615", "DOI": "10.48550/arXiv.2205.12615", "CorpusId": 249063032}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2dfc7f1-a1a2-4b88-8ae7-75aebeaeb47d", "node_type": null, "metadata": {"title": "Autoformalization with Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5", "citationCount": 56, "openAccessPdf": null, "authors": ["Yuhuai Wu", "Albert Qiaochu Jiang", "Wenda Li", "Markus N. Rabe", "Charles Staats", "M. Jamnik", "Christian Szegedy"], "externalIds": {"DBLP": "journals/corr/abs-2205-12615", "ArXiv": "2205.12615", "DOI": "10.48550/arXiv.2205.12615", "CorpusId": 249063032}}, "hash": "c1f4717f12078cde10c71e50ff266c4e1b964fb8ed58db6eceacaddf29e6da64"}}, "hash": "c1f4717f12078cde10c71e50ff266c4e1b964fb8ed58db6eceacaddf29e6da64", "text": "Autoformalization with Large Language Models Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\\%$ to $35.2\\%$.", "start_char_idx": 0, "end_char_idx": 955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8a6ff02-102b-4c71-8503-9d8858564732": {"__data__": {"id_": "b8a6ff02-102b-4c71-8503-9d8858564732", "embedding": null, "metadata": {"title": "Talking About Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "3eed4de25636ac90f39f6e1ef70e3507ed61a2a6", "citationCount": 54, "openAccessPdf": null, "authors": ["M. Shanahan"], "externalIds": {"DBLP": "journals/corr/abs-2212-03551", "ArXiv": "2212.03551", "DOI": "10.48550/arXiv.2212.03551", "CorpusId": 254366666}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "156aa766-fabe-4af1-84b9-1c6f379aa6da", "node_type": null, "metadata": {"title": "Talking About Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "3eed4de25636ac90f39f6e1ef70e3507ed61a2a6", "citationCount": 54, "openAccessPdf": null, "authors": ["M. Shanahan"], "externalIds": {"DBLP": "journals/corr/abs-2212-03551", "ArXiv": "2212.03551", "DOI": "10.48550/arXiv.2212.03551", "CorpusId": 254366666}}, "hash": "e32b915041723eb1af4d96baaba0bd9e8d8df6fdd3664611b851cfaed4a1e4c2"}}, "hash": "e32b915041723eb1af4d96baaba0bd9e8d8df6fdd3664611b851cfaed4a1e4c2", "text": "Talking About Large Language Models Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as\"knows\",\"believes\", and\"thinks\", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.", "start_char_idx": 0, "end_char_idx": 975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "52409e45-5230-4e5e-9999-509af585a51a": {"__data__": {"id_": "52409e45-5230-4e5e-9999-509af585a51a", "embedding": null, "metadata": {"title": "Compositional Semantic Parsing with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "40047a74b707743157051d38f76061ba5ff9aab4", "citationCount": 53, "openAccessPdf": null, "authors": ["Andrew Drozdov", "Nathanael Scharli", "Ekin Akyuurek", "Nathan Scales", "Xinying Song", "Xinyun Chen", "O. Bousquet", "Denny Zhou"], "externalIds": {"ArXiv": "2209.15003", "DBLP": "journals/corr/abs-2209-15003", "CorpusId": 252596001}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86caca98-8de3-4988-99ae-4be11e028419", "node_type": null, "metadata": {"title": "Compositional Semantic Parsing with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "40047a74b707743157051d38f76061ba5ff9aab4", "citationCount": 53, "openAccessPdf": null, "authors": ["Andrew Drozdov", "Nathanael Scharli", "Ekin Akyuurek", "Nathan Scales", "Xinying Song", "Xinyun Chen", "O. Bousquet", "Denny Zhou"], "externalIds": {"ArXiv": "2209.15003", "DBLP": "journals/corr/abs-2209-15003", "CorpusId": 252596001}}, "hash": "6941925e247a0c06738fbb5d4b45e6c3812d25a55da681f16d4f94d35a48712b"}}, "hash": "6941925e247a0c06738fbb5d4b45e6c3812d25a55da681f16d4f94d35a48712b", "text": "Compositional Semantic Parsing with Large Language Models Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.", "start_char_idx": 0, "end_char_idx": 994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f39152ab-9a48-4bca-98d9-215297cfb494": {"__data__": {"id_": "f39152ab-9a48-4bca-98d9-215297cfb494", "embedding": null, "metadata": {"title": "Large Language Models Struggle to Learn Long-Tail Knowledge", "venue": "International Conference on Machine Learning", "year": 2022, "paperId": "75f7e9e2b59fb640ef9d1dff94097175daf46c4d", "citationCount": 49, "openAccessPdf": null, "authors": ["Nikhil Kandpal", "H. Deng", "Adam Roberts", "Eric Wallace", "Colin Raffel"], "externalIds": {"ArXiv": "2211.08411", "DBLP": "journals/corr/abs-2211-08411", "DOI": "10.48550/arXiv.2211.08411", "CorpusId": 253522998}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e078f33-1bcf-4262-8754-6e344d200c90", "node_type": null, "metadata": {"title": "Large Language Models Struggle to Learn Long-Tail Knowledge", "venue": "International Conference on Machine Learning", "year": 2022, "paperId": "75f7e9e2b59fb640ef9d1dff94097175daf46c4d", "citationCount": 49, "openAccessPdf": null, "authors": ["Nikhil Kandpal", "H. Deng", "Adam Roberts", "Eric Wallace", "Colin Raffel"], "externalIds": {"ArXiv": "2211.08411", "DBLP": "journals/corr/abs-2211-08411", "DOI": "10.48550/arXiv.2211.08411", "CorpusId": 253522998}}, "hash": "6c56257832358b39cef0c7d9c55d6f22c263db1e7ed56f406b796e35d2e55fa8"}}, "hash": "6c56257832358b39cef0c7d9c55d6f22c263db1e7ed56f406b796e35d2e55fa8", "text": "Large Language Models Struggle to Learn Long-Tail Knowledge The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.", "start_char_idx": 0, "end_char_idx": 1502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "be061a19-abb3-4887-a7c5-fcc96fd623ab": {"__data__": {"id_": "be061a19-abb3-4887-a7c5-fcc96fd623ab", "embedding": null, "metadata": {"title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "citationCount": 85, "openAccessPdf": null, "authors": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "externalIds": {"DBLP": "conf/iclr/0002IWXJ000023", "ArXiv": "2209.10063", "DOI": "10.48550/arXiv.2209.10063", "CorpusId": 252408513}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b3f6b2c-a193-4d8a-93e0-371072339959", "node_type": null, "metadata": {"title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "citationCount": 85, "openAccessPdf": null, "authors": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "externalIds": {"DBLP": "conf/iclr/0002IWXJ000023", "ArXiv": "2209.10063", "DOI": "10.48550/arXiv.2209.10063", "CorpusId": 252408513}}, "hash": "eda8213a40e1bcfc0f0c28a5528d7e7284e4ba351cd02191938bd2a257eb2dfb"}, "3": {"node_id": "67800253-f967-4692-bba9-64867bcbb3e7", "node_type": null, "metadata": {"title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "citationCount": 85, "openAccessPdf": null, "authors": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "externalIds": {"DBLP": "conf/iclr/0002IWXJ000023", "ArXiv": "2209.10063", "DOI": "10.48550/arXiv.2209.10063", "CorpusId": 252408513}}, "hash": "31fcd6b487ec5acb7aba0d646afcbc9c2993149cb92388ae7f0de5b2e69de228"}}, "hash": "5032bc2e9352938e42d923832a13a9cd9502b66efa04d4347d7f314c7f3e558c", "text": "Generate rather than Retrieve: Large Language Models are Strong Context Generators Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the", "start_char_idx": 0, "end_char_idx": 1331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "67800253-f967-4692-bba9-64867bcbb3e7": {"__data__": {"id_": "67800253-f967-4692-bba9-64867bcbb3e7", "embedding": null, "metadata": {"title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "citationCount": 85, "openAccessPdf": null, "authors": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "externalIds": {"DBLP": "conf/iclr/0002IWXJ000023", "ArXiv": "2209.10063", "DOI": "10.48550/arXiv.2209.10063", "CorpusId": 252408513}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b3f6b2c-a193-4d8a-93e0-371072339959", "node_type": null, "metadata": {"title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "citationCount": 85, "openAccessPdf": null, "authors": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "externalIds": {"DBLP": "conf/iclr/0002IWXJ000023", "ArXiv": "2209.10063", "DOI": "10.48550/arXiv.2209.10063", "CorpusId": 252408513}}, "hash": "eda8213a40e1bcfc0f0c28a5528d7e7284e4ba351cd02191938bd2a257eb2dfb"}, "2": {"node_id": "be061a19-abb3-4887-a7c5-fcc96fd623ab", "node_type": null, "metadata": {"title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "citationCount": 85, "openAccessPdf": null, "authors": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "externalIds": {"DBLP": "conf/iclr/0002IWXJ000023", "ArXiv": "2209.10063", "DOI": "10.48550/arXiv.2209.10063", "CorpusId": 252408513}}, "hash": "5032bc2e9352938e42d923832a13a9cd9502b66efa04d4347d7f314c7f3e558c"}}, "hash": "31fcd6b487ec5acb7aba0d646afcbc9c2993149cb92388ae7f0de5b2e69de228", "text": "exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.", "start_char_idx": 1259, "end_char_idx": 1662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e1b070a8-3dd3-4f0c-8cca-597592d1b1df": {"__data__": {"id_": "e1b070a8-3dd3-4f0c-8cca-597592d1b1df", "embedding": null, "metadata": {"title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models", "venue": "International Conference on Artificial Intelligence and Statistics", "year": 2022, "paperId": "9dcee248452d84b6bf26911ba6726ae5ce1a46f3", "citationCount": 39, "openAccessPdf": null, "authors": ["S. Hegselmann", "Alejandro Buendia", "Hunter Lang", "Monica Agrawal", "Xiaoyi Jiang", "D. Sontag"], "externalIds": {"DBLP": "journals/corr/abs-2210-10723", "ArXiv": "2210.10723", "DOI": "10.48550/arXiv.2210.10723", "CorpusId": 252992811}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dad8280-c405-4ef5-b229-6d47ff7480af", "node_type": null, "metadata": {"title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models", "venue": "International Conference on Artificial Intelligence and Statistics", "year": 2022, "paperId": "9dcee248452d84b6bf26911ba6726ae5ce1a46f3", "citationCount": 39, "openAccessPdf": null, "authors": ["S. Hegselmann", "Alejandro Buendia", "Hunter Lang", "Monica Agrawal", "Xiaoyi Jiang", "D. Sontag"], "externalIds": {"DBLP": "journals/corr/abs-2210-10723", "ArXiv": "2210.10723", "DOI": "10.48550/arXiv.2210.10723", "CorpusId": 252992811}}, "hash": "c586c44df2005b4e16a1b5de9cc9101cc1d1e78a17d07fda4014854563af2841"}}, "hash": "c586c44df2005b4e16a1b5de9cc9101cc1d1e78a17d07fda4014854563af2841", "text": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.", "start_char_idx": 0, "end_char_idx": 1076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "98dc8d84-7e4c-46bf-a3cf-e55e51ef8907": {"__data__": {"id_": "98dc8d84-7e4c-46bf-a3cf-e55e51ef8907", "embedding": null, "metadata": {"title": "Evaluating Large Language Models", "venue": "", "year": 2022, "paperId": "4b2137280915ccc0e06e97b604778b05876a34ad", "citationCount": 95, "openAccessPdf": null, "authors": [], "externalIds": {"CorpusId": 247456179}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b4da62b-3fd8-44c7-9c88-d589ff00f28c", "node_type": null, "metadata": {"title": "Evaluating Large Language Models", "venue": "", "year": 2022, "paperId": "4b2137280915ccc0e06e97b604778b05876a34ad", "citationCount": 95, "openAccessPdf": null, "authors": [], "externalIds": {"CorpusId": 247456179}}, "hash": "0bd77eae9d148a9522824a9c45fb6e0f3b4bad478828d52290068899720e9066"}}, "hash": "0bd77eae9d148a9522824a9c45fb6e0f3b4bad478828d52290068899720e9066", "text": "Evaluating Large Language Models Natural language inference is a complex task with a broader space of decisions on how to prompt LLMs. We work with the three-way classification formulation of the task. Each input in the dataset is a pair of contexts (the premise and the hypothesis): the task is to predict whether the hypothesis is entailed (i.e. always true), contradicted (i.e. always false), or neutral (neither entailed nor contradicted) given the premise. See [6] for further discussion of this task. We use the ANLI dataset [17] with code provided to load the dataset. Two examples from the dataset are shown in Figure 1. ANLI was constructed through an adversarial and iterative data collection process: simply put, the examples in ANLI are quite challenging by design. We use Round 3 of ANLI: in the GPT-3 paper [7], the", "start_char_idx": 0, "end_char_idx": 829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b059fe23-a328-4005-a04b-371267840166": {"__data__": {"id_": "b059fe23-a328-4005-a04b-371267840166", "embedding": null, "metadata": {"title": "Large Language Models Are Reasoning Teachers", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44", "citationCount": 64, "openAccessPdf": null, "authors": ["Namgyu Ho", "Laura Schmid", "Se-Young Yun"], "externalIds": {"ACL": "2023.acl-long.830", "DBLP": "journals/corr/abs-2212-10071", "ArXiv": "2212.10071", "DOI": "10.48550/arXiv.2212.10071", "CorpusId": 254877399}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72cd9b3f-a353-4133-b1e5-fe98326c95d8", "node_type": null, "metadata": {"title": "Large Language Models Are Reasoning Teachers", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44", "citationCount": 64, "openAccessPdf": null, "authors": ["Namgyu Ho", "Laura Schmid", "Se-Young Yun"], "externalIds": {"ACL": "2023.acl-long.830", "DBLP": "journals/corr/abs-2212-10071", "ArXiv": "2212.10071", "DOI": "10.48550/arXiv.2212.10071", "CorpusId": 254877399}}, "hash": "f717280d3b579648f915e2e2e3bd01a557b79c74892e047b5c195554e9f78f1c"}}, "hash": "f717280d3b579648f915e2e2e3bd01a557b79c74892e047b5c195554e9f78f1c", "text": "Large Language Models Are Reasoning Teachers Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model\u2019s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.", "start_char_idx": 0, "end_char_idx": 1360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e214ebbe-a4a2-4577-a1fa-6529171c905d": {"__data__": {"id_": "e214ebbe-a4a2-4577-a1fa-6529171c905d", "embedding": null, "metadata": {"title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8ee45aeb7c97e3346cc62f216f673b91277ac718", "citationCount": 43, "openAccessPdf": null, "authors": ["Chan Hee Song", "Jiaman Wu", "Clay Washington", "Brian M. Sadler", "Wei-Lun Chao", "Yu Su"], "externalIds": {"ArXiv": "2212.04088", "DBLP": "journals/corr/abs-2212-04088", "DOI": "10.48550/arXiv.2212.04088", "CorpusId": 254408960}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cf0347a-2685-4c0f-b46e-51517d2b82a5", "node_type": null, "metadata": {"title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8ee45aeb7c97e3346cc62f216f673b91277ac718", "citationCount": 43, "openAccessPdf": null, "authors": ["Chan Hee Song", "Jiaman Wu", "Clay Washington", "Brian M. Sadler", "Wei-Lun Chao", "Yu Su"], "externalIds": {"ArXiv": "2212.04088", "DBLP": "journals/corr/abs-2212-04088", "DOI": "10.48550/arXiv.2212.04088", "CorpusId": 254408960}}, "hash": "8ecc98606c375f73cac0759dc2d30f3601e0fcc2afd739442fafb31544026612"}}, "hash": "8ecc98606c375f73cac0759dc2d30f3601e0fcc2afd739442fafb31544026612", "text": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner", "start_char_idx": 0, "end_char_idx": 1303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4e335f4c-1af9-48fe-95da-4351af74a1aa": {"__data__": {"id_": "4e335f4c-1af9-48fe-95da-4351af74a1aa", "embedding": null, "metadata": {"title": "Explanations from Large Language Models Make Small Reasoners Better", "venue": "arXiv.org", "year": 2022, "paperId": "7d29a84a589aa5655e5d3fed8d725ea472816599", "citationCount": 41, "openAccessPdf": null, "authors": ["SHIYANG LI", "Jianshu Chen", "Yelong Shen", "Zhiyu Chen", "Xinlu Zhang", "Zekun Li", "Hong Wang", "Jingu Qian", "Baolin Peng", "Yi Mao", "Wenhu Chen", "Xifeng Yan"], "externalIds": {"ArXiv": "2210.06726", "DBLP": "journals/corr/abs-2210-06726", "DOI": "10.48550/arXiv.2210.06726", "CorpusId": 252873123}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "971c9ea2-0ff3-4d54-8971-a2c3dbe03caa", "node_type": null, "metadata": {"title": "Explanations from Large Language Models Make Small Reasoners Better", "venue": "arXiv.org", "year": 2022, "paperId": "7d29a84a589aa5655e5d3fed8d725ea472816599", "citationCount": 41, "openAccessPdf": null, "authors": ["SHIYANG LI", "Jianshu Chen", "Yelong Shen", "Zhiyu Chen", "Xinlu Zhang", "Zekun Li", "Hong Wang", "Jingu Qian", "Baolin Peng", "Yi Mao", "Wenhu Chen", "Xifeng Yan"], "externalIds": {"ArXiv": "2210.06726", "DBLP": "journals/corr/abs-2210-06726", "DOI": "10.48550/arXiv.2210.06726", "CorpusId": 252873123}}, "hash": "3c118348fef64501188a5b278f8a965fbf811206f4c20bf99e550e0ac38f3f81"}}, "hash": "3c118348fef64501188a5b278f8a965fbf811206f4c20bf99e550e0ac38f3f81", "text": "Explanations from Large Language Models Make Small Reasoners Better Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.", "start_char_idx": 0, "end_char_idx": 1121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fb7fb34c-8b57-421b-99b4-09017b849864": {"__data__": {"id_": "fb7fb34c-8b57-421b-99b4-09017b849864", "embedding": null, "metadata": {"title": "Visual Classification via Description from Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "a42b091adaf29b06a092b67192ac07cb93312f2a", "citationCount": 58, "openAccessPdf": null, "authors": ["Sachit Menon", "Carl Vondrick"], "externalIds": {"DBLP": "conf/iclr/MenonV23", "ArXiv": "2210.07183", "DOI": "10.48550/arXiv.2210.07183", "CorpusId": 252872997}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdf9b041-fd18-49af-9797-61ea9bc8af67", "node_type": null, "metadata": {"title": "Visual Classification via Description from Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "a42b091adaf29b06a092b67192ac07cb93312f2a", "citationCount": 58, "openAccessPdf": null, "authors": ["Sachit Menon", "Carl Vondrick"], "externalIds": {"DBLP": "conf/iclr/MenonV23", "ArXiv": "2210.07183", "DOI": "10.48550/arXiv.2210.07183", "CorpusId": 252872997}}, "hash": "9adb080aeb8cdb24e0f4acead18debd33403cb0b91fd7c24505e22b50a28ed20"}}, "hash": "9adb080aeb8cdb24e0f4acead18debd33403cb0b91fd7c24505e22b50a28ed20", "text": "Visual Classification via Description from Large Language Models Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.", "start_char_idx": 0, "end_char_idx": 1584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3300d303-99a4-4525-a60d-d1752aeebd7a": {"__data__": {"id_": "3300d303-99a4-4525-a60d-d1752aeebd7a", "embedding": null, "metadata": {"title": "PromptMaker: Prompt-based Prototyping with Large\u00a0Language\u00a0Models", "venue": "CHI Extended Abstracts", "year": 2022, "paperId": "18a648a2c2d873602708a0a7f96be3ec716f6b1a", "citationCount": 45, "openAccessPdf": null, "authors": ["Ellen Jiang", "Kristen Olson", "Edwin Toh", "A. Molina", "Aaron Donsbach", "Michael Terry", "Carrie J. Cai"], "externalIds": {"DBLP": "conf/chi/JiangOTMDTC22", "DOI": "10.1145/3491101.3503564", "CorpusId": 248419856}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "226c7a90-ff29-42bd-8d04-9f3bd74d1ea4", "node_type": null, "metadata": {"title": "PromptMaker: Prompt-based Prototyping with Large\u00a0Language\u00a0Models", "venue": "CHI Extended Abstracts", "year": 2022, "paperId": "18a648a2c2d873602708a0a7f96be3ec716f6b1a", "citationCount": 45, "openAccessPdf": null, "authors": ["Ellen Jiang", "Kristen Olson", "Edwin Toh", "A. Molina", "Aaron Donsbach", "Michael Terry", "Carrie J. Cai"], "externalIds": {"DBLP": "conf/chi/JiangOTMDTC22", "DOI": "10.1145/3491101.3503564", "CorpusId": 248419856}}, "hash": "8b38a91476ebd3d4d1ea8a844bd141f8b89418939bbb4d626bf98de714ac5395"}}, "hash": "8b38a91476ebd3d4d1ea8a844bd141f8b89418939bbb4d626bf98de714ac5395", "text": "PromptMaker: Prompt-based Prototyping with Large\u00a0Language\u00a0Models Prototyping is notoriously difficult to do with machine learning (ML), but recent advances in large language models may lower the barriers to people prototyping with ML, through the use of natural language prompts. This case study reports on the real-world experiences of industry professionals (e.g. designers, program managers, front-end developers) prototyping new ML-powered feature ideas via prompt-based prototyping. Through interviews with eleven practitioners during a three-week sprint and a workshop, we find that prompt-based prototyping reduced barriers of access by substantially broadening who can prototype with ML, sped up the prototyping process, and grounded communication between collaborators. Yet, it also introduced new challenges, such as the need to reverse-engineer prompt designs, source example data, and debug and evaluate prompt effectiveness. Taken together, this case study provides important implications that lay the groundwork toward a new future of prototyping with ML.", "start_char_idx": 0, "end_char_idx": 1069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "95bd1c65-a23b-4c37-b038-e94823a5f0a4": {"__data__": {"id_": "95bd1c65-a23b-4c37-b038-e94823a5f0a4", "embedding": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5", "node_type": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "hash": "cb19ddb7e0f44d55e2ed9bacc5ba9d429d3941e20e7a6ae4ffeda8f95779b9d6"}, "3": {"node_id": "836e5ae1-10df-4b38-958e-4f88496998f1", "node_type": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "hash": "64c24835d888bd0372e15336f812275f806d771f9e1f5fab9bc1462de77022c3"}}, "hash": "5bc55f8bf4b8e51246ad826930b138fd98ddcf43b86ebfe696f0483e1521d0f4", "text": "Language Models are Few-Shot Learners Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an", "start_char_idx": 0, "end_char_idx": 766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "836e5ae1-10df-4b38-958e-4f88496998f1": {"__data__": {"id_": "836e5ae1-10df-4b38-958e-4f88496998f1", "embedding": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5", "node_type": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "hash": "cb19ddb7e0f44d55e2ed9bacc5ba9d429d3941e20e7a6ae4ffeda8f95779b9d6"}, "2": {"node_id": "95bd1c65-a23b-4c37-b038-e94823a5f0a4", "node_type": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "hash": "5bc55f8bf4b8e51246ad826930b138fd98ddcf43b86ebfe696f0483e1521d0f4"}, "3": {"node_id": "4e80bd3b-2ecd-46cd-838a-292278252d37", "node_type": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "hash": "bf77aac8fa1cc651246d0af58021e46ff0e409eab39551e37c262991da926a38"}}, "hash": "64c24835d888bd0372e15336f812275f806d771f9e1f5fab9bc1462de77022c3", "text": "fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where", "start_char_idx": 717, "end_char_idx": 1454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4e80bd3b-2ecd-46cd-838a-292278252d37": {"__data__": {"id_": "4e80bd3b-2ecd-46cd-838a-292278252d37", "embedding": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5", "node_type": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "hash": "cb19ddb7e0f44d55e2ed9bacc5ba9d429d3941e20e7a6ae4ffeda8f95779b9d6"}, "2": {"node_id": "836e5ae1-10df-4b38-958e-4f88496998f1", "node_type": null, "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}, "hash": "64c24835d888bd0372e15336f812275f806d771f9e1f5fab9bc1462de77022c3"}}, "hash": "bf77aac8fa1cc651246d0af58021e46ff0e409eab39551e37c262991da926a38", "text": "3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "start_char_idx": 1430, "end_char_idx": 1884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "42988a74-efa9-4291-bde2-f2e52df2387f": {"__data__": {"id_": "42988a74-efa9-4291-bde2-f2e52df2387f", "embedding": null, "metadata": {"title": "The debate over understanding in AI\u2019s large language models", "venue": "Proceedings of the National Academy of Sciences of the United States of America", "year": 2022, "paperId": "e32185936ab3b23f39b1dd93e1507e6d80a71776", "citationCount": 46, "openAccessPdf": null, "authors": ["M. Mitchell", "D. Krakauer"], "externalIds": {"PubMedCentral": "10068812", "ArXiv": "2210.13966", "DBLP": "journals/corr/abs-2210-13966", "DOI": "10.1073/pnas.2215907120", "CorpusId": 253107905, "PubMed": "36943882"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bcc2ee4a-a361-4d0e-90f6-481b167db6d7", "node_type": null, "metadata": {"title": "The debate over understanding in AI\u2019s large language models", "venue": "Proceedings of the National Academy of Sciences of the United States of America", "year": 2022, "paperId": "e32185936ab3b23f39b1dd93e1507e6d80a71776", "citationCount": 46, "openAccessPdf": null, "authors": ["M. Mitchell", "D. Krakauer"], "externalIds": {"PubMedCentral": "10068812", "ArXiv": "2210.13966", "DBLP": "journals/corr/abs-2210-13966", "DOI": "10.1073/pnas.2215907120", "CorpusId": 253107905, "PubMed": "36943882"}}, "hash": "0ea14251b3211fc52800013601b0d3c7f64d4a59a7c06be0d189a8973cb1a993"}}, "hash": "0ea14251b3211fc52800013601b0d3c7f64d4a59a7c06be0d189a8973cb1a993", "text": "The debate over understanding in AI\u2019s large language models We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language\u2014and the physical and social situations language encodes\u2014in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.", "start_char_idx": 0, "end_char_idx": 717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fe365372-01e6-489b-a56f-7dc9bc921877": {"__data__": {"id_": "fe365372-01e6-489b-a56f-7dc9bc921877", "embedding": null, "metadata": {"title": "Evaluating the Text-to-SQL Capabilities of Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "51000d9f79be0eefd7972fe94e3c71dddc90d2c6", "citationCount": 54, "openAccessPdf": null, "authors": ["Nitarshan Rajkumar", "Raymond Li", "Dzmitry Bahdanau"], "externalIds": {"DBLP": "journals/corr/abs-2204-00498", "ArXiv": "2204.00498", "DOI": "10.48550/arXiv.2204.00498", "CorpusId": 247922681}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c87d542-9d83-4107-9294-e3f7606fc9ab", "node_type": null, "metadata": {"title": "Evaluating the Text-to-SQL Capabilities of Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "51000d9f79be0eefd7972fe94e3c71dddc90d2c6", "citationCount": 54, "openAccessPdf": null, "authors": ["Nitarshan Rajkumar", "Raymond Li", "Dzmitry Bahdanau"], "externalIds": {"DBLP": "journals/corr/abs-2204-00498", "ArXiv": "2204.00498", "DOI": "10.48550/arXiv.2204.00498", "CorpusId": 247922681}}, "hash": "8de13bd4f2787cb7053dbc98c45cea18a27bcc9babb8aa6775edb1ac6ed0c65d"}}, "hash": "8de13bd4f2787cb7053dbc98c45cea18a27bcc9babb8aa6775edb1ac6ed0c65d", "text": "Evaluating the Text-to-SQL Capabilities of Large Language Models We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.", "start_char_idx": 0, "end_char_idx": 536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3bdd2053-dd18-429e-afb5-8da4ceec0311": {"__data__": {"id_": "3bdd2053-dd18-429e-afb5-8da4ceec0311", "embedding": null, "metadata": {"title": "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book", "venue": "Technical Symposium on Computer Science Education", "year": 2022, "paperId": "2abed82162c47a0cc32cd62afcf46b0745541017", "citationCount": 40, "openAccessPdf": "https://arxiv.org/pdf/2211.02265", "authors": ["S. MacNeil", "Andrew Tran", "Arto Hellas", "Joanne Kim", "Sami Sarsa", "Paul Denny", "Seth Bernstein", "Juho Leinonen"], "externalIds": {"DBLP": "journals/corr/abs-2211-02265", "ArXiv": "2211.02265", "DOI": "10.1145/3545945.3569785", "CorpusId": 253370333}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9612f788-1e71-4259-9bb3-8cbfffe0238e", "node_type": null, "metadata": {"title": "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book", "venue": "Technical Symposium on Computer Science Education", "year": 2022, "paperId": "2abed82162c47a0cc32cd62afcf46b0745541017", "citationCount": 40, "openAccessPdf": "https://arxiv.org/pdf/2211.02265", "authors": ["S. MacNeil", "Andrew Tran", "Arto Hellas", "Joanne Kim", "Sami Sarsa", "Paul Denny", "Seth Bernstein", "Juho Leinonen"], "externalIds": {"DBLP": "journals/corr/abs-2211-02265", "ArXiv": "2211.02265", "DOI": "10.1145/3545945.3569785", "CorpusId": 253370333}}, "hash": "ffdb67c4beb11c397a1da77145e8fe190d4258e86940b70531a418754a6a7d77"}}, "hash": "ffdb67c4beb11c397a1da77145e8fe190d4258e86940b70531a418754a6a7d77", "text": "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.", "start_char_idx": 0, "end_char_idx": 1091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "89636cfc-5d09-437b-9b35-c8f89bbd3b4b": {"__data__": {"id_": "89636cfc-5d09-437b-9b35-c8f89bbd3b4b", "embedding": null, "metadata": {"title": "Meaning without reference in large language models", "venue": "arXiv.org", "year": 2022, "paperId": "50296a5814c4ac7f58f3b0177233a8f63c701565", "citationCount": 43, "openAccessPdf": null, "authors": ["S. Piantadosi", "Felix Hill"], "externalIds": {"ArXiv": "2208.02957", "DBLP": "journals/corr/abs-2208-02957", "DOI": "10.48550/arXiv.2208.02957", "CorpusId": 251371595}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a28b6264-bab4-42a5-be83-1ca8a5d9dda7", "node_type": null, "metadata": {"title": "Meaning without reference in large language models", "venue": "arXiv.org", "year": 2022, "paperId": "50296a5814c4ac7f58f3b0177233a8f63c701565", "citationCount": 43, "openAccessPdf": null, "authors": ["S. Piantadosi", "Felix Hill"], "externalIds": {"ArXiv": "2208.02957", "DBLP": "journals/corr/abs-2208-02957", "DOI": "10.48550/arXiv.2208.02957", "CorpusId": 251371595}}, "hash": "970c0d15e8ed4d8feed02782bd9e3676359e3a8a41aec7b196c46993f33175a3"}}, "hash": "970c0d15e8ed4d8feed02782bd9e3676359e3a8a41aec7b196c46993f33175a3", "text": "Meaning without reference in large language models The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.", "start_char_idx": 0, "end_char_idx": 827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d53ace1a-ac06-4d66-9474-ffc7a89f4aca": {"__data__": {"id_": "d53ace1a-ac06-4d66-9474-ffc7a89f4aca", "embedding": null, "metadata": {"title": "Leveraging Large Language Models for Multiple Choice Question Answering", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "ed38c6b157c11476939c426ec6871c926f2f3524", "citationCount": 34, "openAccessPdf": null, "authors": ["Joshua Robinson", "Christopher Rytting", "D. Wingate"], "externalIds": {"ArXiv": "2210.12353", "DBLP": "conf/iclr/RobinsonW23", "DOI": "10.48550/arXiv.2210.12353", "CorpusId": 253098700}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df9c9841-0cb0-417c-8738-1a527502e9f6", "node_type": null, "metadata": {"title": "Leveraging Large Language Models for Multiple Choice Question Answering", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "ed38c6b157c11476939c426ec6871c926f2f3524", "citationCount": 34, "openAccessPdf": null, "authors": ["Joshua Robinson", "Christopher Rytting", "D. Wingate"], "externalIds": {"ArXiv": "2210.12353", "DBLP": "conf/iclr/RobinsonW23", "DOI": "10.48550/arXiv.2210.12353", "CorpusId": 253098700}}, "hash": "23d81c93e46cf261a8bdf1f0e1fa5b22885aa0c9f13b1ba92ed902b8e1d3e7be"}}, "hash": "23d81c93e46cf261a8bdf1f0e1fa5b22885aa0c9f13b1ba92ed902b8e1d3e7be", "text": "Leveraging Large Language Models for Multiple Choice Question Answering While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g.,\"A\") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.", "start_char_idx": 0, "end_char_idx": 1468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "645024ce-9b96-45ff-b5c9-3c509c1e32cf": {"__data__": {"id_": "645024ce-9b96-45ff-b5c9-3c509c1e32cf", "embedding": null, "metadata": {"title": "Large Language Models are reasoners with Self-Verification", "venue": "arXiv.org", "year": 2022, "paperId": "f02e8f1c9b5ab12ddfb1977570f9f5445a99a973", "citationCount": 31, "openAccessPdf": null, "authors": ["Yixuan Weng", "Minjun Zhu", "Bin Li", "Shizhu He", "Kang Liu", "Jun Zhao"], "externalIds": {"DBLP": "journals/corr/abs-2212-09561", "DOI": "10.48550/arXiv.2212.09561", "CorpusId": 254854206}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ade8cfe-c14b-4766-b7b9-7da722b55530", "node_type": null, "metadata": {"title": "Large Language Models are reasoners with Self-Verification", "venue": "arXiv.org", "year": 2022, "paperId": "f02e8f1c9b5ab12ddfb1977570f9f5445a99a973", "citationCount": 31, "openAccessPdf": null, "authors": ["Yixuan Weng", "Minjun Zhu", "Bin Li", "Shizhu He", "Kang Liu", "Jun Zhao"], "externalIds": {"DBLP": "journals/corr/abs-2212-09561", "DOI": "10.48550/arXiv.2212.09561", "CorpusId": 254854206}}, "hash": "c32a975e80382927b2c668efa0710bb840cb094b60a04bb0253532f9a079414b"}}, "hash": "c32a975e80382927b2c668efa0710bb840cb094b60a04bb0253532f9a079414b", "text": "Large Language Models are reasoners with Self-Verification When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from incorrect CoT. 1", "start_char_idx": 0, "end_char_idx": 1071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4810a3a5-ffb3-4f38-ab8e-9a45a5df5d7e": {"__data__": {"id_": "4810a3a5-ffb3-4f38-ab8e-9a45a5df5d7e", "embedding": null, "metadata": {"title": "Enabling Conversational Interaction with Mobile UI using Large Language Models", "venue": "International Conference on Human Factors in Computing Systems", "year": 2022, "paperId": "99070fb6df9e8d11e30f7aaefcc9f0b0c5a73789", "citationCount": 29, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3544548.3580895", "authors": ["Bryan Wang", "Gang Li", "Yang Li"], "externalIds": {"ArXiv": "2209.08655", "DBLP": "journals/corr/abs-2209-08655", "DOI": "10.1145/3544548.3580895", "CorpusId": 252367445}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a2f9da6-ebbf-47d0-9f32-21325ea11075", "node_type": null, "metadata": {"title": "Enabling Conversational Interaction with Mobile UI using Large Language Models", "venue": "International Conference on Human Factors in Computing Systems", "year": 2022, "paperId": "99070fb6df9e8d11e30f7aaefcc9f0b0c5a73789", "citationCount": 29, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3544548.3580895", "authors": ["Bryan Wang", "Gang Li", "Yang Li"], "externalIds": {"ArXiv": "2209.08655", "DBLP": "journals/corr/abs-2209-08655", "DOI": "10.1145/3544548.3580895", "CorpusId": 252367445}}, "hash": "a3b8226bb10ec2f14456a6e8c4ff9642002e6c903a8f1b43223da5e0ad66c4de"}}, "hash": "a3b8226bb10ec2f14456a6e8c4ff9642002e6c903a8f1b43223da5e0ad66c4de", "text": "Enabling Conversational Interaction with Mobile UI using Large Language Models Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.", "start_char_idx": 0, "end_char_idx": 1072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d4eb84e9-227d-4603-9d86-56ffbde4e943": {"__data__": {"id_": "d4eb84e9-227d-4603-9d86-56ffbde4e943", "embedding": null, "metadata": {"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "citationCount": 53, "openAccessPdf": null, "authors": ["Eldar Kurtic", "Daniel Fernando Campos", "Tuan Nguyen", "Elias Frantar", "Mark Kurtz", "Ben Fineran", "M. Goin", "Dan Alistarh"], "externalIds": {"ACL": "2022.emnlp-main.279", "DBLP": "conf/emnlp/KurticCNFKFGA22", "ArXiv": "2203.07259", "DOI": "10.48550/arXiv.2203.07259", "CorpusId": 247446572}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c0fc807-c715-426f-9647-b320bdeda630", "node_type": null, "metadata": {"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "citationCount": 53, "openAccessPdf": null, "authors": ["Eldar Kurtic", "Daniel Fernando Campos", "Tuan Nguyen", "Elias Frantar", "Mark Kurtz", "Ben Fineran", "M. Goin", "Dan Alistarh"], "externalIds": {"ACL": "2022.emnlp-main.279", "DBLP": "conf/emnlp/KurticCNFKFGA22", "ArXiv": "2203.07259", "DOI": "10.48550/arXiv.2203.07259", "CorpusId": 247446572}}, "hash": "640631aed6ebf90a07e9b4c22a1c7a18b6e79c38927cdecd84b00b83c5814e39"}, "3": {"node_id": "0e858d36-dd07-4ecd-9906-9fde5dd3c8d2", "node_type": null, "metadata": {"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "citationCount": 53, "openAccessPdf": null, "authors": ["Eldar Kurtic", "Daniel Fernando Campos", "Tuan Nguyen", "Elias Frantar", "Mark Kurtz", "Ben Fineran", "M. Goin", "Dan Alistarh"], "externalIds": {"ACL": "2022.emnlp-main.279", "DBLP": "conf/emnlp/KurticCNFKFGA22", "ArXiv": "2203.07259", "DOI": "10.48550/arXiv.2203.07259", "CorpusId": 247446572}}, "hash": "4fa54ac4c7d64cc6f0c3d14b07af488eb3b56e705cc280b211756501600bd4c2"}}, "hash": "35011ff6073cf894e1c80be1c5e74f06c7fa9d9e58565062ee7d15e207ffc940", "text": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate second-order information, which we show to yield state-of-the-art results in both stages of language tasks: pre-training and fine-tuning. Specifically, oBERT extends existing work on second-order pruning by allowing for pruning weight blocks, and is the first such method that is applicable at BERT scale. Second, we investigate compounding compression approaches to obtain highly compressed but accurate models for deployment on edge devices. These models significantly push boundaries of the current state-of-the-art sparse BERT models with respect to all metrics: model size, inference speed and task accuracy. For example, relative to the dense BERT-base, we obtain 10x model size compression with < 1% accuracy drop, 10x CPU-inference speedup with < 2% accuracy drop, and 29x", "start_char_idx": 0, "end_char_idx": 1172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0e858d36-dd07-4ecd-9906-9fde5dd3c8d2": {"__data__": {"id_": "0e858d36-dd07-4ecd-9906-9fde5dd3c8d2", "embedding": null, "metadata": {"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "citationCount": 53, "openAccessPdf": null, "authors": ["Eldar Kurtic", "Daniel Fernando Campos", "Tuan Nguyen", "Elias Frantar", "Mark Kurtz", "Ben Fineran", "M. Goin", "Dan Alistarh"], "externalIds": {"ACL": "2022.emnlp-main.279", "DBLP": "conf/emnlp/KurticCNFKFGA22", "ArXiv": "2203.07259", "DOI": "10.48550/arXiv.2203.07259", "CorpusId": 247446572}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c0fc807-c715-426f-9647-b320bdeda630", "node_type": null, "metadata": {"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "citationCount": 53, "openAccessPdf": null, "authors": ["Eldar Kurtic", "Daniel Fernando Campos", "Tuan Nguyen", "Elias Frantar", "Mark Kurtz", "Ben Fineran", "M. Goin", "Dan Alistarh"], "externalIds": {"ACL": "2022.emnlp-main.279", "DBLP": "conf/emnlp/KurticCNFKFGA22", "ArXiv": "2203.07259", "DOI": "10.48550/arXiv.2203.07259", "CorpusId": 247446572}}, "hash": "640631aed6ebf90a07e9b4c22a1c7a18b6e79c38927cdecd84b00b83c5814e39"}, "2": {"node_id": "d4eb84e9-227d-4603-9d86-56ffbde4e943", "node_type": null, "metadata": {"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "citationCount": 53, "openAccessPdf": null, "authors": ["Eldar Kurtic", "Daniel Fernando Campos", "Tuan Nguyen", "Elias Frantar", "Mark Kurtz", "Ben Fineran", "M. Goin", "Dan Alistarh"], "externalIds": {"ACL": "2022.emnlp-main.279", "DBLP": "conf/emnlp/KurticCNFKFGA22", "ArXiv": "2203.07259", "DOI": "10.48550/arXiv.2203.07259", "CorpusId": 247446572}}, "hash": "35011ff6073cf894e1c80be1c5e74f06c7fa9d9e58565062ee7d15e207ffc940"}}, "hash": "4fa54ac4c7d64cc6f0c3d14b07af488eb3b56e705cc280b211756501600bd4c2", "text": "10x CPU-inference speedup with < 2% accuracy drop, and 29x CPU-inference speedup with < 7.5% accuracy drop. Our code, fully integrated with Transformers and SparseML, is available at https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.", "start_char_idx": 1114, "end_char_idx": 1383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0501062b-74f1-4602-9fd4-c970c0cff831": {"__data__": {"id_": "0501062b-74f1-4602-9fd4-c970c0cff831", "embedding": null, "metadata": {"title": "SKILL: Structured Knowledge Infusion for Large Language Models", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "eea2129457fcd78c4071a9020355a2fe1da4d2fd", "citationCount": 27, "openAccessPdf": "https://aclanthology.org/2022.naacl-main.113.pdf", "authors": ["Fedor Moiseev", "Zhe Dong", "Enrique Alfonseca", "Martin Jaggi"], "externalIds": {"DBLP": "journals/corr/abs-2205-08184", "ArXiv": "2205.08184", "ACL": "2022.naacl-main.113", "DOI": "10.18653/v1/2022.naacl-main.113", "CorpusId": 248834551}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24deb67b-4795-4a2a-9203-89b7fefd583e", "node_type": null, "metadata": {"title": "SKILL: Structured Knowledge Infusion for Large Language Models", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "eea2129457fcd78c4071a9020355a2fe1da4d2fd", "citationCount": 27, "openAccessPdf": "https://aclanthology.org/2022.naacl-main.113.pdf", "authors": ["Fedor Moiseev", "Zhe Dong", "Enrique Alfonseca", "Martin Jaggi"], "externalIds": {"DBLP": "journals/corr/abs-2205-08184", "ArXiv": "2205.08184", "ACL": "2022.naacl-main.113", "DOI": "10.18653/v1/2022.naacl-main.113", "CorpusId": 248834551}}, "hash": "cf0824a081550d6e829b85e445f88e4c561d248d0e08e37f849b24004a974940"}}, "hash": "cf0824a081550d6e829b85e445f88e4c561d248d0e08e37f849b24004a974940", "text": "SKILL: Structured Knowledge Infusion for Large Language Models Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.", "start_char_idx": 0, "end_char_idx": 1144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5382996a-1111-4639-a45a-20d037308d79": {"__data__": {"id_": "5382996a-1111-4639-a45a-20d037308d79", "embedding": null, "metadata": {"title": "Prompting Is Programming: A Query Language for Large Language Models", "venue": "Proc. ACM Program. Lang.", "year": 2022, "paperId": "c2329c685f11efa25c562f97be71ff03103423fd", "citationCount": 25, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3591300", "authors": ["Luca Beurer-Kellner", "Marc Fischer", "Martin T. Vechev"], "externalIds": {"DBLP": "journals/corr/abs-2212-06094", "ArXiv": "2212.06094", "DOI": "10.1145/3591300", "CorpusId": 254564450}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd9ab116-ba7f-4b46-b567-115a805d403d", "node_type": null, "metadata": {"title": "Prompting Is Programming: A Query Language for Large Language Models", "venue": "Proc. ACM Program. Lang.", "year": 2022, "paperId": "c2329c685f11efa25c562f97be71ff03103423fd", "citationCount": 25, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3591300", "authors": ["Luca Beurer-Kellner", "Marc Fischer", "Martin T. Vechev"], "externalIds": {"DBLP": "journals/corr/abs-2212-06094", "ArXiv": "2212.06094", "DOI": "10.1145/3591300", "CorpusId": 254564450}}, "hash": "85cbec6cf844ae4fac60fa716505bbb3b0aad637875f45370a63f21e98d62635"}, "3": {"node_id": "f7fdca32-024d-436c-ab9d-87a0c8f56200", "node_type": null, "metadata": {"title": "Prompting Is Programming: A Query Language for Large Language Models", "venue": "Proc. ACM Program. Lang.", "year": 2022, "paperId": "c2329c685f11efa25c562f97be71ff03103423fd", "citationCount": 25, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3591300", "authors": ["Luca Beurer-Kellner", "Marc Fischer", "Martin T. Vechev"], "externalIds": {"DBLP": "journals/corr/abs-2212-06094", "ArXiv": "2212.06094", "DOI": "10.1145/3591300", "CorpusId": 254564450}}, "hash": "f1ad3c517051ac5c8d07c924ee098988c890fe54a146dbc4049a14be7b69c734"}}, "hash": "e0df3046bfd3d9765ecc8fe57afaa9c5105aeae13004fd716b04c84fd0cb70f9", "text": "Prompting Is Programming: A Query Language for Large Language Models Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction. Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics. To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We", "start_char_idx": 0, "end_char_idx": 1474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f7fdca32-024d-436c-ab9d-87a0c8f56200": {"__data__": {"id_": "f7fdca32-024d-436c-ab9d-87a0c8f56200", "embedding": null, "metadata": {"title": "Prompting Is Programming: A Query Language for Large Language Models", "venue": "Proc. ACM Program. Lang.", "year": 2022, "paperId": "c2329c685f11efa25c562f97be71ff03103423fd", "citationCount": 25, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3591300", "authors": ["Luca Beurer-Kellner", "Marc Fischer", "Martin T. Vechev"], "externalIds": {"DBLP": "journals/corr/abs-2212-06094", "ArXiv": "2212.06094", "DOI": "10.1145/3591300", "CorpusId": 254564450}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd9ab116-ba7f-4b46-b567-115a805d403d", "node_type": null, "metadata": {"title": "Prompting Is Programming: A Query Language for Large Language Models", "venue": "Proc. ACM Program. Lang.", "year": 2022, "paperId": "c2329c685f11efa25c562f97be71ff03103423fd", "citationCount": 25, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3591300", "authors": ["Luca Beurer-Kellner", "Marc Fischer", "Martin T. Vechev"], "externalIds": {"DBLP": "journals/corr/abs-2212-06094", "ArXiv": "2212.06094", "DOI": "10.1145/3591300", "CorpusId": 254564450}}, "hash": "85cbec6cf844ae4fac60fa716505bbb3b0aad637875f45370a63f21e98d62635"}, "2": {"node_id": "5382996a-1111-4639-a45a-20d037308d79", "node_type": null, "metadata": {"title": "Prompting Is Programming: A Query Language for Large Language Models", "venue": "Proc. ACM Program. Lang.", "year": 2022, "paperId": "c2329c685f11efa25c562f97be71ff03103423fd", "citationCount": 25, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3591300", "authors": ["Luca Beurer-Kellner", "Marc Fischer", "Martin T. Vechev"], "externalIds": {"DBLP": "journals/corr/abs-2212-06094", "ArXiv": "2212.06094", "DOI": "10.1145/3591300", "CorpusId": 254564450}}, "hash": "e0df3046bfd3d9765ecc8fe57afaa9c5105aeae13004fd716b04c84fd0cb70f9"}}, "hash": "f1ad3c517051ac5c8d07c924ee098988c890fe54a146dbc4049a14be7b69c734", "text": "that minimizes the number of expensive calls to the underlying language model. We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).", "start_char_idx": 1393, "end_char_idx": 1901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cf0a99d4-e2a4-4989-8f57-b3cf3d48286b": {"__data__": {"id_": "cf0a99d4-e2a4-4989-8f57-b3cf3d48286b", "embedding": null, "metadata": {"title": "Learning Video Representations from Large Language Models", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "paperId": "933b37b21e9d61139660088adb032ff3fdf56d86", "citationCount": 23, "openAccessPdf": "https://arxiv.org/pdf/2212.04501", "authors": ["Yue Zhao", "Ishan Misra", "Philipp Krahenbuhl", "Rohit Girdhar"], "externalIds": {"ArXiv": "2212.04501", "DBLP": "conf/cvpr/0006MKG23", "DOI": "10.1109/CVPR52729.2023.00637", "CorpusId": 254408789}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "900f35db-fc9d-4fd1-bf21-8f466e18d253", "node_type": null, "metadata": {"title": "Learning Video Representations from Large Language Models", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "paperId": "933b37b21e9d61139660088adb032ff3fdf56d86", "citationCount": 23, "openAccessPdf": "https://arxiv.org/pdf/2212.04501", "authors": ["Yue Zhao", "Ishan Misra", "Philipp Krahenbuhl", "Rohit Girdhar"], "externalIds": {"ArXiv": "2212.04501", "DBLP": "conf/cvpr/0006MKG23", "DOI": "10.1109/CVPR52729.2023.00637", "CorpusId": 254408789}}, "hash": "1e4cf898fdc6a5f545ec3e3290dc0c29c384c5fb0116eb54be3f920cdde890c3"}}, "hash": "1e4cf898fdc6a5f545ec3e3290dc0c29c384c5fb0116eb54be3f920cdde890c3", "text": "Learning Video Representations from Large Language Models We introduce LAVILA, a new approach to learning video-language representations by leveraging Large Language Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and finetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text. The video-language embedding learned contrastively with these narrations outperforms the previous state-of-the-art on multiple first-person and third-person video tasks, both in zero-shot and finetuned setups. Most notably, Lavilaobtains an absolute gain of 10.1% on EGTEA classification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LaVilatrained with only half the narrations from the Ego4D dataset outperforms models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size.", "start_char_idx": 0, "end_char_idx": 1064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0aec158f-2af8-4b02-8188-33bb287bba3e": {"__data__": {"id_": "0aec158f-2af8-4b02-8188-33bb287bba3e", "embedding": null, "metadata": {"title": "Understanding HTML with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "6dd4743ee5430157c981a8dfe9a7434d99be2e8b", "citationCount": 17, "openAccessPdf": null, "authors": ["Izzeddin Gur", "Ofir Nachum", "Yingjie Miao", "Mustafa Safdari", "Austin Huang", "Aakanksha Chowdhery", "Sharan Narang", "Noah Fiedel", "Aleksandra Faust"], "externalIds": {"DBLP": "journals/corr/abs-2210-03945", "ArXiv": "2210.03945", "DOI": "10.48550/arXiv.2210.03945", "CorpusId": 252780086}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33555ccf-3b2c-4463-a1ba-dbf354d2b7d5", "node_type": null, "metadata": {"title": "Understanding HTML with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "6dd4743ee5430157c981a8dfe9a7434d99be2e8b", "citationCount": 17, "openAccessPdf": null, "authors": ["Izzeddin Gur", "Ofir Nachum", "Yingjie Miao", "Mustafa Safdari", "Austin Huang", "Aakanksha Chowdhery", "Sharan Narang", "Noah Fiedel", "Aleksandra Faust"], "externalIds": {"DBLP": "journals/corr/abs-2210-03945", "ArXiv": "2210.03945", "DOI": "10.48550/arXiv.2210.03945", "CorpusId": 252780086}}, "hash": "6fec11ce3bd90f1d4ff4f59291cd6d9af9ffb0b066eff9a3ae22321c9f8c3390"}, "3": {"node_id": "ae9138c8-f473-4bc8-aa3d-0b84845c0bd5", "node_type": null, "metadata": {"title": "Understanding HTML with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "6dd4743ee5430157c981a8dfe9a7434d99be2e8b", "citationCount": 17, "openAccessPdf": null, "authors": ["Izzeddin Gur", "Ofir Nachum", "Yingjie Miao", "Mustafa Safdari", "Austin Huang", "Aakanksha Chowdhery", "Sharan Narang", "Noah Fiedel", "Aleksandra Faust"], "externalIds": {"DBLP": "journals/corr/abs-2210-03945", "ArXiv": "2210.03945", "DOI": "10.48550/arXiv.2210.03945", "CorpusId": 252780086}}, "hash": "5162f2fe3f58a0fbc892b2aac969b638850ed968bb44f7fb41b625694cf378a7"}}, "hash": "c93249fb0fffb7a6ed51e35d801dee8d72322be9a1741b2f85a66ba93f2f53c2", "text": "Understanding HTML with Large Language Models Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture.", "start_char_idx": 0, "end_char_idx": 1304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae9138c8-f473-4bc8-aa3d-0b84845c0bd5": {"__data__": {"id_": "ae9138c8-f473-4bc8-aa3d-0b84845c0bd5", "embedding": null, "metadata": {"title": "Understanding HTML with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "6dd4743ee5430157c981a8dfe9a7434d99be2e8b", "citationCount": 17, "openAccessPdf": null, "authors": ["Izzeddin Gur", "Ofir Nachum", "Yingjie Miao", "Mustafa Safdari", "Austin Huang", "Aakanksha Chowdhery", "Sharan Narang", "Noah Fiedel", "Aleksandra Faust"], "externalIds": {"DBLP": "journals/corr/abs-2210-03945", "ArXiv": "2210.03945", "DOI": "10.48550/arXiv.2210.03945", "CorpusId": 252780086}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33555ccf-3b2c-4463-a1ba-dbf354d2b7d5", "node_type": null, "metadata": {"title": "Understanding HTML with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "6dd4743ee5430157c981a8dfe9a7434d99be2e8b", "citationCount": 17, "openAccessPdf": null, "authors": ["Izzeddin Gur", "Ofir Nachum", "Yingjie Miao", "Mustafa Safdari", "Austin Huang", "Aakanksha Chowdhery", "Sharan Narang", "Noah Fiedel", "Aleksandra Faust"], "externalIds": {"DBLP": "journals/corr/abs-2210-03945", "ArXiv": "2210.03945", "DOI": "10.48550/arXiv.2210.03945", "CorpusId": 252780086}}, "hash": "6fec11ce3bd90f1d4ff4f59291cd6d9af9ffb0b066eff9a3ae22321c9f8c3390"}, "2": {"node_id": "0aec158f-2af8-4b02-8188-33bb287bba3e", "node_type": null, "metadata": {"title": "Understanding HTML with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "6dd4743ee5430157c981a8dfe9a7434d99be2e8b", "citationCount": 17, "openAccessPdf": null, "authors": ["Izzeddin Gur", "Ofir Nachum", "Yingjie Miao", "Mustafa Safdari", "Austin Huang", "Aakanksha Chowdhery", "Sharan Narang", "Noah Fiedel", "Aleksandra Faust"], "externalIds": {"DBLP": "journals/corr/abs-2210-03945", "ArXiv": "2210.03945", "DOI": "10.48550/arXiv.2210.03945", "CorpusId": 252780086}}, "hash": "c93249fb0fffb7a6ed51e35d801dee8d72322be9a1741b2f85a66ba93f2f53c2"}}, "hash": "5162f2fe3f58a0fbc892b2aac969b638850ed968bb44f7fb41b625694cf378a7", "text": "models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.", "start_char_idx": 1231, "end_char_idx": 1462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "de84f76a-2bfb-4f8c-a018-25270138caaa": {"__data__": {"id_": "de84f76a-2bfb-4f8c-a018-25270138caaa", "embedding": null, "metadata": {"title": "Using Large Language Models to Enhance Programming Error Messages", "venue": "Technical Symposium on Computer Science Education", "year": 2022, "paperId": "668faca09fcefd18a46ab5ce4eab765c065e1d5e", "citationCount": 36, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3545945.3569770", "authors": ["Juho Leinonen", "Arto Hellas", "Sami Sarsa", "B. Reeves", "Paul Denny", "J. Prather", "Brett A. Becker"], "externalIds": {"ArXiv": "2210.11630", "DBLP": "journals/corr/abs-2210-11630", "DOI": "10.1145/3545945.3569770", "CorpusId": 253080791}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15740998-e241-46ad-a84e-948b02bf809b", "node_type": null, "metadata": {"title": "Using Large Language Models to Enhance Programming Error Messages", "venue": "Technical Symposium on Computer Science Education", "year": 2022, "paperId": "668faca09fcefd18a46ab5ce4eab765c065e1d5e", "citationCount": 36, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3545945.3569770", "authors": ["Juho Leinonen", "Arto Hellas", "Sami Sarsa", "B. Reeves", "Paul Denny", "J. Prather", "Brett A. Becker"], "externalIds": {"ArXiv": "2210.11630", "DBLP": "journals/corr/abs-2210-11630", "DOI": "10.1145/3545945.3569770", "CorpusId": 253080791}}, "hash": "193c1d933e02a8c329b05bbe261cf2715022bb7647246c241b23db1e6670b71f"}}, "hash": "193c1d933e02a8c329b05bbe261cf2715022bb7647246c241b23db1e6670b71f", "text": "Using Large Language Models to Enhance Programming Error Messages A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix them. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.", "start_char_idx": 0, "end_char_idx": 1322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "094946a2-d061-41ee-ab11-a29b2e6c2f68": {"__data__": {"id_": "094946a2-d061-41ee-ab11-a29b2e6c2f68", "embedding": null, "metadata": {"title": "Capturing Failures of Large Language Models via Human Cognitive Biases", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "76f023c3a819fc58989a064a1b50825b11fce95d", "citationCount": 35, "openAccessPdf": null, "authors": ["Erik Jones", "J. Steinhardt"], "externalIds": {"ArXiv": "2202.12299", "DBLP": "journals/corr/abs-2202-12299", "CorpusId": 247084098}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1162a76f-1d11-4771-8655-e8c108f4cf1c", "node_type": null, "metadata": {"title": "Capturing Failures of Large Language Models via Human Cognitive Biases", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "76f023c3a819fc58989a064a1b50825b11fce95d", "citationCount": 35, "openAccessPdf": null, "authors": ["Erik Jones", "J. Steinhardt"], "externalIds": {"ArXiv": "2202.12299", "DBLP": "journals/corr/abs-2202-12299", "CorpusId": 247084098}}, "hash": "4b633899e2590e17058123c1260020c92ce29c23dd10e66014301e73ec5559da"}}, "hash": "4b633899e2590e17058123c1260020c92ce29c23dd10e66014301e73ec5559da", "text": "Capturing Failures of Large Language Models via Human Cognitive Biases Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code. In order to asses the reliability of these open-ended generation systems, we aim to identify qualitative categories of erroneous behavior, beyond identifying individual errors. To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate hypotheses for problems that models may have, and (ii) develop experiments that elicit these problems. Using code generation as a case study, we find that OpenAI's Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly deleting files. Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave.", "start_char_idx": 0, "end_char_idx": 1193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8780dc0c-b52d-483f-bb05-a37c44ab8431": {"__data__": {"id_": "8780dc0c-b52d-483f-bb05-a37c44ab8431", "embedding": null, "metadata": {"title": "A Survey of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "1d29334cfbe9a1a943082058876f0c22d44c62fd", "citationCount": 443, "openAccessPdf": null, "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "externalIds": {"DBLP": "journals/corr/abs-2303-18223", "ArXiv": "2303.18223", "DOI": "10.48550/arXiv.2303.18223", "CorpusId": 257900969}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d698339-0250-40db-b79d-28caddeab50d", "node_type": null, "metadata": {"title": "A Survey of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "1d29334cfbe9a1a943082058876f0c22d44c62fd", "citationCount": 443, "openAccessPdf": null, "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "externalIds": {"DBLP": "journals/corr/abs-2303-18223", "ArXiv": "2303.18223", "DOI": "10.48550/arXiv.2303.18223", "CorpusId": 257900969}}, "hash": "0166d2c30f0f6abf3fbd399b3db934552cd78b2d0fab15d9db5b0f0b8d3e27c5"}, "3": {"node_id": "4e26d892-407e-4cad-b511-79efd956a7f5", "node_type": null, "metadata": {"title": "A Survey of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "1d29334cfbe9a1a943082058876f0c22d44c62fd", "citationCount": 443, "openAccessPdf": null, "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "externalIds": {"DBLP": "journals/corr/abs-2303-18223", "ArXiv": "2303.18223", "DOI": "10.48550/arXiv.2303.18223", "CorpusId": 257900969}}, "hash": "cdfa1832819688714bb22b7460c5044ac4d6fd81c9cd5c84a8d7d196865be247"}}, "hash": "9ab4f5fa13de43aa01e8e98cf97881be4740d2da39e7a77b3e92c8f98529626f", "text": "A Survey of Large Language Models Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language", "start_char_idx": 0, "end_char_idx": 1154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4e26d892-407e-4cad-b511-79efd956a7f5": {"__data__": {"id_": "4e26d892-407e-4cad-b511-79efd956a7f5", "embedding": null, "metadata": {"title": "A Survey of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "1d29334cfbe9a1a943082058876f0c22d44c62fd", "citationCount": 443, "openAccessPdf": null, "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "externalIds": {"DBLP": "journals/corr/abs-2303-18223", "ArXiv": "2303.18223", "DOI": "10.48550/arXiv.2303.18223", "CorpusId": 257900969}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d698339-0250-40db-b79d-28caddeab50d", "node_type": null, "metadata": {"title": "A Survey of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "1d29334cfbe9a1a943082058876f0c22d44c62fd", "citationCount": 443, "openAccessPdf": null, "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "externalIds": {"DBLP": "journals/corr/abs-2303-18223", "ArXiv": "2303.18223", "DOI": "10.48550/arXiv.2303.18223", "CorpusId": 257900969}}, "hash": "0166d2c30f0f6abf3fbd399b3db934552cd78b2d0fab15d9db5b0f0b8d3e27c5"}, "2": {"node_id": "8780dc0c-b52d-483f-bb05-a37c44ab8431", "node_type": null, "metadata": {"title": "A Survey of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "1d29334cfbe9a1a943082058876f0c22d44c62fd", "citationCount": 443, "openAccessPdf": null, "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "externalIds": {"DBLP": "journals/corr/abs-2303-18223", "ArXiv": "2303.18223", "DOI": "10.48550/arXiv.2303.18223", "CorpusId": 257900969}}, "hash": "9ab4f5fa13de43aa01e8e98cf97881be4740d2da39e7a77b3e92c8f98529626f"}}, "hash": "cdfa1832819688714bb22b7460c5044ac4d6fd81c9cd5c84a8d7d196865be247", "text": "the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "start_char_idx": 1062, "end_char_idx": 1950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "901c1fca-489b-4f9a-b0aa-c8ac24b67a31": {"__data__": {"id_": "901c1fca-489b-4f9a-b0aa-c8ac24b67a31", "embedding": null, "metadata": {"title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "05d77715d49714506a920f26c5432b92078cd37c", "citationCount": 29, "openAccessPdf": null, "authors": ["Bernd Bohnet", "Vinh Q. Tran", "Pat Verga", "Roee Aharoni", "D. Andor", "Livio Baldini Soares", "Jacob Eisenstein", "Kuzman Ganchev", "Jonathan Herzig", "Kai Hui", "T. Kwiatkowski", "Ji Ma", "Jianmo Ni", "Tal Schuster", "William W. Cohen", "Michael Collins", "Dipanjan Das", "Donald Metzler", "Slav Petrov", "Kellie Webster"], "externalIds": {"DBLP": "journals/corr/abs-2212-08037", "ArXiv": "2212.08037", "DOI": "10.48550/arXiv.2212.08037", "CorpusId": 254685584}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a595946b-5120-4796-a7db-4ec3b76aece2", "node_type": null, "metadata": {"title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "05d77715d49714506a920f26c5432b92078cd37c", "citationCount": 29, "openAccessPdf": null, "authors": ["Bernd Bohnet", "Vinh Q. Tran", "Pat Verga", "Roee Aharoni", "D. Andor", "Livio Baldini Soares", "Jacob Eisenstein", "Kuzman Ganchev", "Jonathan Herzig", "Kai Hui", "T. Kwiatkowski", "Ji Ma", "Jianmo Ni", "Tal Schuster", "William W. Cohen", "Michael Collins", "Dipanjan Das", "Donald Metzler", "Slav Petrov", "Kellie Webster"], "externalIds": {"DBLP": "journals/corr/abs-2212-08037", "ArXiv": "2212.08037", "DOI": "10.48550/arXiv.2212.08037", "CorpusId": 254685584}}, "hash": "310cb9c9700bad039fc899761cf8b523328ac7a981f5a96e4daa2b61756fdb8c"}}, "hash": "310cb9c9700bad039fc899761cf8b523328ac7a981f5a96e4daa2b61756fdb8c", "text": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Large language models (LLMs) have shown impressive results while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial in this setting. We formulate and study Attributed QA as a key first step in the development of attributed LLMs. We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures. We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development. Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).", "start_char_idx": 0, "end_char_idx": 989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e9a94b93-dc0d-4a09-9e18-fe7888463686": {"__data__": {"id_": "e9a94b93-dc0d-4a09-9e18-fe7888463686", "embedding": null, "metadata": {"title": "Automated Repair of Programs from Large Language Models", "venue": "International Conference on Software Engineering", "year": 2022, "paperId": "0bcd59da541fdae66884afba8d25475a54a9da1a", "citationCount": 28, "openAccessPdf": "https://arxiv.org/pdf/2205.10583", "authors": ["Zhiyu Fan", "Xiang Gao", "M. Mirchev", "Abhik Roychoudhury", "Shin Hwei Tan"], "externalIds": {"ArXiv": "2205.10583", "DBLP": "conf/icse/FanGMRT23", "DOI": "10.1109/ICSE48619.2023.00128", "CorpusId": 255372224}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59318c71-e193-4755-b4c9-a266d0c7c5a8", "node_type": null, "metadata": {"title": "Automated Repair of Programs from Large Language Models", "venue": "International Conference on Software Engineering", "year": 2022, "paperId": "0bcd59da541fdae66884afba8d25475a54a9da1a", "citationCount": 28, "openAccessPdf": "https://arxiv.org/pdf/2205.10583", "authors": ["Zhiyu Fan", "Xiang Gao", "M. Mirchev", "Abhik Roychoudhury", "Shin Hwei Tan"], "externalIds": {"ArXiv": "2205.10583", "DBLP": "conf/icse/FanGMRT23", "DOI": "10.1109/ICSE48619.2023.00128", "CorpusId": 255372224}}, "hash": "475d0c0a3172b9d6415e1f0667acedffde96f22c19d3333a920343bb23e58559"}, "3": {"node_id": "520d449b-f0a4-409c-9a1b-9153f4148ab6", "node_type": null, "metadata": {"title": "Automated Repair of Programs from Large Language Models", "venue": "International Conference on Software Engineering", "year": 2022, "paperId": "0bcd59da541fdae66884afba8d25475a54a9da1a", "citationCount": 28, "openAccessPdf": "https://arxiv.org/pdf/2205.10583", "authors": ["Zhiyu Fan", "Xiang Gao", "M. Mirchev", "Abhik Roychoudhury", "Shin Hwei Tan"], "externalIds": {"ArXiv": "2205.10583", "DBLP": "conf/icse/FanGMRT23", "DOI": "10.1109/ICSE48619.2023.00128", "CorpusId": 255372224}}, "hash": "eecb1bf58f2692a0954b1244c012d744991a18782705a033b75c268ef6572f9c"}}, "hash": "c6edd6117ce604538f5b90cf91a561fd40047dff060893cb2c4b775a4e1b0b23", "text": "Automated Repair of Programs from Large Language Models Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on", "start_char_idx": 0, "end_char_idx": 1448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "520d449b-f0a4-409c-9a1b-9153f4148ab6": {"__data__": {"id_": "520d449b-f0a4-409c-9a1b-9153f4148ab6", "embedding": null, "metadata": {"title": "Automated Repair of Programs from Large Language Models", "venue": "International Conference on Software Engineering", "year": 2022, "paperId": "0bcd59da541fdae66884afba8d25475a54a9da1a", "citationCount": 28, "openAccessPdf": "https://arxiv.org/pdf/2205.10583", "authors": ["Zhiyu Fan", "Xiang Gao", "M. Mirchev", "Abhik Roychoudhury", "Shin Hwei Tan"], "externalIds": {"ArXiv": "2205.10583", "DBLP": "conf/icse/FanGMRT23", "DOI": "10.1109/ICSE48619.2023.00128", "CorpusId": 255372224}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59318c71-e193-4755-b4c9-a266d0c7c5a8", "node_type": null, "metadata": {"title": "Automated Repair of Programs from Large Language Models", "venue": "International Conference on Software Engineering", "year": 2022, "paperId": "0bcd59da541fdae66884afba8d25475a54a9da1a", "citationCount": 28, "openAccessPdf": "https://arxiv.org/pdf/2205.10583", "authors": ["Zhiyu Fan", "Xiang Gao", "M. Mirchev", "Abhik Roychoudhury", "Shin Hwei Tan"], "externalIds": {"ArXiv": "2205.10583", "DBLP": "conf/icse/FanGMRT23", "DOI": "10.1109/ICSE48619.2023.00128", "CorpusId": 255372224}}, "hash": "475d0c0a3172b9d6415e1f0667acedffde96f22c19d3333a920343bb23e58559"}, "2": {"node_id": "e9a94b93-dc0d-4a09-9e18-fe7888463686", "node_type": null, "metadata": {"title": "Automated Repair of Programs from Large Language Models", "venue": "International Conference on Software Engineering", "year": 2022, "paperId": "0bcd59da541fdae66884afba8d25475a54a9da1a", "citationCount": 28, "openAccessPdf": "https://arxiv.org/pdf/2205.10583", "authors": ["Zhiyu Fan", "Xiang Gao", "M. Mirchev", "Abhik Roychoudhury", "Shin Hwei Tan"], "externalIds": {"ArXiv": "2205.10583", "DBLP": "conf/icse/FanGMRT23", "DOI": "10.1109/ICSE48619.2023.00128", "CorpusId": 255372224}}, "hash": "c6edd6117ce604538f5b90cf91a561fd40047dff060893cb2c4b775a4e1b0b23"}}, "hash": "eecb1bf58f2692a0954b1244c012d744991a18782705a033b75c268ef6572f9c", "text": "desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.", "start_char_idx": 1365, "end_char_idx": 1657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d4380c8-a715-4864-ab4c-d5527b17ed6e": {"__data__": {"id_": "2d4380c8-a715-4864-ab4c-d5527b17ed6e", "embedding": null, "metadata": {"title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models", "venue": "International Symposium on Software Testing and Analysis", "year": 2022, "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67", "citationCount": 27, "openAccessPdf": "https://arxiv.org/pdf/2212.14834", "authors": ["Yinlin Deng", "Chun Xia", "Haoran Peng", "Chenyuan Yang", "Lingming Zhang"], "externalIds": {"DBLP": "conf/issta/DengXPY023", "ArXiv": "2212.14834", "DOI": "10.1145/3597926.3598067", "CorpusId": 255340904}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ddb59b9-8a4c-4a1a-8ffb-9834a8a136d4", "node_type": null, "metadata": {"title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models", "venue": "International Symposium on Software Testing and Analysis", "year": 2022, "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67", "citationCount": 27, "openAccessPdf": "https://arxiv.org/pdf/2212.14834", "authors": ["Yinlin Deng", "Chun Xia", "Haoran Peng", "Chenyuan Yang", "Lingming Zhang"], "externalIds": {"DBLP": "conf/issta/DengXPY023", "ArXiv": "2212.14834", "DOI": "10.1145/3597926.3598067", "CorpusId": 255340904}}, "hash": "47f086a4470682fc64f93351f5ceaa188fefa778e71e4248a45fd2416e19dc53"}, "3": {"node_id": "0555ff75-23f0-4fa0-b4cc-d44c5b7d14b9", "node_type": null, "metadata": {"title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models", "venue": "International Symposium on Software Testing and Analysis", "year": 2022, "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67", "citationCount": 27, "openAccessPdf": "https://arxiv.org/pdf/2212.14834", "authors": ["Yinlin Deng", "Chun Xia", "Haoran Peng", "Chenyuan Yang", "Lingming Zhang"], "externalIds": {"DBLP": "conf/issta/DengXPY023", "ArXiv": "2212.14834", "DOI": "10.1145/3597926.3598067", "CorpusId": 255340904}}, "hash": "57b5252df349e515b993d5f9f2bc4d52be045cc1cdc8d93f25d16db17d630f7b"}}, "hash": "0c91dff23226ac8da6f2528e734e9ae11ebd20f17a22f6f7a2a66fda0a105be6", "text": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations. To address these limitations, we propose TitanFuzz \u2013 the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More", "start_char_idx": 0, "end_char_idx": 1315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0555ff75-23f0-4fa0-b4cc-d44c5b7d14b9": {"__data__": {"id_": "0555ff75-23f0-4fa0-b4cc-d44c5b7d14b9", "embedding": null, "metadata": {"title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models", "venue": "International Symposium on Software Testing and Analysis", "year": 2022, "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67", "citationCount": 27, "openAccessPdf": "https://arxiv.org/pdf/2212.14834", "authors": ["Yinlin Deng", "Chun Xia", "Haoran Peng", "Chenyuan Yang", "Lingming Zhang"], "externalIds": {"DBLP": "conf/issta/DengXPY023", "ArXiv": "2212.14834", "DOI": "10.1145/3597926.3598067", "CorpusId": 255340904}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ddb59b9-8a4c-4a1a-8ffb-9834a8a136d4", "node_type": null, "metadata": {"title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models", "venue": "International Symposium on Software Testing and Analysis", "year": 2022, "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67", "citationCount": 27, "openAccessPdf": "https://arxiv.org/pdf/2212.14834", "authors": ["Yinlin Deng", "Chun Xia", "Haoran Peng", "Chenyuan Yang", "Lingming Zhang"], "externalIds": {"DBLP": "conf/issta/DengXPY023", "ArXiv": "2212.14834", "DOI": "10.1145/3597926.3598067", "CorpusId": 255340904}}, "hash": "47f086a4470682fc64f93351f5ceaa188fefa778e71e4248a45fd2416e19dc53"}, "2": {"node_id": "2d4380c8-a715-4864-ab4c-d5527b17ed6e", "node_type": null, "metadata": {"title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models", "venue": "International Symposium on Software Testing and Analysis", "year": 2022, "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67", "citationCount": 27, "openAccessPdf": "https://arxiv.org/pdf/2212.14834", "authors": ["Yinlin Deng", "Chun Xia", "Haoran Peng", "Chenyuan Yang", "Lingming Zhang"], "externalIds": {"DBLP": "conf/issta/DengXPY023", "ArXiv": "2212.14834", "DOI": "10.1145/3597926.3598067", "CorpusId": 255340904}}, "hash": "0c91dff23226ac8da6f2528e734e9ae11ebd20f17a22f6f7a2a66fda0a105be6"}}, "hash": "57b5252df349e515b993d5f9f2bc4d52be045cc1cdc8d93f25d16db17d630f7b", "text": "and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs. This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.", "start_char_idx": 1245, "end_char_idx": 2095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "108dee8e-0588-4e37-8379-1b1ac801fdfa": {"__data__": {"id_": "108dee8e-0588-4e37-8379-1b1ac801fdfa", "embedding": null, "metadata": {"title": "Large Language Models and the Reverse Turing Test", "venue": "Neural Computation", "year": 2022, "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d", "citationCount": 28, "openAccessPdf": "https://direct.mit.edu/neco/article-pdf/35/3/309/2071839/neco_a_01563.pdf", "authors": ["T. Sejnowski"], "externalIds": {"ArXiv": "2207.14382", "DBLP": "journals/neco/Sejnowski23", "DOI": "10.1162/neco_a_01563", "CorpusId": 251196636, "PubMed": "36746144"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0a4f260-a296-4bbd-bd6d-4e4df8b3bee0", "node_type": null, "metadata": {"title": "Large Language Models and the Reverse Turing Test", "venue": "Neural Computation", "year": 2022, "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d", "citationCount": 28, "openAccessPdf": "https://direct.mit.edu/neco/article-pdf/35/3/309/2071839/neco_a_01563.pdf", "authors": ["T. Sejnowski"], "externalIds": {"ArXiv": "2207.14382", "DBLP": "journals/neco/Sejnowski23", "DOI": "10.1162/neco_a_01563", "CorpusId": 251196636, "PubMed": "36746144"}}, "hash": "ca89d7618136fbd8c358f00a37e68ffdc901cf06d5831076b7d9b5f101d4fe8d"}, "3": {"node_id": "9083decc-bb9a-4c29-bba7-0d11ead008f6", "node_type": null, "metadata": {"title": "Large Language Models and the Reverse Turing Test", "venue": "Neural Computation", "year": 2022, "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d", "citationCount": 28, "openAccessPdf": "https://direct.mit.edu/neco/article-pdf/35/3/309/2071839/neco_a_01563.pdf", "authors": ["T. Sejnowski"], "externalIds": {"ArXiv": "2207.14382", "DBLP": "journals/neco/Sejnowski23", "DOI": "10.1162/neco_a_01563", "CorpusId": 251196636, "PubMed": "36746144"}}, "hash": "5008d9ef46588873c34b147d67c40d0914d112fcd98bda22994f2a3698f5cb45"}}, "hash": "ba7361dfb63d5603de713023bfdffd0a22bb90ebd87e56a755eeaec42887a589", "text": "Large Language Models and the Reverse Turing Test Abstract Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can", "start_char_idx": 0, "end_char_idx": 1390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9083decc-bb9a-4c29-bba7-0d11ead008f6": {"__data__": {"id_": "9083decc-bb9a-4c29-bba7-0d11ead008f6", "embedding": null, "metadata": {"title": "Large Language Models and the Reverse Turing Test", "venue": "Neural Computation", "year": 2022, "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d", "citationCount": 28, "openAccessPdf": "https://direct.mit.edu/neco/article-pdf/35/3/309/2071839/neco_a_01563.pdf", "authors": ["T. Sejnowski"], "externalIds": {"ArXiv": "2207.14382", "DBLP": "journals/neco/Sejnowski23", "DOI": "10.1162/neco_a_01563", "CorpusId": 251196636, "PubMed": "36746144"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0a4f260-a296-4bbd-bd6d-4e4df8b3bee0", "node_type": null, "metadata": {"title": "Large Language Models and the Reverse Turing Test", "venue": "Neural Computation", "year": 2022, "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d", "citationCount": 28, "openAccessPdf": "https://direct.mit.edu/neco/article-pdf/35/3/309/2071839/neco_a_01563.pdf", "authors": ["T. Sejnowski"], "externalIds": {"ArXiv": "2207.14382", "DBLP": "journals/neco/Sejnowski23", "DOI": "10.1162/neco_a_01563", "CorpusId": 251196636, "PubMed": "36746144"}}, "hash": "ca89d7618136fbd8c358f00a37e68ffdc901cf06d5831076b7d9b5f101d4fe8d"}, "2": {"node_id": "108dee8e-0588-4e37-8379-1b1ac801fdfa", "node_type": null, "metadata": {"title": "Large Language Models and the Reverse Turing Test", "venue": "Neural Computation", "year": 2022, "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d", "citationCount": 28, "openAccessPdf": "https://direct.mit.edu/neco/article-pdf/35/3/309/2071839/neco_a_01563.pdf", "authors": ["T. Sejnowski"], "externalIds": {"ArXiv": "2207.14382", "DBLP": "journals/neco/Sejnowski23", "DOI": "10.1162/neco_a_01563", "CorpusId": 251196636, "PubMed": "36746144"}}, "hash": "ba7361dfb63d5603de713023bfdffd0a22bb90ebd87e56a755eeaec42887a589"}}, "hash": "5008d9ef46588873c34b147d67c40d0914d112fcd98bda22994f2a3698f5cb45", "text": "LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.", "start_char_idx": 1332, "end_char_idx": 1634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d10250a-8c80-4a6f-bf9a-0a0dc4cfc7c3": {"__data__": {"id_": "0d10250a-8c80-4a6f-bf9a-0a0dc4cfc7c3", "embedding": null, "metadata": {"title": "Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models", "venue": "Minds and Machines", "year": 2022, "paperId": "d072b46a0504ac023d5035d8ec0c7876151245c4", "citationCount": 28, "openAccessPdf": "https://link.springer.com/content/pdf/10.1007/s11023-022-09602-0.pdf", "authors": ["Adam Sobieszek", "Tadeusz Price"], "externalIds": {"DBLP": "journals/mima/SobieszekP22", "DOI": "10.1007/s11023-022-09602-0", "CorpusId": 248634256}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7991df4-901e-4302-bb9d-11fdadb86650", "node_type": null, "metadata": {"title": "Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models", "venue": "Minds and Machines", "year": 2022, "paperId": "d072b46a0504ac023d5035d8ec0c7876151245c4", "citationCount": 28, "openAccessPdf": "https://link.springer.com/content/pdf/10.1007/s11023-022-09602-0.pdf", "authors": ["Adam Sobieszek", "Tadeusz Price"], "externalIds": {"DBLP": "journals/mima/SobieszekP22", "DOI": "10.1007/s11023-022-09602-0", "CorpusId": 248634256}}, "hash": "aab52cbf8104a02530393f4dda8e2ac44de463ad126549e818e58cb9398fd6b3"}}, "hash": "aab52cbf8104a02530393f4dda8e2ac44de463ad126549e818e58cb9398fd6b3", "text": "Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d86640df-d07c-423c-ae35-8ebf441d4cd2": {"__data__": {"id_": "d86640df-d07c-423c-ae35-8ebf441d4cd2", "embedding": null, "metadata": {"title": "Do Large Language Models Understand Us?", "venue": "Daedalus", "year": 2022, "paperId": "39a45eba627199ee12c168dffcead45e138e9a01", "citationCount": 25, "openAccessPdf": "https://direct.mit.edu/daed/article-pdf/151/2/183/2060575/daed_a_01909.pdf", "authors": ["Blaise Ag\u00fcera y Arcas"], "externalIds": {"DOI": "10.1162/daed_a_01909", "CorpusId": 248377874}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e74b3d2e-2beb-40fe-9ef0-3fc75e9daaa0", "node_type": null, "metadata": {"title": "Do Large Language Models Understand Us?", "venue": "Daedalus", "year": 2022, "paperId": "39a45eba627199ee12c168dffcead45e138e9a01", "citationCount": 25, "openAccessPdf": "https://direct.mit.edu/daed/article-pdf/151/2/183/2060575/daed_a_01909.pdf", "authors": ["Blaise Ag\u00fcera y Arcas"], "externalIds": {"DOI": "10.1162/daed_a_01909", "CorpusId": 248377874}}, "hash": "3a5633e06cddf25e3a7dd8222c15d02ee3b1cf22fec427ce4eb66ef5f147b5e2"}}, "hash": "3a5633e06cddf25e3a7dd8222c15d02ee3b1cf22fec427ce4eb66ef5f147b5e2", "text": "Do Large Language Models Understand Us? Abstract Large language models (LLMs) represent a major advance in artificial intelligence and, in particular, toward the goal of human-like artificial general intelligence. It is sometimes claimed, though, that machine learning is \u201cjust statistics,\u201d hence that, in this grander ambition, progress in AI is illusory. Here I take the contrary view that LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. Specifically: statistics do amount to understanding, in any falsifiable sense. Furthermore, much of what we consider intelligence is inherently dialogic, hence social; it requires a theory of mind. Complex sequence learning and social interaction may be a sufficient basis for general intelligence, including theory of mind and consciousness. Since the interior state of another being can only be understood through interaction, no objective answer is possible to the question of when an \u201cit\u201d becomes a \u201cwho,\u201d but for many people, neural nets running on computers are likely to cross this threshold in the very near future.", "start_char_idx": 0, "end_char_idx": 1136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "be66680d-0829-4aee-948f-cd0c54c959d5": {"__data__": {"id_": "be66680d-0829-4aee-948f-cd0c54c959d5", "embedding": null, "metadata": {"title": "Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey", "venue": "arXiv.org", "year": 2022, "paperId": "475c3014a68d545f1d2319f94fd3ab99fc3f6bec", "citationCount": 18, "openAccessPdf": null, "authors": ["Mengnan Du", "Fengxiang He", "Na Zou", "Dacheng Tao", "Xia Hu"], "externalIds": {"DBLP": "journals/corr/abs-2208-11857", "ArXiv": "2208.11857", "DOI": "10.48550/arXiv.2208.11857", "CorpusId": 251800110}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb6a87f4-e6f2-45e0-82d2-2066ec7c2771", "node_type": null, "metadata": {"title": "Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey", "venue": "arXiv.org", "year": 2022, "paperId": "475c3014a68d545f1d2319f94fd3ab99fc3f6bec", "citationCount": 18, "openAccessPdf": null, "authors": ["Mengnan Du", "Fengxiang He", "Na Zou", "Dacheng Tao", "Xia Hu"], "externalIds": {"DBLP": "journals/corr/abs-2208-11857", "ArXiv": "2208.11857", "DOI": "10.48550/arXiv.2208.11857", "CorpusId": 251800110}}, "hash": "ba11b8822dd9894d21761443a74f85b59a147fe8445eb0f5ac25702247a72bbe"}}, "hash": "ba11b8822dd9894d21761443a74f85b59a147fe8445eb0f5ac25702247a72bbe", "text": "Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey Large language models (LLMs) have achieved state-of-the-art performance on a series of natural language understanding tasks. However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness. In this paper, we provide a review of recent developments that address the shortcut learning and robustness challenge of LLMs. We first introduce the concepts of shortcut learning of language models. We then introduce methods to identify shortcut learning behavior in language models, characterize the reasons for shortcut learning, as well as introduce mitigation solutions. Finally, we discuss key research challenges and potential research directions in order to advance the field of LLMs.", "start_char_idx": 0, "end_char_idx": 877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "80cd099c-ed94-4564-ae7d-6a2987abf609": {"__data__": {"id_": "80cd099c-ed94-4564-ae7d-6a2987abf609", "embedding": null, "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "citationCount": 17, "openAccessPdf": "https://aclanthology.org/2023.acl-long.411.pdf", "authors": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "externalIds": {"ACL": "2023.acl-long.411", "ArXiv": "2212.09420", "DBLP": "conf/acl/ZanCZLWGWL23", "DOI": "10.18653/v1/2023.acl-long.411", "CorpusId": 258557362}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abbb03a1-5cbe-423e-8b58-9374443c1584", "node_type": null, "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "citationCount": 17, "openAccessPdf": "https://aclanthology.org/2023.acl-long.411.pdf", "authors": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "externalIds": {"ACL": "2023.acl-long.411", "ArXiv": "2212.09420", "DBLP": "conf/acl/ZanCZLWGWL23", "DOI": "10.18653/v1/2023.acl-long.411", "CorpusId": 258557362}}, "hash": "d85d28543f254e832ea8508a1921253e8d25d109d376a149e038a3f7db2a52de"}, "3": {"node_id": "8e5ddb88-7d7f-455a-a1ef-81ff31069171", "node_type": null, "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "citationCount": 17, "openAccessPdf": "https://aclanthology.org/2023.acl-long.411.pdf", "authors": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "externalIds": {"ACL": "2023.acl-long.411", "ArXiv": "2212.09420", "DBLP": "conf/acl/ZanCZLWGWL23", "DOI": "10.18653/v1/2023.acl-long.411", "CorpusId": 258557362}}, "hash": "37eb898dd5332c4a717f58f2eb12c9a4241b1bf2ea358cbdd6d3bc477d215cdc"}}, "hash": "4838b932eb7a3b9085be5d0ed73492e85007709ee2b51a50e0c9b325326943a0", "text": "Large Language Models Meet NL2Code: A Survey The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \u201cLarge Size, Premium Data, Expert Tuning\u201d. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development", "start_char_idx": 0, "end_char_idx": 1205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e5ddb88-7d7f-455a-a1ef-81ff31069171": {"__data__": {"id_": "8e5ddb88-7d7f-455a-a1ef-81ff31069171", "embedding": null, "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "citationCount": 17, "openAccessPdf": "https://aclanthology.org/2023.acl-long.411.pdf", "authors": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "externalIds": {"ACL": "2023.acl-long.411", "ArXiv": "2212.09420", "DBLP": "conf/acl/ZanCZLWGWL23", "DOI": "10.18653/v1/2023.acl-long.411", "CorpusId": 258557362}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abbb03a1-5cbe-423e-8b58-9374443c1584", "node_type": null, "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "citationCount": 17, "openAccessPdf": "https://aclanthology.org/2023.acl-long.411.pdf", "authors": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "externalIds": {"ACL": "2023.acl-long.411", "ArXiv": "2212.09420", "DBLP": "conf/acl/ZanCZLWGWL23", "DOI": "10.18653/v1/2023.acl-long.411", "CorpusId": 258557362}}, "hash": "d85d28543f254e832ea8508a1921253e8d25d109d376a149e038a3f7db2a52de"}, "2": {"node_id": "80cd099c-ed94-4564-ae7d-6a2987abf609", "node_type": null, "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "citationCount": 17, "openAccessPdf": "https://aclanthology.org/2023.acl-long.411.pdf", "authors": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "externalIds": {"ACL": "2023.acl-long.411", "ArXiv": "2212.09420", "DBLP": "conf/acl/ZanCZLWGWL23", "DOI": "10.18653/v1/2023.acl-long.411", "CorpusId": 258557362}}, "hash": "4838b932eb7a3b9085be5d0ed73492e85007709ee2b51a50e0c9b325326943a0"}}, "hash": "37eb898dd5332c4a717f58f2eb12c9a4241b1bf2ea358cbdd6d3bc477d215cdc", "text": "language models for NL2Code, and we believe it will contribute to the ongoing development of the field.", "start_char_idx": 1116, "end_char_idx": 1219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ac4c69b5-04e8-45de-a61b-f07ea0b89e1c": {"__data__": {"id_": "ac4c69b5-04e8-45de-a61b-f07ea0b89e1c", "embedding": null, "metadata": {"title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm", "venue": "CHI Extended Abstracts", "year": 2021, "paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "citationCount": 305, "openAccessPdf": "https://arxiv.org/pdf/2102.07350", "authors": ["Laria Reynolds", "Kyle McDonell"], "externalIds": {"DBLP": "journals/corr/abs-2102-07350", "ArXiv": "2102.07350", "DOI": "10.1145/3411763.3451760", "CorpusId": 231925131}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "549c635a-fff8-47fa-8d18-b71cb0b716c9", "node_type": null, "metadata": {"title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm", "venue": "CHI Extended Abstracts", "year": 2021, "paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "citationCount": 305, "openAccessPdf": "https://arxiv.org/pdf/2102.07350", "authors": ["Laria Reynolds", "Kyle McDonell"], "externalIds": {"DBLP": "journals/corr/abs-2102-07350", "ArXiv": "2102.07350", "DOI": "10.1145/3411763.3451760", "CorpusId": 231925131}}, "hash": "038b1982bfc132210e949f65df14f81885092b44929bab14feb17753c0aae4c8"}}, "hash": "038b1982bfc132210e949f65df14f81885092b44929bab14feb17753c0aae4c8", "text": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models\u2019 novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.", "start_char_idx": 0, "end_char_idx": 1294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8be8af7a-2e77-4b7d-bdd8-0ba9e9712dba": {"__data__": {"id_": "8be8af7a-2e77-4b7d-bdd8-0ba9e9712dba", "embedding": null, "metadata": {"title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "f577654d9dd29d88c6db9ee39a4fd831573b8770", "citationCount": 108, "openAccessPdf": null, "authors": ["Alex Tamkin", "Miles Brundage", "Jack Clark", "Deep Ganguli"], "externalIds": {"ArXiv": "2102.02503", "DBLP": "journals/corr/abs-2102-02503", "CorpusId": 231802467}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3809e1ba-81f3-4c4b-8903-9447067af8ff", "node_type": null, "metadata": {"title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "f577654d9dd29d88c6db9ee39a4fd831573b8770", "citationCount": 108, "openAccessPdf": null, "authors": ["Alex Tamkin", "Miles Brundage", "Jack Clark", "Deep Ganguli"], "externalIds": {"ArXiv": "2102.02503", "DBLP": "journals/corr/abs-2102-02503", "CorpusId": 231802467}}, "hash": "3ff0f1e46b3be82804cb3c1392a33d816b9c66b58489b84a446743c92c47bb76"}}, "hash": "3ff0f1e46b3be82804cb3c1392a33d816b9c66b58489b84a446743c92c47bb76", "text": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models 1. What are the technical capabilities and limitations of large language models? The discussion touched on several key areas including: the surprising impact of scale on model capabilities, the difficulty in assessing whether large language models truly understand language, the importance of training models on multiple data modalities, and challenges in aligning model objectives with human values.", "start_char_idx": 0, "end_char_idx": 490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "23ea0f97-2a4b-4130-9df7-6a01b247612b": {"__data__": {"id_": "23ea0f97-2a4b-4130-9df7-6a01b247612b", "embedding": null, "metadata": {"title": "A Recipe for Arbitrary Text Style Transfer with Large Language Models", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "paperId": "7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd", "citationCount": 75, "openAccessPdf": "https://aclanthology.org/2022.acl-short.94.pdf", "authors": ["Emily Reif", "Daphne Ippolito", "Ann Yuan", "Andy Coenen", "Chris Callison-Burch", "Jason Wei"], "externalIds": {"ACL": "2022.acl-short.94", "DBLP": "journals/corr/abs-2109-03910", "ArXiv": "2109.03910", "DOI": "10.18653/v1/2022.acl-short.94", "CorpusId": 237263305}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c132b5b-24ec-47c6-a405-10017b6c0274", "node_type": null, "metadata": {"title": "A Recipe for Arbitrary Text Style Transfer with Large Language Models", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "paperId": "7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd", "citationCount": 75, "openAccessPdf": "https://aclanthology.org/2022.acl-short.94.pdf", "authors": ["Emily Reif", "Daphne Ippolito", "Ann Yuan", "Andy Coenen", "Chris Callison-Burch", "Jason Wei"], "externalIds": {"ACL": "2022.acl-short.94", "DBLP": "journals/corr/abs-2109-03910", "ArXiv": "2109.03910", "DOI": "10.18653/v1/2022.acl-short.94", "CorpusId": 237263305}}, "hash": "a17577537cdf41df0c576b75099f449cc689c467e04893b963fc64b9672cde54"}}, "hash": "a17577537cdf41df0c576b75099f449cc689c467e04893b963fc64b9672cde54", "text": "A Recipe for Arbitrary Text Style Transfer with Large Language Models In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as \u2018make this melodramatic\u2019 or \u2018insert a metaphor.\u2019", "start_char_idx": 0, "end_char_idx": 637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2711496c-7b6c-401d-8066-4c0f0f0630ec": {"__data__": {"id_": "2711496c-7b6c-401d-8066-4c0f0f0630ec", "embedding": null, "metadata": {"title": "Jigsaw: Large Language Models meet Program Synthesis", "venue": "International Conference on Software Engineering", "year": 2021, "paperId": "d095f9ffcb5905bf0858ad1769d3d90e2e8737e2", "citationCount": 79, "openAccessPdf": null, "authors": ["Naman Jain", "Skanda Vaidyanath", "Arun Shankar Iyer", "Nagarajan Natarajan", "Suresh Parthasarathy", "S. Rajamani", "Rahul Sharma"], "externalIds": {"DBLP": "conf/icse/JainVINPR022", "ArXiv": "2112.02969", "DOI": "10.1145/3510003.3510203", "CorpusId": 244908632}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c1c9973-9d55-471b-9863-a6cea419fef8", "node_type": null, "metadata": {"title": "Jigsaw: Large Language Models meet Program Synthesis", "venue": "International Conference on Software Engineering", "year": 2021, "paperId": "d095f9ffcb5905bf0858ad1769d3d90e2e8737e2", "citationCount": 79, "openAccessPdf": null, "authors": ["Naman Jain", "Skanda Vaidyanath", "Arun Shankar Iyer", "Nagarajan Natarajan", "Suresh Parthasarathy", "S. Rajamani", "Rahul Sharma"], "externalIds": {"DBLP": "conf/icse/JainVINPR022", "ArXiv": "2112.02969", "DOI": "10.1145/3510003.3510203", "CorpusId": 244908632}}, "hash": "667e1bc5d06a4ecc94360ced63b0f0ec3a9f1b5547474cb94e7550e80acff60c"}}, "hash": "667e1bc5d06a4ecc94360ced63b0f0ec3a9f1b5547474cb94e7550e80acff60c", "text": "Jigsaw: Large Language Models meet Program Synthesis Large pre-trained language models such as GPT-3 [10], Codex [11], and Coogle's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.", "start_char_idx": 0, "end_char_idx": 1285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f4cb3e47-1364-40ed-8dc2-bbe11b000d79": {"__data__": {"id_": "f4cb3e47-1364-40ed-8dc2-bbe11b000d79", "embedding": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "b38bdb362c7cb6b2d1abfb557e29fc0ae5e328bcd50a0980bf62118fe892ef8f"}, "3": {"node_id": "5cd979f0-c9af-43ba-a3c6-c30cb89e6a19", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "7f1aebee73cc8bf25e7764692c7be875235f867e83e37509d0c424565a76a89e"}}, "hash": "6a10e7d2fe0c37128d6a97e896cf392811f4263c3794289b193ff650ca6aee60", "text": "Measuring Progress on Scalable Oversight for Large Language Models Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have", "start_char_idx": 0, "end_char_idx": 367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5cd979f0-c9af-43ba-a3c6-c30cb89e6a19": {"__data__": {"id_": "5cd979f0-c9af-43ba-a3c6-c30cb89e6a19", "embedding": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "b38bdb362c7cb6b2d1abfb557e29fc0ae5e328bcd50a0980bf62118fe892ef8f"}, "2": {"node_id": "f4cb3e47-1364-40ed-8dc2-bbe11b000d79", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "6a10e7d2fe0c37128d6a97e896cf392811f4263c3794289b193ff650ca6aee60"}, "3": {"node_id": "3e6f12d6-3cc1-4de8-afed-1404ff93e418", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "712474781b4e5962142c9d36d27a7bcdb32dc03947487916abdee53b93c6e2d6"}}, "hash": "7f1aebee73cc8bf25e7764692c7be875235f867e83e37509d0c424565a76a89e", "text": "Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI", "start_char_idx": 302, "end_char_idx": 684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3e6f12d6-3cc1-4de8-afed-1404ff93e418": {"__data__": {"id_": "3e6f12d6-3cc1-4de8-afed-1404ff93e418", "embedding": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "b38bdb362c7cb6b2d1abfb557e29fc0ae5e328bcd50a0980bf62118fe892ef8f"}, "2": {"node_id": "5cd979f0-c9af-43ba-a3c6-c30cb89e6a19", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "7f1aebee73cc8bf25e7764692c7be875235f867e83e37509d0c424565a76a89e"}, "3": {"node_id": "0f8026a5-5204-4eb9-9ff4-46e8870ca444", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "8a99c7ebc46d5bb93580f4fc0fed78921cefc85ea944ed12a352e1491427a353"}}, "hash": "712474781b4e5962142c9d36d27a7bcdb32dc03947487916abdee53b93c6e2d6", "text": "centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who", "start_char_idx": 670, "end_char_idx": 1029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f8026a5-5204-4eb9-9ff4-46e8870ca444": {"__data__": {"id_": "0f8026a5-5204-4eb9-9ff4-46e8870ca444", "embedding": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "b38bdb362c7cb6b2d1abfb557e29fc0ae5e328bcd50a0980bf62118fe892ef8f"}, "2": {"node_id": "3e6f12d6-3cc1-4de8-afed-1404ff93e418", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "712474781b4e5962142c9d36d27a7bcdb32dc03947487916abdee53b93c6e2d6"}, "3": {"node_id": "8c1374ab-fec3-4d6c-9f9a-2aa55e724243", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "829517a8b9ab22a06255c3b67f30beb88355d0454aa773dbbc9eb0d42c6e0ea9"}}, "hash": "8a99c7ebc46d5bb93580f4fc0fed78921cefc85ea944ed12a352e1491427a353", "text": "and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will", "start_char_idx": 1045, "end_char_idx": 1406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c1374ab-fec3-4d6c-9f9a-2aa55e724243": {"__data__": {"id_": "8c1374ab-fec3-4d6c-9f9a-2aa55e724243", "embedding": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "b38bdb362c7cb6b2d1abfb557e29fc0ae5e328bcd50a0980bf62118fe892ef8f"}, "2": {"node_id": "0f8026a5-5204-4eb9-9ff4-46e8870ca444", "node_type": null, "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}, "hash": "8a99c7ebc46d5bb93580f4fc0fed78921cefc85ea944ed12a352e1491427a353"}}, "hash": "829517a8b9ab22a06255c3b67f30beb88355d0454aa773dbbc9eb0d42c6e0ea9", "text": "own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.", "start_char_idx": 1381, "end_char_idx": 1622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6adbd397-4b40-4df8-b866-f3c46272092a": {"__data__": {"id_": "6adbd397-4b40-4df8-b866-f3c46272092a", "embedding": null, "metadata": {"title": "Persistent Anti-Muslim Bias in Large Language Models", "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "year": 2021, "paperId": "4c2733d191e347753bb28afa46a1c55c65e085be", "citationCount": 214, "openAccessPdf": "https://arxiv.org/pdf/2101.05783", "authors": ["Abubakar Abid", "Maheen Farooqi", "James Y. Zou"], "externalIds": {"DBLP": "conf/aies/AbidF021", "ArXiv": "2101.05783", "DOI": "10.1145/3461702.3462624", "CorpusId": 231603388}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd11a346-f5e8-4319-b698-af78eb861136", "node_type": null, "metadata": {"title": "Persistent Anti-Muslim Bias in Large Language Models", "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "year": 2021, "paperId": "4c2733d191e347753bb28afa46a1c55c65e085be", "citationCount": 214, "openAccessPdf": "https://arxiv.org/pdf/2101.05783", "authors": ["Abubakar Abid", "Maheen Farooqi", "James Y. Zou"], "externalIds": {"DBLP": "conf/aies/AbidF021", "ArXiv": "2101.05783", "DOI": "10.1145/3461702.3462624", "CorpusId": 231603388}}, "hash": "16d7bf4cf435eaf5d3d5f598f0691a855a36892ae614b6c1620d5179a11b3daa"}}, "hash": "16d7bf4cf435eaf5d3d5f598f0691a855a36892ae614b6c1620d5179a11b3daa", "text": "Persistent Anti-Muslim Bias in Large Language Models It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.", "start_char_idx": 0, "end_char_idx": 1067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bd735b8d-8ddc-470b-bacd-6a6630252f05": {"__data__": {"id_": "bd735b8d-8ddc-470b-bacd-6a6630252f05", "embedding": null, "metadata": {"title": "Large language models associate Muslims with violence", "venue": "Nature Machine Intelligence", "year": 2021, "paperId": "4eda2b9eaef3ae892382acc21593eed6f56f2ea1", "citationCount": 74, "openAccessPdf": null, "authors": ["Abubakar Abid", "Maheen Farooqi", "James Zou"], "externalIds": {"MAG": "3170344956", "DBLP": "journals/natmi/AbidFZ21", "DOI": "10.1038/s42256-021-00359-2", "CorpusId": 236384212}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0130dfa3-a254-4f8f-adfa-d021ec30798c", "node_type": null, "metadata": {"title": "Large language models associate Muslims with violence", "venue": "Nature Machine Intelligence", "year": 2021, "paperId": "4eda2b9eaef3ae892382acc21593eed6f56f2ea1", "citationCount": 74, "openAccessPdf": null, "authors": ["Abubakar Abid", "Maheen Farooqi", "James Zou"], "externalIds": {"MAG": "3170344956", "DBLP": "journals/natmi/AbidFZ21", "DOI": "10.1038/s42256-021-00359-2", "CorpusId": 236384212}}, "hash": "f3c02688653dbc497abd0720eb522a5179ea75ec1354ec0f698ff59b95804b18"}}, "hash": "f3c02688653dbc497abd0720eb522a5179ea75ec1354ec0f698ff59b95804b18", "text": "Large language models associate Muslims with violence", "start_char_idx": 0, "end_char_idx": 53, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "001ccbe4-a7ba-4511-a35f-0aeadae961a5": {"__data__": {"id_": "001ccbe4-a7ba-4511-a35f-0aeadae961a5", "embedding": null, "metadata": {"title": "Structured Pruning of Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2019, "paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "citationCount": 155, "openAccessPdf": "https://arxiv.org/pdf/1910.04732", "authors": ["Ziheng Wang", "Jeremy Wohlwend", "Tao Lei"], "externalIds": {"MAG": "2979691890", "ArXiv": "1910.04732", "ACL": "2020.emnlp-main.496", "DBLP": "journals/corr/abs-1910-04732", "DOI": "10.18653/v1/2020.emnlp-main.496", "CorpusId": 204009154}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dfb1163f-e732-40ed-9253-f227f0dff022", "node_type": null, "metadata": {"title": "Structured Pruning of Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2019, "paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "citationCount": 155, "openAccessPdf": "https://arxiv.org/pdf/1910.04732", "authors": ["Ziheng Wang", "Jeremy Wohlwend", "Tao Lei"], "externalIds": {"MAG": "2979691890", "ArXiv": "1910.04732", "ACL": "2020.emnlp-main.496", "DBLP": "journals/corr/abs-1910-04732", "DOI": "10.18653/v1/2020.emnlp-main.496", "CorpusId": 204009154}}, "hash": "96eb80d55de28f0c17a7cf4db1b66c012fec334731abb53a4ab8649826b4eb80"}}, "hash": "96eb80d55de28f0c17a7cf4db1b66c012fec334731abb53a4ab8649826b4eb80", "text": "Structured Pruning of Large Language Models Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks.", "start_char_idx": 0, "end_char_idx": 1058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c70b554-a085-491d-a67b-14ef84bd4425": {"__data__": {"id_": "1c70b554-a085-491d-a67b-14ef84bd4425", "embedding": null, "metadata": {"title": "Examining Zero-Shot Vulnerability Repair with Large Language Models", "venue": "IEEE Symposium on Security and Privacy", "year": 2021, "paperId": "a5731122200fbb8b37f048010a1e1ca4474aa606", "citationCount": 53, "openAccessPdf": "https://arxiv.org/pdf/2112.02125", "authors": ["H. Pearce", "Benjamin Tan", "Baleegh Ahmad", "R. Karri", "Brendan Dolan-Gavitt"], "externalIds": {"ArXiv": "2112.02125", "DBLP": "conf/sp/PearceTAKD23", "DOI": "10.1109/SP46215.2023.10179324", "CorpusId": 251563966}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6925cf83-6ee8-4cc9-96cb-b69c650af276", "node_type": null, "metadata": {"title": "Examining Zero-Shot Vulnerability Repair with Large Language Models", "venue": "IEEE Symposium on Security and Privacy", "year": 2021, "paperId": "a5731122200fbb8b37f048010a1e1ca4474aa606", "citationCount": 53, "openAccessPdf": "https://arxiv.org/pdf/2112.02125", "authors": ["H. Pearce", "Benjamin Tan", "Baleegh Ahmad", "R. Karri", "Brendan Dolan-Gavitt"], "externalIds": {"ArXiv": "2112.02125", "DBLP": "conf/sp/PearceTAKD23", "DOI": "10.1109/SP46215.2023.10179324", "CorpusId": 251563966}}, "hash": "57a08dff5ecc800add2f3a1cfce6c8ba836dafe3a6f8d7488be8f368f2a0b6da"}}, "hash": "57a08dff5ecc800add2f3a1cfce6c8ba836dafe3a6f8d7488be8f368f2a0b6da", "text": "Examining Zero-Shot Vulnerability Repair with Large Language Models Human developers can produce code with cybersecurity bugs. Can emerging \u2018smart\u2019 code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI\u2019s Codex and AI21\u2019s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information\u2014 both semantically and syntactically\u2014with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model\u2019s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.", "start_char_idx": 0, "end_char_idx": 1167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8ba77af6-6aff-4b64-9206-8895a271a929": {"__data__": {"id_": "8ba77af6-6aff-4b64-9206-8895a271a929", "embedding": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a36974b6-77f6-4c00-b9af-e56ea621bfb3", "node_type": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "hash": "c5826400fea90516f3b1cc06c6c18c38c6400f046fdc92d1d6cb621b28ca6ec0"}, "3": {"node_id": "b37a6fc9-c135-44b1-8c3c-df0bac34ad8b", "node_type": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "hash": "155943d193e51562f35841760a98cf941a06b61768bae1902677e901f68a6fb3"}}, "hash": "89c76b59aefd28837fe3e3c2583962b9f64b272b2656074ce86ae854f708f923", "text": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT\u2019s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT\u2019s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2,", "start_char_idx": 0, "end_char_idx": 1392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b37a6fc9-c135-44b1-8c3c-df0bac34ad8b": {"__data__": {"id_": "b37a6fc9-c135-44b1-8c3c-df0bac34ad8b", "embedding": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a36974b6-77f6-4c00-b9af-e56ea621bfb3", "node_type": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "hash": "c5826400fea90516f3b1cc06c6c18c38c6400f046fdc92d1d6cb621b28ca6ec0"}, "2": {"node_id": "8ba77af6-6aff-4b64-9206-8895a271a929", "node_type": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "hash": "89c76b59aefd28837fe3e3c2583962b9f64b272b2656074ce86ae854f708f923"}, "3": {"node_id": "342e0ad6-4569-49c5-906e-1fe303e5f668", "node_type": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "hash": "56ab0b6ebad03c6fce5b1c4739616fbfd34beac1e2fa80b521b7431a2866798c"}}, "hash": "155943d193e51562f35841760a98cf941a06b61768bae1902677e901f68a6fb3", "text": "Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT\u2019s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data", "start_char_idx": 1352, "end_char_idx": 2433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "342e0ad6-4569-49c5-906e-1fe303e5f668": {"__data__": {"id_": "342e0ad6-4569-49c5-906e-1fe303e5f668", "embedding": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a36974b6-77f6-4c00-b9af-e56ea621bfb3", "node_type": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "hash": "c5826400fea90516f3b1cc06c6c18c38c6400f046fdc92d1d6cb621b28ca6ec0"}, "2": {"node_id": "b37a6fc9-c135-44b1-8c3c-df0bac34ad8b", "node_type": null, "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}, "hash": "155943d193e51562f35841760a98cf941a06b61768bae1902677e901f68a6fb3"}}, "hash": "56ab0b6ebad03c6fce5b1c4739616fbfd34beac1e2fa80b521b7431a2866798c", "text": "at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT\u2019s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.", "start_char_idx": 2414, "end_char_idx": 2854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6ef7a3d8-859e-4536-b0cb-87da8a690803": {"__data__": {"id_": "6ef7a3d8-859e-4536-b0cb-87da8a690803", "embedding": null, "metadata": {"title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "citationCount": 150, "openAccessPdf": null, "authors": ["Stella Rose Biderman", "Hailey Schoelkopf", "Quentin G. Anthony", "Herbie Bradley", "Kyle O'Brien", "Eric Hallahan", "Mohammad Aflah Khan", "Shivanshu Purohit", "USVSN Sai Prashanth", "Edward Raff", "Aviya Skowron", "Lintang Sutawika", "Oskar van der Wal"], "externalIds": {"DBLP": "conf/icml/BidermanSABOHKP23", "ArXiv": "2304.01373", "DOI": "10.48550/arXiv.2304.01373", "CorpusId": 257921893}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dbeb5079-4654-4901-98e2-a47c333d1066", "node_type": null, "metadata": {"title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "citationCount": 150, "openAccessPdf": null, "authors": ["Stella Rose Biderman", "Hailey Schoelkopf", "Quentin G. Anthony", "Herbie Bradley", "Kyle O'Brien", "Eric Hallahan", "Mohammad Aflah Khan", "Shivanshu Purohit", "USVSN Sai Prashanth", "Edward Raff", "Aviya Skowron", "Lintang Sutawika", "Oskar van der Wal"], "externalIds": {"DBLP": "conf/icml/BidermanSABOHKP23", "ArXiv": "2304.01373", "DOI": "10.48550/arXiv.2304.01373", "CorpusId": 257921893}}, "hash": "a216ee3750a95d770941b83334ff5a00e96131071d56f03ad03923a4bdb29904"}}, "hash": "a216ee3750a95d770941b83334ff5a00e96131071d56f03ad03923a4bdb29904", "text": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.", "start_char_idx": 0, "end_char_idx": 1025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ccba7ccf-a07c-4c2a-b833-d714a143b17b": {"__data__": {"id_": "ccba7ccf-a07c-4c2a-b833-d714a143b17b", "embedding": null, "metadata": {"title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "venue": "arXiv.org", "year": 2023, "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "citationCount": 116, "openAccessPdf": null, "authors": ["Qinghao Ye", "Haiyang Xu", "Guohai Xu", "Jiabo Ye", "Ming Yan", "Yi Zhou", "Junyan Wang", "Anwen Hu", "Pengcheng Shi", "Yaya Shi", "Chenliang Li", "Yuanhong Xu", "Hehong Chen", "Junfeng Tian", "Qiang Qi", "Ji Zhang", "Feiyan Huang"], "externalIds": {"ArXiv": "2304.14178", "DBLP": "journals/corr/abs-2304-14178", "DOI": "10.48550/arXiv.2304.14178", "CorpusId": 258352455}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc7cdf4d-72a7-40e4-a387-416a320e7a77", "node_type": null, "metadata": {"title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "venue": "arXiv.org", "year": 2023, "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "citationCount": 116, "openAccessPdf": null, "authors": ["Qinghao Ye", "Haiyang Xu", "Guohai Xu", "Jiabo Ye", "Ming Yan", "Yi Zhou", "Junyan Wang", "Anwen Hu", "Pengcheng Shi", "Yaya Shi", "Chenliang Li", "Yuanhong Xu", "Hehong Chen", "Junfeng Tian", "Qiang Qi", "Ji Zhang", "Feiyan Huang"], "externalIds": {"ArXiv": "2304.14178", "DBLP": "journals/corr/abs-2304-14178", "DOI": "10.48550/arXiv.2304.14178", "CorpusId": 258352455}}, "hash": "e05b2c8da0d2a7d616e7454d36a76b365ac3b74a1ca1189f7350353079185108"}, "3": {"node_id": "8cc7c4ee-2c73-4664-9226-71600afb3e2a", "node_type": null, "metadata": {"title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "venue": "arXiv.org", "year": 2023, "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "citationCount": 116, "openAccessPdf": null, "authors": ["Qinghao Ye", "Haiyang Xu", "Guohai Xu", "Jiabo Ye", "Ming Yan", "Yi Zhou", "Junyan Wang", "Anwen Hu", "Pengcheng Shi", "Yaya Shi", "Chenliang Li", "Yuanhong Xu", "Hehong Chen", "Junfeng Tian", "Qiang Qi", "Ji Zhang", "Feiyan Huang"], "externalIds": {"ArXiv": "2304.14178", "DBLP": "journals/corr/abs-2304-14178", "DOI": "10.48550/arXiv.2304.14178", "CorpusId": 258352455}}, "hash": "a7f5819aedba3827819b3897fd3fa422bcd7dd38259fe15280097a3a703f4595"}}, "hash": "5fb241d2bf189706ccf33cf5c40c402a34d0f36a015b367484c7c551b7d5b8dc", "text": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly", "start_char_idx": 0, "end_char_idx": 1068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8cc7c4ee-2c73-4664-9226-71600afb3e2a": {"__data__": {"id_": "8cc7c4ee-2c73-4664-9226-71600afb3e2a", "embedding": null, "metadata": {"title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "venue": "arXiv.org", "year": 2023, "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "citationCount": 116, "openAccessPdf": null, "authors": ["Qinghao Ye", "Haiyang Xu", "Guohai Xu", "Jiabo Ye", "Ming Yan", "Yi Zhou", "Junyan Wang", "Anwen Hu", "Pengcheng Shi", "Yaya Shi", "Chenliang Li", "Yuanhong Xu", "Hehong Chen", "Junfeng Tian", "Qiang Qi", "Ji Zhang", "Feiyan Huang"], "externalIds": {"ArXiv": "2304.14178", "DBLP": "journals/corr/abs-2304-14178", "DOI": "10.48550/arXiv.2304.14178", "CorpusId": 258352455}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc7cdf4d-72a7-40e4-a387-416a320e7a77", "node_type": null, "metadata": {"title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "venue": "arXiv.org", "year": 2023, "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "citationCount": 116, "openAccessPdf": null, "authors": ["Qinghao Ye", "Haiyang Xu", "Guohai Xu", "Jiabo Ye", "Ming Yan", "Yi Zhou", "Junyan Wang", "Anwen Hu", "Pengcheng Shi", "Yaya Shi", "Chenliang Li", "Yuanhong Xu", "Hehong Chen", "Junfeng Tian", "Qiang Qi", "Ji Zhang", "Feiyan Huang"], "externalIds": {"ArXiv": "2304.14178", "DBLP": "journals/corr/abs-2304-14178", "DOI": "10.48550/arXiv.2304.14178", "CorpusId": 258352455}}, "hash": "e05b2c8da0d2a7d616e7454d36a76b365ac3b74a1ca1189f7350353079185108"}, "2": {"node_id": "ccba7ccf-a07c-4c2a-b833-d714a143b17b", "node_type": null, "metadata": {"title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "venue": "arXiv.org", "year": 2023, "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "citationCount": 116, "openAccessPdf": null, "authors": ["Qinghao Ye", "Haiyang Xu", "Guohai Xu", "Jiabo Ye", "Ming Yan", "Yi Zhou", "Junyan Wang", "Anwen Hu", "Pengcheng Shi", "Yaya Shi", "Chenliang Li", "Yuanhong Xu", "Hehong Chen", "Junfeng Tian", "Qiang Qi", "Ji Zhang", "Feiyan Huang"], "externalIds": {"ArXiv": "2304.14178", "DBLP": "journals/corr/abs-2304-14178", "DOI": "10.48550/arXiv.2304.14178", "CorpusId": 258352455}}, "hash": "5fb241d2bf189706ccf33cf5c40c402a34d0f36a015b367484c7c551b7d5b8dc"}}, "hash": "a7f5819aedba3827819b3897fd3fa422bcd7dd38259fe15280097a3a703f4595", "text": "language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.", "start_char_idx": 999, "end_char_idx": 1932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "af3a62d1-20ab-41c3-86fe-b6a3859b448a": {"__data__": {"id_": "af3a62d1-20ab-41c3-86fe-b6a3859b448a", "embedding": null, "metadata": {"title": "A Watermark for Large Language Models", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "cb5b71a622aff47014d4f28a958679629a8b6363", "citationCount": 125, "openAccessPdf": null, "authors": ["John Kirchenbauer", "Jonas Geiping", "Yuxin Wen", "Jonathan Katz", "Ian Miers", "T. Goldstein"], "externalIds": {"DBLP": "journals/corr/abs-2301-10226", "ArXiv": "2301.10226", "DOI": "10.48550/arXiv.2301.10226", "CorpusId": 256194179}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc10c197-ca0c-454c-9715-847fc89f5753", "node_type": null, "metadata": {"title": "A Watermark for Large Language Models", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "cb5b71a622aff47014d4f28a958679629a8b6363", "citationCount": 125, "openAccessPdf": null, "authors": ["John Kirchenbauer", "Jonas Geiping", "Yuxin Wen", "Jonathan Katz", "Ian Miers", "T. Goldstein"], "externalIds": {"DBLP": "journals/corr/abs-2301-10226", "ArXiv": "2301.10226", "DOI": "10.48550/arXiv.2301.10226", "CorpusId": 256194179}}, "hash": "e2c9acbfdc0384496a3c470044a8f6a7326379ade368a14dad998a9f11973205"}}, "hash": "e2c9acbfdc0384496a3c470044a8f6a7326379ade368a14dad998a9f11973205", "text": "A Watermark for Large Language Models Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.", "start_char_idx": 0, "end_char_idx": 1000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d9620c25-abf4-4b19-8c60-e2be6bd035f2": {"__data__": {"id_": "d9620c25-abf4-4b19-8c60-e2be6bd035f2", "embedding": null, "metadata": {"title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "venue": "arXiv.org", "year": 2023, "paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "citationCount": 122, "openAccessPdf": null, "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "externalIds": {"ArXiv": "2304.12244", "DBLP": "journals/corr/abs-2304-12244", "DOI": "10.48550/arXiv.2304.12244", "CorpusId": 258298159}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92196516-0ccb-43f6-9292-15a8044b5b4b", "node_type": null, "metadata": {"title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "venue": "arXiv.org", "year": 2023, "paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "citationCount": 122, "openAccessPdf": null, "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "externalIds": {"ArXiv": "2304.12244", "DBLP": "journals/corr/abs-2304-12244", "DOI": "10.48550/arXiv.2304.12244", "CorpusId": 258298159}}, "hash": "5d69d950303c81576e9621d34c1c96bf79d88c5f93774689db30b660e448f61c"}, "3": {"node_id": "87bcc8c2-7705-4f82-83ee-83c7400b0a60", "node_type": null, "metadata": {"title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "venue": "arXiv.org", "year": 2023, "paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "citationCount": 122, "openAccessPdf": null, "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "externalIds": {"ArXiv": "2304.12244", "DBLP": "journals/corr/abs-2304-12244", "DOI": "10.48550/arXiv.2304.12244", "CorpusId": 258298159}}, "hash": "ef3c814cacc60c63d88987e3387687029b747cf8e6866f8d6aa48485d8fa52e9"}}, "hash": "26db22516b5b6453ad0810c26eed465fedc01146a7a302c834e7a35df2837b50", "text": "WizardLM: Empowering Large Language Models to Follow Complex Instructions Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our", "start_char_idx": 0, "end_char_idx": 1332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "87bcc8c2-7705-4f82-83ee-83c7400b0a60": {"__data__": {"id_": "87bcc8c2-7705-4f82-83ee-83c7400b0a60", "embedding": null, "metadata": {"title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "venue": "arXiv.org", "year": 2023, "paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "citationCount": 122, "openAccessPdf": null, "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "externalIds": {"ArXiv": "2304.12244", "DBLP": "journals/corr/abs-2304-12244", "DOI": "10.48550/arXiv.2304.12244", "CorpusId": 258298159}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92196516-0ccb-43f6-9292-15a8044b5b4b", "node_type": null, "metadata": {"title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "venue": "arXiv.org", "year": 2023, "paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "citationCount": 122, "openAccessPdf": null, "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "externalIds": {"ArXiv": "2304.12244", "DBLP": "journals/corr/abs-2304-12244", "DOI": "10.48550/arXiv.2304.12244", "CorpusId": 258298159}}, "hash": "5d69d950303c81576e9621d34c1c96bf79d88c5f93774689db30b660e448f61c"}, "2": {"node_id": "d9620c25-abf4-4b19-8c60-e2be6bd035f2", "node_type": null, "metadata": {"title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "venue": "arXiv.org", "year": 2023, "paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "citationCount": 122, "openAccessPdf": null, "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "externalIds": {"ArXiv": "2304.12244", "DBLP": "journals/corr/abs-2304-12244", "DOI": "10.48550/arXiv.2304.12244", "CorpusId": 258298159}}, "hash": "26db22516b5b6453ad0810c26eed465fedc01146a7a302c834e7a35df2837b50"}}, "hash": "ef3c814cacc60c63d88987e3387687029b747cf8e6866f8d6aa48485d8fa52e9", "text": "with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM", "start_char_idx": 1255, "end_char_idx": 1397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7eea075b-2dd2-44de-9f85-ad7e42a6a039": {"__data__": {"id_": "7eea075b-2dd2-44de-9f85-ad7e42a6a039", "embedding": null, "metadata": {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "citationCount": 78, "openAccessPdf": null, "authors": ["Pan Lu", "Baolin Peng", "Hao Cheng", "Michel Galley", "Kai-Wei Chang", "Y. Wu", "Song-Chun Zhu", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2304-09842", "ArXiv": "2304.09842", "DOI": "10.48550/arXiv.2304.09842", "CorpusId": 258212542}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3234fd39-7653-4a79-a5c1-53a2c59f3b13", "node_type": null, "metadata": {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "citationCount": 78, "openAccessPdf": null, "authors": ["Pan Lu", "Baolin Peng", "Hao Cheng", "Michel Galley", "Kai-Wei Chang", "Y. Wu", "Song-Chun Zhu", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2304-09842", "ArXiv": "2304.09842", "DOI": "10.48550/arXiv.2304.09842", "CorpusId": 258212542}}, "hash": "8e0db000e5d40d1da8b1d70060d105716ccc44494e178486aa78693560b8adc4"}, "3": {"node_id": "a0259824-9d8b-45eb-80c2-5e501c61e451", "node_type": null, "metadata": {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "citationCount": 78, "openAccessPdf": null, "authors": ["Pan Lu", "Baolin Peng", "Hao Cheng", "Michel Galley", "Kai-Wei Chang", "Y. Wu", "Song-Chun Zhu", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2304-09842", "ArXiv": "2304.09842", "DOI": "10.48550/arXiv.2304.09842", "CorpusId": 258212542}}, "hash": "f52e14002640c72e531b9e309699d35117207fc39272d6542b4afcc30d3043bc"}}, "hash": "4366ee6c864bcd7b0e72a90ef3e9a96cbf21b28f967709a586a3d37ab9af9468", "text": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP,", "start_char_idx": 0, "end_char_idx": 1227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a0259824-9d8b-45eb-80c2-5e501c61e451": {"__data__": {"id_": "a0259824-9d8b-45eb-80c2-5e501c61e451", "embedding": null, "metadata": {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "citationCount": 78, "openAccessPdf": null, "authors": ["Pan Lu", "Baolin Peng", "Hao Cheng", "Michel Galley", "Kai-Wei Chang", "Y. Wu", "Song-Chun Zhu", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2304-09842", "ArXiv": "2304.09842", "DOI": "10.48550/arXiv.2304.09842", "CorpusId": 258212542}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3234fd39-7653-4a79-a5c1-53a2c59f3b13", "node_type": null, "metadata": {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "citationCount": 78, "openAccessPdf": null, "authors": ["Pan Lu", "Baolin Peng", "Hao Cheng", "Michel Galley", "Kai-Wei Chang", "Y. Wu", "Song-Chun Zhu", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2304-09842", "ArXiv": "2304.09842", "DOI": "10.48550/arXiv.2304.09842", "CorpusId": 258212542}}, "hash": "8e0db000e5d40d1da8b1d70060d105716ccc44494e178486aa78693560b8adc4"}, "2": {"node_id": "7eea075b-2dd2-44de-9f85-ad7e42a6a039", "node_type": null, "metadata": {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "citationCount": 78, "openAccessPdf": null, "authors": ["Pan Lu", "Baolin Peng", "Hao Cheng", "Michel Galley", "Kai-Wei Chang", "Y. Wu", "Song-Chun Zhu", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2304-09842", "ArXiv": "2304.09842", "DOI": "10.48550/arXiv.2304.09842", "CorpusId": 258212542}}, "hash": "4366ee6c864bcd7b0e72a90ef3e9a96cbf21b28f967709a586a3d37ab9af9468"}}, "hash": "f52e14002640c72e531b9e309699d35117207fc39272d6542b4afcc30d3043bc", "text": "improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.", "start_char_idx": 1161, "end_char_idx": 1525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "21de50ed-52ad-4757-811b-a011348864bb": {"__data__": {"id_": "21de50ed-52ad-4757-811b-a011348864bb", "embedding": null, "metadata": {"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "citationCount": 81, "openAccessPdf": null, "authors": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "externalIds": {"DBLP": "journals/corr/abs-2303-08896", "ArXiv": "2303.08896", "DOI": "10.48550/arXiv.2303.08896", "CorpusId": 257557820}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dc919c1-7b45-4d76-a07b-b4fed3e88eb4", "node_type": null, "metadata": {"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "citationCount": 81, "openAccessPdf": null, "authors": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "externalIds": {"DBLP": "journals/corr/abs-2303-08896", "ArXiv": "2303.08896", "DOI": "10.48550/arXiv.2303.08896", "CorpusId": 257557820}}, "hash": "86a70af8058503d149d4efc718b592a13e957362b2f5eb33c9eef764672a741b"}, "3": {"node_id": "8eb10aee-8654-4adf-9274-f9e6bc2c095d", "node_type": null, "metadata": {"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "citationCount": 81, "openAccessPdf": null, "authors": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "externalIds": {"DBLP": "journals/corr/abs-2303-08896", "ArXiv": "2303.08896", "DOI": "10.48550/arXiv.2303.08896", "CorpusId": 257557820}}, "hash": "bfc7d8305c5df532681a5f209c86b11df4c960f60e3d8187bb7128adf3dd22f4"}}, "hash": "fd257c0298745164c70e819d56e18ba54d53852e7a2346fdccb02e3cf5bedb73", "text": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our", "start_char_idx": 0, "end_char_idx": 1384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8eb10aee-8654-4adf-9274-f9e6bc2c095d": {"__data__": {"id_": "8eb10aee-8654-4adf-9274-f9e6bc2c095d", "embedding": null, "metadata": {"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "citationCount": 81, "openAccessPdf": null, "authors": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "externalIds": {"DBLP": "journals/corr/abs-2303-08896", "ArXiv": "2303.08896", "DOI": "10.48550/arXiv.2303.08896", "CorpusId": 257557820}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dc919c1-7b45-4d76-a07b-b4fed3e88eb4", "node_type": null, "metadata": {"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "citationCount": 81, "openAccessPdf": null, "authors": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "externalIds": {"DBLP": "journals/corr/abs-2303-08896", "ArXiv": "2303.08896", "DOI": "10.48550/arXiv.2303.08896", "CorpusId": 257557820}}, "hash": "86a70af8058503d149d4efc718b592a13e957362b2f5eb33c9eef764672a741b"}, "2": {"node_id": "21de50ed-52ad-4757-811b-a011348864bb", "node_type": null, "metadata": {"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "citationCount": 81, "openAccessPdf": null, "authors": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "externalIds": {"DBLP": "journals/corr/abs-2303-08896", "ArXiv": "2303.08896", "DOI": "10.48550/arXiv.2303.08896", "CorpusId": 257557820}}, "hash": "fd257c0298745164c70e819d56e18ba54d53852e7a2346fdccb02e3cf5bedb73"}}, "hash": "bfc7d8305c5df532681a5f209c86b11df4c960f60e3d8187bb7128adf3dd22f4", "text": "sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.", "start_char_idx": 1313, "end_char_idx": 1620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c40f5dad-f3b7-40e3-8a77-797bb40e0fb2": {"__data__": {"id_": "c40f5dad-f3b7-40e3-8a77-797bb40e0fb2", "embedding": null, "metadata": {"title": "Theory of Mind Might Have Spontaneously Emerged in Large Language Models", "venue": "", "year": 2023, "paperId": "da358fea1b9e23ebe027edec36ffbb56bd7d33f0", "citationCount": 74, "openAccessPdf": null, "authors": ["Michal Kosinskihttps://www.semanticscholar.org/me/account"], "externalIds": {"ArXiv": "2302.02083", "CorpusId": 256616268}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b879d8cd-e884-4787-b79d-18819ec7477d", "node_type": null, "metadata": {"title": "Theory of Mind Might Have Spontaneously Emerged in Large Language Models", "venue": "", "year": 2023, "paperId": "da358fea1b9e23ebe027edec36ffbb56bd7d33f0", "citationCount": 74, "openAccessPdf": null, "authors": ["Michal Kosinskihttps://www.semanticscholar.org/me/account"], "externalIds": {"ArXiv": "2302.02083", "CorpusId": 256616268}}, "hash": "6dcc06742cd26418bce0469b8799b095856f1b98ba2496ea173fceaa8370da5f"}}, "hash": "6dcc06742cd26418bce0469b8799b095856f1b98ba2496ea173fceaa8370da5f", "text": "Theory of Mind Might Have Spontaneously Emerged in Large Language Models We explore the intriguing possibility that theory of mind (ToM), or the uniquely human ability to impute unobservable mental states to others, might have spontaneously emerged in large language models (LLMs). We designed 40 false-belief tasks, considered a gold standard in testing ToM in humans, and administered them to several LLMs. Each task included a false-belief scenario, three closely matched true-belief controls, and the reversed versions of all four. Smaller and older models solved no tasks; GPT-3-davinci-001 (from May 2020) and GPT-3-davinci-002 (from January 2022) solved 10%; and GPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 35% of the tasks, mirroring the performance of three-year-old children. ChatGPT-4 (from June 2023) solved 90% of the tasks, matching the performance of seven-year-old children. These findings suggest the intriguing possibility that ToM, previously considered exclusive to humans, may have spontaneously emerged as a byproduct of LLMs' improving language skills.", "start_char_idx": 0, "end_char_idx": 1117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "09861dcc-9906-4806-b515-15c80ef781b6": {"__data__": {"id_": "09861dcc-9906-4806-b515-15c80ef781b6", "embedding": null, "metadata": {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "venue": "arXiv.org", "year": 2023, "paperId": "9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3", "citationCount": 69, "openAccessPdf": null, "authors": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "externalIds": {"ArXiv": "2302.01560", "DBLP": "journals/corr/abs-2302-01560", "DOI": "10.48550/arXiv.2302.01560", "CorpusId": 256598146}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da5f1c01-d813-43ae-9217-2d0346f39ca4", "node_type": null, "metadata": {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "venue": "arXiv.org", "year": 2023, "paperId": "9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3", "citationCount": 69, "openAccessPdf": null, "authors": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "externalIds": {"ArXiv": "2302.01560", "DBLP": "journals/corr/abs-2302-01560", "DOI": "10.48550/arXiv.2302.01560", "CorpusId": 256598146}}, "hash": "077919e3b1a75e81196e05e497fd26da888de190fef56e6b80973bdb9dc26c75"}, "3": {"node_id": "36e58545-d8a8-43df-90e4-ffb3c6e6457b", "node_type": null, "metadata": {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "venue": "arXiv.org", "year": 2023, "paperId": "9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3", "citationCount": 69, "openAccessPdf": null, "authors": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "externalIds": {"ArXiv": "2302.01560", "DBLP": "journals/corr/abs-2302-01560", "DOI": "10.48550/arXiv.2302.01560", "CorpusId": 256598146}}, "hash": "23dace1c8d9f389a1087415d9d8570c15d877e86e18f2f9dc7db3636e985f30d"}}, "hash": "bea505a7b6391a0fe95c17246b188037c11d7606da5af5b91a06834738c1bee3", "text": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents. We've found two primary challenges of empowering such agents with planning: 1) planning in an open-ended world like Minecraft requires precise and multi-step reasoning due to the long-term nature of the tasks, and 2) as vanilla planners do not consider the proximity to the current agent when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient. To this end, we propose\"Describe, Explain, Plan and Select\"(DEPS), an interactive planning approach based on Large Language Models (LLMs). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal Selector, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly doubles the overall performances. Finally, the ablation and exploratory studies detail how our design beats the counterparts and", "start_char_idx": 0, "end_char_idx": 1365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "36e58545-d8a8-43df-90e4-ffb3c6e6457b": {"__data__": {"id_": "36e58545-d8a8-43df-90e4-ffb3c6e6457b", "embedding": null, "metadata": {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "venue": "arXiv.org", "year": 2023, "paperId": "9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3", "citationCount": 69, "openAccessPdf": null, "authors": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "externalIds": {"ArXiv": "2302.01560", "DBLP": "journals/corr/abs-2302-01560", "DOI": "10.48550/arXiv.2302.01560", "CorpusId": 256598146}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da5f1c01-d813-43ae-9217-2d0346f39ca4", "node_type": null, "metadata": {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "venue": "arXiv.org", "year": 2023, "paperId": "9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3", "citationCount": 69, "openAccessPdf": null, "authors": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "externalIds": {"ArXiv": "2302.01560", "DBLP": "journals/corr/abs-2302-01560", "DOI": "10.48550/arXiv.2302.01560", "CorpusId": 256598146}}, "hash": "077919e3b1a75e81196e05e497fd26da888de190fef56e6b80973bdb9dc26c75"}, "2": {"node_id": "09861dcc-9906-4806-b515-15c80ef781b6", "node_type": null, "metadata": {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "venue": "arXiv.org", "year": 2023, "paperId": "9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3", "citationCount": 69, "openAccessPdf": null, "authors": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "externalIds": {"ArXiv": "2302.01560", "DBLP": "journals/corr/abs-2302-01560", "DOI": "10.48550/arXiv.2302.01560", "CorpusId": 256598146}}, "hash": "bea505a7b6391a0fe95c17246b188037c11d7606da5af5b91a06834738c1bee3"}}, "hash": "23dace1c8d9f389a1087415d9d8570c15d877e86e18f2f9dc7db3636e985f30d", "text": "the ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.", "start_char_idx": 1280, "end_char_idx": 1526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f8150398-a7b7-46b3-bcf5-8a2783bb228d": {"__data__": {"id_": "f8150398-a7b7-46b3-bcf5-8a2783bb228d", "embedding": null, "metadata": {"title": "Teaching Large Language Models to Self-Debug", "venue": "arXiv.org", "year": 2023, "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "citationCount": 96, "openAccessPdf": null, "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Sch\u00e4rli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2304-05128", "ArXiv": "2304.05128", "DOI": "10.48550/arXiv.2304.05128", "CorpusId": 258059885}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55e6e486-bc6e-4073-bba5-89639e708e8c", "node_type": null, "metadata": {"title": "Teaching Large Language Models to Self-Debug", "venue": "arXiv.org", "year": 2023, "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "citationCount": 96, "openAccessPdf": null, "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Sch\u00e4rli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2304-05128", "ArXiv": "2304.05128", "DOI": "10.48550/arXiv.2304.05128", "CorpusId": 258059885}}, "hash": "55774d45084da3aa4b6ac801729df7d2c961155affbc806fc9db602ccf6b586d"}, "3": {"node_id": "236bba97-317c-442d-b369-ddb085356bee", "node_type": null, "metadata": {"title": "Teaching Large Language Models to Self-Debug", "venue": "arXiv.org", "year": 2023, "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "citationCount": 96, "openAccessPdf": null, "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Sch\u00e4rli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2304-05128", "ArXiv": "2304.05128", "DOI": "10.48550/arXiv.2304.05128", "CorpusId": 258059885}}, "hash": "258aa52cd6000aedff5068caebf0779d7a96f1a54e289c8ae404fda042c703bb"}}, "hash": "c24ac578322123ba75fdc464a852228a4f42ca6e2c55cf5a7d0417c7d97f9caf", "text": "Teaching Large Language Models to Self-Debug Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging", "start_char_idx": 0, "end_char_idx": 1499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "236bba97-317c-442d-b369-ddb085356bee": {"__data__": {"id_": "236bba97-317c-442d-b369-ddb085356bee", "embedding": null, "metadata": {"title": "Teaching Large Language Models to Self-Debug", "venue": "arXiv.org", "year": 2023, "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "citationCount": 96, "openAccessPdf": null, "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Sch\u00e4rli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2304-05128", "ArXiv": "2304.05128", "DOI": "10.48550/arXiv.2304.05128", "CorpusId": 258059885}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55e6e486-bc6e-4073-bba5-89639e708e8c", "node_type": null, "metadata": {"title": "Teaching Large Language Models to Self-Debug", "venue": "arXiv.org", "year": 2023, "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "citationCount": 96, "openAccessPdf": null, "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Sch\u00e4rli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2304-05128", "ArXiv": "2304.05128", "DOI": "10.48550/arXiv.2304.05128", "CorpusId": 258059885}}, "hash": "55774d45084da3aa4b6ac801729df7d2c961155affbc806fc9db602ccf6b586d"}, "2": {"node_id": "f8150398-a7b7-46b3-bcf5-8a2783bb228d", "node_type": null, "metadata": {"title": "Teaching Large Language Models to Self-Debug", "venue": "arXiv.org", "year": 2023, "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "citationCount": 96, "openAccessPdf": null, "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Sch\u00e4rli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2304-05128", "ArXiv": "2304.05128", "DOI": "10.48550/arXiv.2304.05128", "CorpusId": 258059885}}, "hash": "c24ac578322123ba75fdc464a852228a4f42ca6e2c55cf5a7d0417c7d97f9caf"}}, "hash": "258aa52cd6000aedff5068caebf0779d7a96f1a54e289c8ae404fda042c703bb", "text": "by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.", "start_char_idx": 1421, "end_char_idx": 1627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8f1c72ae-ed18-466f-ae6d-55600385b60e": {"__data__": {"id_": "8f1c72ae-ed18-466f-ae6d-55600385b60e", "embedding": null, "metadata": {"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "5501d00310b06e00351295529498cc684187148d", "citationCount": 118, "openAccessPdf": null, "authors": ["Tyna Eloundou", "Sam Manning", "Pamela Mishkin", "Daniel Rock"], "externalIds": {"ArXiv": "2303.10130", "DBLP": "journals/corr/abs-2303-10130", "CorpusId": 257622601}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5c97630-d99a-4175-bc31-b30aa39d8535", "node_type": null, "metadata": {"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "5501d00310b06e00351295529498cc684187148d", "citationCount": 118, "openAccessPdf": null, "authors": ["Tyna Eloundou", "Sam Manning", "Pamela Mishkin", "Daniel Rock"], "externalIds": {"ArXiv": "2303.10130", "DBLP": "journals/corr/abs-2303-10130", "CorpusId": 257622601}}, "hash": "329958cb7028c1f8c16187de17ca589abab1edaad98bb01b1bbc0c137a10cb51"}, "3": {"node_id": "b595587e-a0a9-4374-bfd0-0fd465e80743", "node_type": null, "metadata": {"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "5501d00310b06e00351295529498cc684187148d", "citationCount": 118, "openAccessPdf": null, "authors": ["Tyna Eloundou", "Sam Manning", "Pamela Mishkin", "Daniel Rock"], "externalIds": {"ArXiv": "2303.10130", "DBLP": "journals/corr/abs-2303-10130", "CorpusId": 257622601}}, "hash": "86671d8f9170bc34104c15e8093327a165a4f4a4db3130924482d4a639f3fca7"}}, "hash": "2e0e6e31b93d1e9fa7679a9ab4c966c2ab037086979fe0e883007808d11e047b", "text": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs", "start_char_idx": 0, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b595587e-a0a9-4374-bfd0-0fd465e80743": {"__data__": {"id_": "b595587e-a0a9-4374-bfd0-0fd465e80743", "embedding": null, "metadata": {"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "5501d00310b06e00351295529498cc684187148d", "citationCount": 118, "openAccessPdf": null, "authors": ["Tyna Eloundou", "Sam Manning", "Pamela Mishkin", "Daniel Rock"], "externalIds": {"ArXiv": "2303.10130", "DBLP": "journals/corr/abs-2303-10130", "CorpusId": 257622601}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5c97630-d99a-4175-bc31-b30aa39d8535", "node_type": null, "metadata": {"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "5501d00310b06e00351295529498cc684187148d", "citationCount": 118, "openAccessPdf": null, "authors": ["Tyna Eloundou", "Sam Manning", "Pamela Mishkin", "Daniel Rock"], "externalIds": {"ArXiv": "2303.10130", "DBLP": "journals/corr/abs-2303-10130", "CorpusId": 257622601}}, "hash": "329958cb7028c1f8c16187de17ca589abab1edaad98bb01b1bbc0c137a10cb51"}, "2": {"node_id": "8f1c72ae-ed18-466f-ae6d-55600385b60e", "node_type": null, "metadata": {"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "5501d00310b06e00351295529498cc684187148d", "citationCount": 118, "openAccessPdf": null, "authors": ["Tyna Eloundou", "Sam Manning", "Pamela Mishkin", "Daniel Rock"], "externalIds": {"ArXiv": "2303.10130", "DBLP": "journals/corr/abs-2303-10130", "CorpusId": 257622601}}, "hash": "2e0e6e31b93d1e9fa7679a9ab4c966c2ab037086979fe0e883007808d11e047b"}}, "hash": "86671d8f9170bc34104c15e8093327a165a4f4a4db3130924482d4a639f3fca7", "text": "economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.", "start_char_idx": 1428, "end_char_idx": 1641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1ef81d84-76c3-49b6-a697-0f1aacb7b67a": {"__data__": {"id_": "1ef81d84-76c3-49b6-a697-0f1aacb7b67a", "embedding": null, "metadata": {"title": "Dissociating language and thought in large language models: a cognitive perspective", "venue": "arXiv.org", "year": 2023, "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d", "citationCount": 96, "openAccessPdf": null, "authors": ["Kyle Mahowald", "Anna A. Ivanova", "I. Blank", "N. Kanwisher", "J. Tenenbaum", "Evelina Fedorenko"], "externalIds": {"DBLP": "journals/corr/abs-2301-06627", "ArXiv": "2301.06627", "DOI": "10.48550/arXiv.2301.06627", "CorpusId": 255941592}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a73bcb04-a9cd-4978-b5f1-1fc1fe019048", "node_type": null, "metadata": {"title": "Dissociating language and thought in large language models: a cognitive perspective", "venue": "arXiv.org", "year": 2023, "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d", "citationCount": 96, "openAccessPdf": null, "authors": ["Kyle Mahowald", "Anna A. Ivanova", "I. Blank", "N. Kanwisher", "J. Tenenbaum", "Evelina Fedorenko"], "externalIds": {"DBLP": "journals/corr/abs-2301-06627", "ArXiv": "2301.06627", "DOI": "10.48550/arXiv.2301.06627", "CorpusId": 255941592}}, "hash": "6d089637c26fadd9a8073b512b8e5fbec7ae560ee3e86c5a254941469a7bdde1"}, "3": {"node_id": "9ac10e17-e2d0-4606-9d34-d55dd28e076d", "node_type": null, "metadata": {"title": "Dissociating language and thought in large language models: a cognitive perspective", "venue": "arXiv.org", "year": 2023, "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d", "citationCount": 96, "openAccessPdf": null, "authors": ["Kyle Mahowald", "Anna A. Ivanova", "I. Blank", "N. Kanwisher", "J. Tenenbaum", "Evelina Fedorenko"], "externalIds": {"DBLP": "journals/corr/abs-2301-06627", "ArXiv": "2301.06627", "DOI": "10.48550/arXiv.2301.06627", "CorpusId": 255941592}}, "hash": "4681b37f8ae24808cceba4aa2410c44e074ccfd1ea478e0a2468a7fd243b9655"}}, "hash": "0b712a610351980e1092e7de01428e7beddb1158f551c2f46d39d6d36b5a84b8", "text": "Dissociating language and thought in large language models: a cognitive perspective Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life", "start_char_idx": 0, "end_char_idx": 1421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9ac10e17-e2d0-4606-9d34-d55dd28e076d": {"__data__": {"id_": "9ac10e17-e2d0-4606-9d34-d55dd28e076d", "embedding": null, "metadata": {"title": "Dissociating language and thought in large language models: a cognitive perspective", "venue": "arXiv.org", "year": 2023, "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d", "citationCount": 96, "openAccessPdf": null, "authors": ["Kyle Mahowald", "Anna A. Ivanova", "I. Blank", "N. Kanwisher", "J. Tenenbaum", "Evelina Fedorenko"], "externalIds": {"DBLP": "journals/corr/abs-2301-06627", "ArXiv": "2301.06627", "DOI": "10.48550/arXiv.2301.06627", "CorpusId": 255941592}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a73bcb04-a9cd-4978-b5f1-1fc1fe019048", "node_type": null, "metadata": {"title": "Dissociating language and thought in large language models: a cognitive perspective", "venue": "arXiv.org", "year": 2023, "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d", "citationCount": 96, "openAccessPdf": null, "authors": ["Kyle Mahowald", "Anna A. Ivanova", "I. Blank", "N. Kanwisher", "J. Tenenbaum", "Evelina Fedorenko"], "externalIds": {"DBLP": "journals/corr/abs-2301-06627", "ArXiv": "2301.06627", "DOI": "10.48550/arXiv.2301.06627", "CorpusId": 255941592}}, "hash": "6d089637c26fadd9a8073b512b8e5fbec7ae560ee3e86c5a254941469a7bdde1"}, "2": {"node_id": "1ef81d84-76c3-49b6-a697-0f1aacb7b67a", "node_type": null, "metadata": {"title": "Dissociating language and thought in large language models: a cognitive perspective", "venue": "arXiv.org", "year": 2023, "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d", "citationCount": 96, "openAccessPdf": null, "authors": ["Kyle Mahowald", "Anna A. Ivanova", "I. Blank", "N. Kanwisher", "J. Tenenbaum", "Evelina Fedorenko"], "externalIds": {"DBLP": "journals/corr/abs-2301-06627", "ArXiv": "2301.06627", "DOI": "10.48550/arXiv.2301.06627", "CorpusId": 255941592}}, "hash": "0b712a610351980e1092e7de01428e7beddb1158f551c2f46d39d6d36b5a84b8"}}, "hash": "4681b37f8ae24808cceba4aa2410c44e074ccfd1ea478e0a2468a7fd243b9655", "text": "as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.", "start_char_idx": 1350, "end_char_idx": 1823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b030a5f8-ac32-4683-a6f6-d1442e6f1aa5": {"__data__": {"id_": "b030a5f8-ac32-4683-a6f6-d1442e6f1aa5", "embedding": null, "metadata": {"title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "venue": "arXiv.org", "year": 2023, "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805", "citationCount": 55, "openAccessPdf": null, "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "externalIds": {"ArXiv": "2306.08568", "DBLP": "journals/corr/abs-2306-08568", "DOI": "10.48550/arXiv.2306.08568", "CorpusId": 259164815}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67b87faa-cc33-4f08-863a-92ed7b442b87", "node_type": null, "metadata": {"title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "venue": "arXiv.org", "year": 2023, "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805", "citationCount": 55, "openAccessPdf": null, "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "externalIds": {"ArXiv": "2306.08568", "DBLP": "journals/corr/abs-2306-08568", "DOI": "10.48550/arXiv.2306.08568", "CorpusId": 259164815}}, "hash": "aea3993e4b9e4d23da2a60c9bae949e1d1ae2a71f06a6267dc42d588e436b0b5"}}, "hash": "aea3993e4b9e4d23da2a60c9bae949e1d1ae2a71f06a6267dc42d588e436b0b5", "text": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "start_char_idx": 0, "end_char_idx": 936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c73b9439-1f3e-42d3-8b50-f4958199bfde": {"__data__": {"id_": "c73b9439-1f3e-42d3-8b50-f4958199bfde", "embedding": null, "metadata": {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "citationCount": 213, "openAccessPdf": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "externalIds": {"ArXiv": "2305.10601", "DBLP": "journals/corr/abs-2305-10601", "DOI": "10.48550/arXiv.2305.10601", "CorpusId": 258762525}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc24877-e43c-40fc-893c-aa6f9a1c917f", "node_type": null, "metadata": {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "citationCount": 213, "openAccessPdf": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "externalIds": {"ArXiv": "2305.10601", "DBLP": "journals/corr/abs-2305-10601", "DOI": "10.48550/arXiv.2305.10601", "CorpusId": 258762525}}, "hash": "2b848de29c6bf7b7d02f71e1022ba7bcb4c37b76d8cffac7a8574ad7ad04bd48"}, "3": {"node_id": "9c9e7d52-0514-4f79-b5ab-586cbde1f5d3", "node_type": null, "metadata": {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "citationCount": 213, "openAccessPdf": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "externalIds": {"ArXiv": "2305.10601", "DBLP": "journals/corr/abs-2305-10601", "DOI": "10.48550/arXiv.2305.10601", "CorpusId": 258762525}}, "hash": "b0b509a6e97f668c72f1cb812ee5767c716fce75389d2224afb2256faad57dc7"}}, "hash": "a328ab9579c6152e473e5d6746a4a9f793cfbf8729d5abb0b62e788a6193f7b3", "text": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all", "start_char_idx": 0, "end_char_idx": 1351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9c9e7d52-0514-4f79-b5ab-586cbde1f5d3": {"__data__": {"id_": "9c9e7d52-0514-4f79-b5ab-586cbde1f5d3", "embedding": null, "metadata": {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "citationCount": 213, "openAccessPdf": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "externalIds": {"ArXiv": "2305.10601", "DBLP": "journals/corr/abs-2305-10601", "DOI": "10.48550/arXiv.2305.10601", "CorpusId": 258762525}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc24877-e43c-40fc-893c-aa6f9a1c917f", "node_type": null, "metadata": {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "citationCount": 213, "openAccessPdf": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "externalIds": {"ArXiv": "2305.10601", "DBLP": "journals/corr/abs-2305-10601", "DOI": "10.48550/arXiv.2305.10601", "CorpusId": 258762525}}, "hash": "2b848de29c6bf7b7d02f71e1022ba7bcb4c37b76d8cffac7a8574ad7ad04bd48"}, "2": {"node_id": "c73b9439-1f3e-42d3-8b50-f4958199bfde", "node_type": null, "metadata": {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "citationCount": 213, "openAccessPdf": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "externalIds": {"ArXiv": "2305.10601", "DBLP": "journals/corr/abs-2305-10601", "DOI": "10.48550/arXiv.2305.10601", "CorpusId": 258762525}}, "hash": "a328ab9579c6152e473e5d6746a4a9f793cfbf8729d5abb0b62e788a6193f7b3"}}, "hash": "b0b509a6e97f668c72f1cb812ee5767c716fce75389d2224afb2256faad57dc7", "text": "of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.", "start_char_idx": 1280, "end_char_idx": 1408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c2d0994-4bd0-41f7-ad53-31422c268a09": {"__data__": {"id_": "8c2d0994-4bd0-41f7-ad53-31422c268a09", "embedding": null, "metadata": {"title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality", "venue": "European Association for Machine Translation Conferences/Workshops", "year": 2023, "paperId": "4161ad2d2495d8af1d62dc5e71882bde642cd1c1", "citationCount": 68, "openAccessPdf": null, "authors": ["Tom Kocmi", "C. Federmann"], "externalIds": {"ArXiv": "2302.14520", "DBLP": "journals/corr/abs-2302-14520", "ACL": "2023.eamt-1.19", "DOI": "10.48550/arXiv.2302.14520", "CorpusId": 257232490}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "037cd110-ee3a-4926-8674-4817122f5327", "node_type": null, "metadata": {"title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality", "venue": "European Association for Machine Translation Conferences/Workshops", "year": 2023, "paperId": "4161ad2d2495d8af1d62dc5e71882bde642cd1c1", "citationCount": 68, "openAccessPdf": null, "authors": ["Tom Kocmi", "C. Federmann"], "externalIds": {"ArXiv": "2302.14520", "DBLP": "journals/corr/abs-2302-14520", "ACL": "2023.eamt-1.19", "DOI": "10.48550/arXiv.2302.14520", "CorpusId": 257232490}}, "hash": "39ef86a0356dbe4e8df049597a04b4e9e390ed1f83758a1f656379b3268e55ef"}}, "hash": "39ef86a0356dbe4e8df049597a04b4e9e390ed1f83758a1f656379b3268e55ef", "text": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate seven versions of GPT models, including ChatGPT. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22\u2019s Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.", "start_char_idx": 0, "end_char_idx": 1191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bb5ea7d5-a2af-45cc-83d0-ea2ac11f1fe7": {"__data__": {"id_": "bb5ea7d5-a2af-45cc-83d0-ea2ac11f1fe7", "embedding": null, "metadata": {"title": "ChatGPT and a new academic reality: Artificial Intelligence\u2010written research papers and the ethics of the large language models in scholarly publishing", "venue": "J. Assoc. Inf. Sci. Technol.", "year": 2023, "paperId": "da9683e826c37a6383c124b5c6cddefcb35ee8fd", "citationCount": 82, "openAccessPdf": "https://arxiv.org/pdf/2303.13367", "authors": ["Brady D. Lund", "Ting Wang", "Nishith Reddy Mannuru", "Bing Nie", "S. Shimray", "Ziang Wang"], "externalIds": {"DBLP": "journals/corr/abs-2303-13367", "ArXiv": "2303.13367", "DOI": "10.1002/asi.24750", "CorpusId": 257463753}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e3e953f-e419-4782-8eae-dc40d3bb8f26", "node_type": null, "metadata": {"title": "ChatGPT and a new academic reality: Artificial Intelligence\u2010written research papers and the ethics of the large language models in scholarly publishing", "venue": "J. Assoc. Inf. Sci. Technol.", "year": 2023, "paperId": "da9683e826c37a6383c124b5c6cddefcb35ee8fd", "citationCount": 82, "openAccessPdf": "https://arxiv.org/pdf/2303.13367", "authors": ["Brady D. Lund", "Ting Wang", "Nishith Reddy Mannuru", "Bing Nie", "S. Shimray", "Ziang Wang"], "externalIds": {"DBLP": "journals/corr/abs-2303-13367", "ArXiv": "2303.13367", "DOI": "10.1002/asi.24750", "CorpusId": 257463753}}, "hash": "7a8d6e1cd3fc82394d8cf282c813a37114ea7228cacf42e811fd7fcbfc3d530b"}}, "hash": "7a8d6e1cd3fc82394d8cf282c813a37114ea7228cacf42e811fd7fcbfc3d530b", "text": "ChatGPT and a new academic reality: Artificial Intelligence\u2010written research papers and the ethics of the large language models in scholarly publishing This article discusses OpenAI's ChatGPT, a generative pre\u2010trained transformer, which uses natural language processing to fulfill text\u2010based user requests (i.e., a \u201cchatbot\u201d). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT\u20103, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.", "start_char_idx": 0, "end_char_idx": 1013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0497ec8e-ad55-4ba1-a0e2-8c8fe4429b1f": {"__data__": {"id_": "0497ec8e-ad55-4ba1-a0e2-8c8fe4429b1f", "embedding": null, "metadata": {"title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e", "citationCount": 67, "openAccessPdf": null, "authors": ["Freda Shi", "Xinyun Chen", "Kanishka Misra", "Nathan Scales", "David Dohan", "E. Chi", "Nathanael Scharli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2302-00093", "ArXiv": "2302.00093", "DOI": "10.48550/arXiv.2302.00093", "CorpusId": 256459776}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26316f62-a24c-46fb-afff-b354925bc1c8", "node_type": null, "metadata": {"title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e", "citationCount": 67, "openAccessPdf": null, "authors": ["Freda Shi", "Xinyun Chen", "Kanishka Misra", "Nathan Scales", "David Dohan", "E. Chi", "Nathanael Scharli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2302-00093", "ArXiv": "2302.00093", "DOI": "10.48550/arXiv.2302.00093", "CorpusId": 256459776}}, "hash": "bae8eee3af9ea0c36f909ae05f5896fb0a9810b65ef70b96c8e0f78b6a8d016f"}}, "hash": "bae8eee3af9ea0c36f909ae05f5896fb0a9810b65ef70b96c8e0f78b6a8d016f", "text": "Large Language Models Can Be Easily Distracted by Irrelevant Context Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "start_char_idx": 0, "end_char_idx": 1079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7129cae3-93a6-45be-a1ab-0104a6c46987": {"__data__": {"id_": "7129cae3-93a6-45be-a1ab-0104a6c46987", "embedding": null, "metadata": {"title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "venue": "arXiv.org", "year": 2023, "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "citationCount": 108, "openAccessPdf": null, "authors": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lid\u00e9n", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2302-12813", "ArXiv": "2302.12813", "DOI": "10.48550/arXiv.2302.12813", "CorpusId": 257205781}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12b644db-be9e-45c7-91f3-d6a67efa7afa", "node_type": null, "metadata": {"title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "venue": "arXiv.org", "year": 2023, "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "citationCount": 108, "openAccessPdf": null, "authors": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lid\u00e9n", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2302-12813", "ArXiv": "2302.12813", "DOI": "10.48550/arXiv.2302.12813", "CorpusId": 257205781}}, "hash": "f6726bb1e34c7fb9ebba5a98b8566396dfdd2bfa7a0831c75cb2c3763025eb59"}, "3": {"node_id": "c92b16b4-8633-4dd4-bce0-f8eceef1ebad", "node_type": null, "metadata": {"title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "venue": "arXiv.org", "year": 2023, "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "citationCount": 108, "openAccessPdf": null, "authors": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lid\u00e9n", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2302-12813", "ArXiv": "2302.12813", "DOI": "10.48550/arXiv.2302.12813", "CorpusId": 257205781}}, "hash": "356da5d945539156d020a22bf7cba63dd3f319115d48e53349e20cdce644cb52"}}, "hash": "8dc2bd98400074cc7f3c77c38f95c220e4b444b21114c5840d88d1d95833ac7d", "text": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and", "start_char_idx": 0, "end_char_idx": 1178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c92b16b4-8633-4dd4-bce0-f8eceef1ebad": {"__data__": {"id_": "c92b16b4-8633-4dd4-bce0-f8eceef1ebad", "embedding": null, "metadata": {"title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "venue": "arXiv.org", "year": 2023, "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "citationCount": 108, "openAccessPdf": null, "authors": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lid\u00e9n", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2302-12813", "ArXiv": "2302.12813", "DOI": "10.48550/arXiv.2302.12813", "CorpusId": 257205781}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12b644db-be9e-45c7-91f3-d6a67efa7afa", "node_type": null, "metadata": {"title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "venue": "arXiv.org", "year": 2023, "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "citationCount": 108, "openAccessPdf": null, "authors": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lid\u00e9n", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2302-12813", "ArXiv": "2302.12813", "DOI": "10.48550/arXiv.2302.12813", "CorpusId": 257205781}}, "hash": "f6726bb1e34c7fb9ebba5a98b8566396dfdd2bfa7a0831c75cb2c3763025eb59"}, "2": {"node_id": "7129cae3-93a6-45be-a1ab-0104a6c46987", "node_type": null, "metadata": {"title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "venue": "arXiv.org", "year": 2023, "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "citationCount": 108, "openAccessPdf": null, "authors": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lid\u00e9n", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2302-12813", "ArXiv": "2302.12813", "DOI": "10.48550/arXiv.2302.12813", "CorpusId": 257205781}}, "hash": "8dc2bd98400074cc7f3c77c38f95c220e4b444b21114c5840d88d1d95833ac7d"}}, "hash": "356da5d945539156d020a22bf7cba63dd3f319115d48e53349e20cdce644cb52", "text": "the fluency and informativeness of its responses. We make the source code and models publicly available.", "start_char_idx": 1101, "end_char_idx": 1205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0ced9a6f-0b73-4247-9619-dfc9de229cc0": {"__data__": {"id_": "0ced9a6f-0b73-4247-9619-dfc9de229cc0", "embedding": null, "metadata": {"title": "Large Language Models are not Fair Evaluators", "venue": "arXiv.org", "year": 2023, "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "citationCount": 74, "openAccessPdf": null, "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "externalIds": {"ArXiv": "2305.17926", "DBLP": "journals/corr/abs-2305-17926", "DOI": "10.48550/arXiv.2305.17926", "CorpusId": 258960339}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af725450-3369-4ed4-8f24-a8c80adb25dd", "node_type": null, "metadata": {"title": "Large Language Models are not Fair Evaluators", "venue": "arXiv.org", "year": 2023, "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "citationCount": 74, "openAccessPdf": null, "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "externalIds": {"ArXiv": "2305.17926", "DBLP": "journals/corr/abs-2305-17926", "DOI": "10.48550/arXiv.2305.17926", "CorpusId": 258960339}}, "hash": "be83c926ff988c4d06e89942d881de2a4b795cbf8cbbf0c7d24b13dec8126a0c"}, "3": {"node_id": "9187459a-bfae-4caa-b74e-815145561e7b", "node_type": null, "metadata": {"title": "Large Language Models are not Fair Evaluators", "venue": "arXiv.org", "year": 2023, "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "citationCount": 74, "openAccessPdf": null, "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "externalIds": {"ArXiv": "2305.17926", "DBLP": "journals/corr/abs-2305-17926", "DOI": "10.48550/arXiv.2305.17926", "CorpusId": 258960339}}, "hash": "c228dbc11f330b589308de19456f9c94534c223555d4ee9abd0338448382ce04"}}, "hash": "db1ea487b08ed89230eaedc14d65f5cd0d1a0c4af1f5936b6dbf189c29cca21b", "text": "Large Language Models are not Fair Evaluators In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\"win/tie/lose\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's", "start_char_idx": 0, "end_char_idx": 1252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9187459a-bfae-4caa-b74e-815145561e7b": {"__data__": {"id_": "9187459a-bfae-4caa-b74e-815145561e7b", "embedding": null, "metadata": {"title": "Large Language Models are not Fair Evaluators", "venue": "arXiv.org", "year": 2023, "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "citationCount": 74, "openAccessPdf": null, "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "externalIds": {"ArXiv": "2305.17926", "DBLP": "journals/corr/abs-2305-17926", "DOI": "10.48550/arXiv.2305.17926", "CorpusId": 258960339}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af725450-3369-4ed4-8f24-a8c80adb25dd", "node_type": null, "metadata": {"title": "Large Language Models are not Fair Evaluators", "venue": "arXiv.org", "year": 2023, "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "citationCount": 74, "openAccessPdf": null, "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "externalIds": {"ArXiv": "2305.17926", "DBLP": "journals/corr/abs-2305-17926", "DOI": "10.48550/arXiv.2305.17926", "CorpusId": 258960339}}, "hash": "be83c926ff988c4d06e89942d881de2a4b795cbf8cbbf0c7d24b13dec8126a0c"}, "2": {"node_id": "0ced9a6f-0b73-4247-9619-dfc9de229cc0", "node_type": null, "metadata": {"title": "Large Language Models are not Fair Evaluators", "venue": "arXiv.org", "year": 2023, "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "citationCount": 74, "openAccessPdf": null, "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "externalIds": {"ArXiv": "2305.17926", "DBLP": "journals/corr/abs-2305-17926", "DOI": "10.48550/arXiv.2305.17926", "CorpusId": 258960339}}, "hash": "db1ea487b08ed89230eaedc14d65f5cd0d1a0c4af1f5936b6dbf189c29cca21b"}}, "hash": "c228dbc11f330b589308de19456f9c94534c223555d4ee9abd0338448382ce04", "text": "from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.", "start_char_idx": 1199, "end_char_idx": 1533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "832ae626-c45b-49f1-80bb-218120a7f0d9": {"__data__": {"id_": "832ae626-c45b-49f1-80bb-218120a7f0d9", "embedding": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "8187ad14bdcc3b6ccefd779afc604bfe61c0754ee94c9ad74624197993063aee"}, "3": {"node_id": "c0b99e15-2625-4f6d-84cd-1c60de929e27", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "ceddc0641a72e15a4913739325394fa9641e8dea04321f0fef6580f10ce33cfe"}}, "hash": "d87ee38eda03453f97e3c0d7844103d068eceaba2127d5b89d0e2a86a764613f", "text": "Towards Expert-Level Medical Question Answering with Large Language Models Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for", "start_char_idx": 0, "end_char_idx": 685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0b99e15-2625-4f6d-84cd-1c60de929e27": {"__data__": {"id_": "c0b99e15-2625-4f6d-84cd-1c60de929e27", "embedding": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "8187ad14bdcc3b6ccefd779afc604bfe61c0754ee94c9ad74624197993063aee"}, "2": {"node_id": "832ae626-c45b-49f1-80bb-218120a7f0d9", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "d87ee38eda03453f97e3c0d7844103d068eceaba2127d5b89d0e2a86a764613f"}, "3": {"node_id": "a10f5859-717a-4f94-a2be-5771da77b16e", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "efad2e4da3391513f1114298c42008fbd33ee9ebd840e24a38cb4bfd1348b533"}}, "hash": "ceddc0641a72e15a4913739325394fa9641e8dea04321f0fef6580f10ce33cfe", "text": "MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across", "start_char_idx": 617, "end_char_idx": 1198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a10f5859-717a-4f94-a2be-5771da77b16e": {"__data__": {"id_": "a10f5859-717a-4f94-a2be-5771da77b16e", "embedding": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "8187ad14bdcc3b6ccefd779afc604bfe61c0754ee94c9ad74624197993063aee"}, "2": {"node_id": "c0b99e15-2625-4f6d-84cd-1c60de929e27", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "ceddc0641a72e15a4913739325394fa9641e8dea04321f0fef6580f10ce33cfe"}, "3": {"node_id": "6b21224f-cfc0-4f90-a1c0-8076867d148e", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "af2dd6a12448c57e9bea7adc2fc4e72e3195ed578ad13582669db26c222845bc"}}, "hash": "efad2e4da3391513f1114298c42008fbd33ee9ebd840e24a38cb4bfd1348b533", "text": "We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions", "start_char_idx": 1199, "end_char_idx": 1818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b21224f-cfc0-4f90-a1c0-8076867d148e": {"__data__": {"id_": "6b21224f-cfc0-4f90-a1c0-8076867d148e", "embedding": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "8187ad14bdcc3b6ccefd779afc604bfe61c0754ee94c9ad74624197993063aee"}, "2": {"node_id": "a10f5859-717a-4f94-a2be-5771da77b16e", "node_type": null, "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}, "hash": "efad2e4da3391513f1114298c42008fbd33ee9ebd840e24a38cb4bfd1348b533"}}, "hash": "af2dd6a12448c57e9bea7adc2fc4e72e3195ed578ad13582669db26c222845bc", "text": "on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.", "start_char_idx": 1820, "end_char_idx": 2120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6ec5bc0c-2754-49b8-b738-fc4edcb04697": {"__data__": {"id_": "6ec5bc0c-2754-49b8-b738-fc4edcb04697", "embedding": null, "metadata": {"title": "Benchmarking Large Language Models for News Summarization", "venue": "arXiv.org", "year": 2023, "paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29", "citationCount": 67, "openAccessPdf": null, "authors": ["Tianyi Zhang", "Faisal Ladhak", "Esin Durmus", "Percy Liang", "K. McKeown", "Tatsunori Hashimoto"], "externalIds": {"DBLP": "journals/corr/abs-2301-13848", "ArXiv": "2301.13848", "DOI": "10.48550/arXiv.2301.13848", "CorpusId": 256416014}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a023287c-0fb4-4b10-a1c4-9a94b7d4ba4f", "node_type": null, "metadata": {"title": "Benchmarking Large Language Models for News Summarization", "venue": "arXiv.org", "year": 2023, "paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29", "citationCount": 67, "openAccessPdf": null, "authors": ["Tianyi Zhang", "Faisal Ladhak", "Esin Durmus", "Percy Liang", "K. McKeown", "Tatsunori Hashimoto"], "externalIds": {"DBLP": "journals/corr/abs-2301-13848", "ArXiv": "2301.13848", "DOI": "10.48550/arXiv.2301.13848", "CorpusId": 256416014}}, "hash": "42beaeaed2cdeb3b2dcc33f06cb1f33f7ddc036d5971eaecee31b80cf1f55e31"}}, "hash": "42beaeaed2cdeb3b2dcc33f06cb1f33f7ddc036d5971eaecee31b80cf1f55e31", "text": "Benchmarking Large Language Models for News Summarization Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, and not model size, is the key to the LLM's zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LMM summaries are judged to be on par with human written summaries.", "start_char_idx": 0, "end_char_idx": 889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b259adb-188d-48a3-98ca-8689991857c2": {"__data__": {"id_": "6b259adb-188d-48a3-98ca-8689991857c2", "embedding": null, "metadata": {"title": "Training language models to follow instructions with human feedback", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "citationCount": 2856, "openAccessPdf": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "J. Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "P. Welinder", "P. Christiano", "J. Leike", "Ryan J. Lowe"], "externalIds": {"DBLP": "journals/corr/abs-2203-02155", "ArXiv": "2203.02155", "DOI": "10.48550/arXiv.2203.02155", "CorpusId": 246426909}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba222a86-8474-4ab0-b55f-65ef9ef4acfb", "node_type": null, "metadata": {"title": "Training language models to follow instructions with human feedback", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "citationCount": 2856, "openAccessPdf": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "J. Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "P. Welinder", "P. Christiano", "J. Leike", "Ryan J. Lowe"], "externalIds": {"DBLP": "journals/corr/abs-2203-02155", "ArXiv": "2203.02155", "DOI": "10.48550/arXiv.2203.02155", "CorpusId": 246426909}}, "hash": "b87aa0090cd01e8746a5f71b154b0e1dfe2f7997336309f1d2cd2f719afbacea"}, "3": {"node_id": "83ca4193-6149-4f16-9344-1f144888073e", "node_type": null, "metadata": {"title": "Training language models to follow instructions with human feedback", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "citationCount": 2856, "openAccessPdf": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "J. Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "P. Welinder", "P. Christiano", "J. Leike", "Ryan J. Lowe"], "externalIds": {"DBLP": "journals/corr/abs-2203-02155", "ArXiv": "2203.02155", "DOI": "10.48550/arXiv.2203.02155", "CorpusId": 246426909}}, "hash": "7952680742b7d5f9a205224db0d4cb4a8072856007a47db78969c34eed532fd2"}}, "hash": "4596d7d975941ee4861fa2c66fb1197a37bdeacef5e283174462de91e1f35621", "text": "Training language models to follow instructions with human feedback Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the", "start_char_idx": 0, "end_char_idx": 1053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "83ca4193-6149-4f16-9344-1f144888073e": {"__data__": {"id_": "83ca4193-6149-4f16-9344-1f144888073e", "embedding": null, "metadata": {"title": "Training language models to follow instructions with human feedback", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "citationCount": 2856, "openAccessPdf": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "J. Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "P. Welinder", "P. Christiano", "J. Leike", "Ryan J. Lowe"], "externalIds": {"DBLP": "journals/corr/abs-2203-02155", "ArXiv": "2203.02155", "DOI": "10.48550/arXiv.2203.02155", "CorpusId": 246426909}}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba222a86-8474-4ab0-b55f-65ef9ef4acfb", "node_type": null, "metadata": {"title": "Training language models to follow instructions with human feedback", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "citationCount": 2856, "openAccessPdf": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "J. Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "P. Welinder", "P. Christiano", "J. Leike", "Ryan J. Lowe"], "externalIds": {"DBLP": "journals/corr/abs-2203-02155", "ArXiv": "2203.02155", "DOI": "10.48550/arXiv.2203.02155", "CorpusId": 246426909}}, "hash": "b87aa0090cd01e8746a5f71b154b0e1dfe2f7997336309f1d2cd2f719afbacea"}, "2": {"node_id": "6b259adb-188d-48a3-98ca-8689991857c2", "node_type": null, "metadata": {"title": "Training language models to follow instructions with human feedback", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "citationCount": 2856, "openAccessPdf": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "J. Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "P. Welinder", "P. Christiano", "J. Leike", "Ryan J. Lowe"], "externalIds": {"DBLP": "journals/corr/abs-2203-02155", "ArXiv": "2203.02155", "DOI": "10.48550/arXiv.2203.02155", "CorpusId": 246426909}}, "hash": "4596d7d975941ee4861fa2c66fb1197a37bdeacef5e283174462de91e1f35621"}}, "hash": "7952680742b7d5f9a205224db0d4cb4a8072856007a47db78969c34eed532fd2", "text": "from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "start_char_idx": 978, "end_char_idx": 1457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"9d4e69ee-65e2-4301-b7b9-a248a086615a": {"node_ids": ["6f3cbbc8-d0b2-47cb-bd1f-d1cc0cff6fb2"], "metadata": {"title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "citationCount": 2015, "openAccessPdf": null, "authors": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "E. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "externalIds": {"DBLP": "conf/nips/Wei0SBIXCLZ22", "ArXiv": "2201.11903", "CorpusId": 246411621}}}, "b6dbe332-1721-40e5-931f-9ddf24e63c8c": {"node_ids": ["e0cbdf8b-55a1-4c36-ae9b-3bb12616f62f", "0be527d9-32a9-4b5c-ae26-d0e0f10b0824"], "metadata": {"title": "Large Language Models are Zero-Shot Reasoners", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "citationCount": 841, "openAccessPdf": null, "authors": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "externalIds": {"DBLP": "journals/corr/abs-2205-11916", "ArXiv": "2205.11916", "CorpusId": 249017743}}}, "5e964383-21ab-4cdd-ab29-7f4c25ef61e0": {"node_ids": ["0a8aefbc-7226-4adc-a9a8-76fdb2621bfd", "fff9973f-f7ce-46c6-87a4-6b2486cc976b", "5de093c7-cd0f-4c81-b60e-082fde23e8a9", "69a84425-eb34-4c70-933d-f272791884bb", "102420fa-1de2-49c5-bec9-e11dbe9754b9"], "metadata": {"title": "Evaluating Large Language Models Trained on Code", "venue": "arXiv.org", "year": 2021, "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "citationCount": 1570, "openAccessPdf": null, "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "S. Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "J. Leike", "Joshua Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "P. Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "externalIds": {"DBLP": "journals/corr/abs-2107-03374", "ArXiv": "2107.03374", "CorpusId": 235755472}}}, "225d23c1-c920-47f7-82a5-b067349b8523": {"node_ids": ["54a5d4ba-ea89-4ea8-8f4b-edae0669a2b9"], "metadata": {"title": "Emergent Abilities of Large Language Models", "venue": "Trans. Mach. Learn. Res.", "year": 2022, "paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "citationCount": 809, "openAccessPdf": null, "authors": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "E. Chi", "Tatsunori Hashimoto", "Oriol Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "externalIds": {"DBLP": "journals/corr/abs-2206-07682", "ArXiv": "2206.07682", "DOI": "10.48550/arXiv.2206.07682", "CorpusId": 249674500}}}, "2a7e6612-65b2-48e3-84f4-09ef65816354": {"node_ids": ["7eb6de56-76ee-4ec1-8ecb-9e18ed8c01e4", "65f57cbe-5504-4bc8-a463-49c5bf40351d"], "metadata": {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "citationCount": 1291, "openAccessPdf": null, "authors": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "externalIds": {"DBLP": "conf/iclr/HuSWALWWC22", "ArXiv": "2106.09685", "CorpusId": 235458009}}}, "d485337f-9833-41ce-8c75-207c5bb68fbb": {"node_ids": ["698ea218-5b80-4962-9760-6a9e446e532c", "dc38a3b7-a08a-4349-8e7b-c5e82af532f8"], "metadata": {"title": "Training Compute-Optimal Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "citationCount": 725, "openAccessPdf": null, "authors": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "Oriol Vinyals", "L. Sifre"], "externalIds": {"ArXiv": "2203.15556", "DBLP": "journals/corr/abs-2203-15556", "DOI": "10.48550/arXiv.2203.15556", "CorpusId": 247778764}}}, "d3893411-0ab4-4320-a1a3-994833771c92": {"node_ids": ["f04ff5eb-f20a-43e3-a5f1-bca6b660d27f"], "metadata": {"title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models", "venue": "medRxiv", "year": 2022, "paperId": "cf1f26e7cbed3958b3c2870656568c299fece6e3", "citationCount": 693, "openAccessPdf": "https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000198&type=printable", "authors": ["Tiffany H. Kung", "Morgan Cheatham", "Arielle Medenilla", "Czarina Sillos", "Lorie De Leon", "Camille Elepa\u00f1o", "Maria Madriaga", "Rimel Aggabao", "Giezel Diaz-Candido", "James Maningo", "Victor Tseng"], "externalIds": {"PubMedCentral": "9931230", "DOI": "10.1371/journal.pdig.0000198", "CorpusId": 254876189, "PubMed": "36812645"}}}, "2b03a5fa-c92c-45fb-beb7-76322ab9b25d": {"node_ids": ["c96cc3bf-4133-4d21-93db-e9cd883a77c0"], "metadata": {"title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "citationCount": 576, "openAccessPdf": null, "authors": ["Junnan Li", "Dongxu Li", "S. Savarese", "Steven C. H. Hoi"], "externalIds": {"DBLP": "conf/icml/0008LSH23", "ArXiv": "2301.12597", "DOI": "10.48550/arXiv.2301.12597", "CorpusId": 256390509}}}, "91419461-dd89-4137-950d-27a807e1e774": {"node_ids": ["39e9db19-cebf-496d-bb7b-35270bde9b97", "195fdb5b-3cc0-4ea7-98b7-78d650698154"], "metadata": {"title": "Program Synthesis with Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "citationCount": 406, "openAccessPdf": null, "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "H. Michalewski", "David Dohan", "Ellen Jiang", "Carrie J. Cai", "Michael Terry", "Quoc V. Le", "Charles Sutton"], "externalIds": {"DBLP": "journals/corr/abs-2108-07732", "ArXiv": "2108.07732", "CorpusId": 237142385}}}, "24f98f71-2d40-48bc-8c24-589f8a059ec0": {"node_ids": ["97b84ea5-6bdd-4026-aeb6-465747f6a59f", "9232ed54-292c-4300-b938-5d8a70849947"], "metadata": {"title": "Automatic Chain of Thought Prompting in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "citationCount": 192, "openAccessPdf": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "externalIds": {"DBLP": "journals/corr/abs-2210-03493", "ArXiv": "2210.03493", "DOI": "10.48550/arXiv.2210.03493", "CorpusId": 252762275}}}, "8a62d240-c08a-4ace-86d0-44301e9ae387": {"node_ids": ["9d5a5c37-6e9b-4a09-bb76-f803eaae1d0c"], "metadata": {"title": "Extracting Training Data from Large Language Models", "venue": "USENIX Security Symposium", "year": 2020, "paperId": "62d1a3137b01a69443bebf4d92c1990ec512a6a1", "citationCount": 770, "openAccessPdf": null, "authors": ["Nicholas Carlini", "Florian Tram\u00e8r", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom B. Brown", "D. Song", "\u00da. Erlingsson", "Alina Oprea", "Colin Raffel"], "externalIds": {"DBLP": "journals/corr/abs-2012-07805", "MAG": "3112689365", "ArXiv": "2012.07805", "CorpusId": 229156229}}}, "98161ef1-9562-44b3-90ce-783ed2dcf5ef": {"node_ids": ["1de38d24-26cf-4ad9-bb1b-3b732a34e878", "44513a67-2ea3-4b6e-8dc1-78120fb2782a"], "metadata": {"title": "A systematic evaluation of large language models of code", "venue": "MAPS@PLDI", "year": 2022, "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "citationCount": 188, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862", "authors": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "V. Hellendoorn"], "externalIds": {"DBLP": "conf/pldi/Xu0NH22", "ArXiv": "2202.13169", "DOI": "10.1145/3520312.3534862", "CorpusId": 247158549}}}, "074db97e-555a-4aec-9721-a23a6b55e16f": {"node_ids": ["7a8f40b2-e0b6-4c4b-a8d0-727daf6c0465"], "metadata": {"title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models", "venue": "International Conference on Machine Learning", "year": 2022, "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "citationCount": 100, "openAccessPdf": null, "authors": ["Guangxuan Xiao", "Ji Lin", "Mickael Seznec", "Julien Demouth", "Song Han"], "externalIds": {"DBLP": "journals/corr/abs-2211-10438", "ArXiv": "2211.10438", "DOI": "10.48550/arXiv.2211.10438", "CorpusId": 253708271}}}, "9f27c421-d5ac-439d-a29a-01a3795f6370": {"node_ids": ["31723c28-0a50-4554-ada8-ffa913cc5e39", "38c5224b-9be4-4e41-8558-039157de37a3"], "metadata": {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "5437e8adab596d7294124c0e798708e050e25321", "citationCount": 358, "openAccessPdf": null, "authors": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "E. Chi"], "externalIds": {"DBLP": "journals/corr/abs-2205-10625", "ArXiv": "2205.10625", "DOI": "10.48550/arXiv.2205.10625", "CorpusId": 248986239}}}, "4d07cbcd-7c48-4a24-a1f8-b3f9e227c6dd": {"node_ids": ["8e8deaab-8a86-44bf-b3d6-2ecf20b28c18"], "metadata": {"title": "Large language models encode clinical knowledge", "venue": "Nature", "year": 2022, "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "citationCount": 277, "openAccessPdf": "https://www.nature.com/articles/s41586-023-06291-2.pdf", "authors": ["K. Singhal", "Shekoofeh Azizi", "T. Tu", "S. Mahdavi", "Jason Wei", "Hyung Won Chung", "Nathan Scales", "A. Tanwani", "H. Cole-Lewis", "S. Pfohl", "P. Payne", "Martin G. Seneviratne", "P. Gamble", "C. Kelly", "Nathaneal Scharli", "Aakanksha Chowdhery", "P. A. Mansfield", "B. A. Y. Arcas", "D. Webster", "Greg S. Corrado", "Y. Matias", "K. Chou", "Juraj Gottweis", "Nenad Toma\u0161ev", "Yun Liu", "A. Rajkomar", "J. Barral", "Christopher Semturs", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"PubMedCentral": "10396962", "DBLP": "journals/corr/abs-2212-13138", "ArXiv": "2212.13138", "DOI": "10.1038/s41586-023-06291-2", "CorpusId": 255124952, "PubMed": "37438534"}}}, "9f3eaf45-cb13-4d46-bc0d-ab2244ed68c0": {"node_ids": ["c6dcc55f-5ece-4543-b00d-7f28f0ccbf53"], "metadata": {"title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd", "citationCount": 133, "openAccessPdf": null, "authors": ["Antonia Creswell", "M. Shanahan", "I. Higgins"], "externalIds": {"DBLP": "journals/corr/abs-2205-09712", "ArXiv": "2205.09712", "DOI": "10.48550/arXiv.2205.09712", "CorpusId": 248887351}}}, "086e27a9-78d9-4d0b-938f-3516774e4760": {"node_ids": ["12256cf7-a7ce-432d-9b90-23d5ebb351a5"], "metadata": {"title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models", "venue": "IEEE International Conference on Robotics and Automation", "year": 2022, "paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "citationCount": 134, "openAccessPdf": "https://arxiv.org/pdf/2209.11302", "authors": ["Ishika Singh", "Valts Blukis", "Arsalan Mousavian", "Ankit Goyal", "Danfei Xu", "Jonathan Tremblay", "D. Fox", "Jesse Thomason", "Animesh Garg"], "externalIds": {"DBLP": "journals/corr/abs-2209-11302", "ArXiv": "2209.11302", "DOI": "10.1109/ICRA48891.2023.10161317", "CorpusId": 252519594}}}, "6e6a9122-135d-4446-9dd5-fc2453b6604d": {"node_ids": ["032e038d-a860-4e32-94fe-80e1db629093"], "metadata": {"title": "Exploring Length Generalization in Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "citationCount": 66, "openAccessPdf": null, "authors": ["Cem Anil", "Yuhuai Wu", "Anders Andreassen", "Aitor Lewkowycz", "Vedant Misra", "V. Ramasesh", "Ambrose Slone", "Guy Gur-Ari", "Ethan Dyer", "Behnam Neyshabur"], "externalIds": {"ArXiv": "2207.04901", "DBLP": "conf/nips/AnilWALMRSGDN22", "DOI": "10.48550/arXiv.2207.04901", "CorpusId": 250425737}}}, "ad15f097-c2c3-47a1-90be-7e6540b2fcc2": {"node_ids": ["8a71ecf5-e9b3-43d5-973b-00530135c89b", "0c39c7c2-5fc3-4982-92d1-1daaf213d51a"], "metadata": {"title": "Large Language Models Are Human-Level Prompt Engineers", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "citationCount": 181, "openAccessPdf": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "externalIds": {"ArXiv": "2211.01910", "DBLP": "conf/iclr/ZhouMHPPCB23", "DOI": "10.48550/arXiv.2211.01910", "CorpusId": 253265328}}}, "3faa6ae6-1c30-4cfe-98c2-cc03acf9a2aa": {"node_ids": ["0eac94a7-dc16-437a-81d1-975297d031c1"], "metadata": {"title": "Expectation vs.\u00a0Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models", "venue": "CHI Extended Abstracts", "year": 2022, "paperId": "4054fc9e8776dc0324cfc215462d606eb75916c0", "citationCount": 134, "openAccessPdf": null, "authors": ["Priyan Vaithilingam", "Tianyi Zhang", "Elena L. Glassman"], "externalIds": {"DBLP": "conf/chi/Vaithilingam0G22", "DOI": "10.1145/3491101.3519665", "CorpusId": 247255943}}}, "3de42693-0990-4d05-95e3-13b8955283c5": {"node_ids": ["df62aff7-b4e1-409c-9a81-e3c497188423"], "metadata": {"title": "Large Language Models Can Self-Improve", "venue": "arXiv.org", "year": 2022, "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd", "citationCount": 133, "openAccessPdf": null, "authors": ["Jiaxin Huang", "S. Gu", "Le Hou", "Yuexin Wu", "Xuezhi Wang", "Hongkun Yu", "Jiawei Han"], "externalIds": {"DBLP": "journals/corr/abs-2210-11610", "ArXiv": "2210.11610", "DOI": "10.48550/arXiv.2210.11610", "CorpusId": 253080328}}}, "654edb2d-bbb8-4c26-94c8-2b85c98a14ae": {"node_ids": ["d107e6e2-4aac-410a-b730-87b570742df9", "691358af-db41-4a08-ac41-9e6031fd6c12"], "metadata": {"title": "Large Language Models Can Be Strong Differentially Private Learners", "venue": "International Conference on Learning Representations", "year": 2021, "paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "citationCount": 159, "openAccessPdf": null, "authors": ["Xuechen Li", "Florian Tram\u00e8r", "Percy Liang", "Tatsunori B. Hashimoto"], "externalIds": {"DBLP": "conf/iclr/LiTLH22", "ArXiv": "2110.05679", "CorpusId": 238634219}}}, "8efdc06f-210f-4381-97ce-5fa91166fc13": {"node_ids": ["8cf2dc98-f5aa-45a6-836a-8267aa6909de", "06369f7d-fb14-4f91-834d-9f296b153f00"], "metadata": {"title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "citationCount": 253, "openAccessPdf": null, "authors": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "externalIds": {"ArXiv": "2304.10592", "DBLP": "journals/corr/abs-2304-10592", "DOI": "10.48550/arXiv.2304.10592", "CorpusId": 258291930}}}, "c579e6f5-a675-4b34-8915-d831cadb6f61": {"node_ids": ["818b3416-c03d-47ed-9483-b5be82d8e179", "1fe355a9-395e-4006-946b-0e7073ce9c96"], "metadata": {"title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)", "venue": "arXiv.org", "year": 2022, "paperId": "6d147d1b7a283252052cda28a98ee6cc6379f932", "citationCount": 77, "openAccessPdf": null, "authors": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "externalIds": {"ArXiv": "2206.10498", "DBLP": "journals/corr/abs-2206-10498", "DOI": "10.48550/arXiv.2206.10498", "CorpusId": 249889477}}}, "af2d4143-fbaa-4af1-8e3a-eddfd114c125": {"node_ids": ["ba252561-ac1e-48a3-a28b-ffca8d1fb2cb"], "metadata": {"title": "Large language models are few-shot clinical information extractors", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "686d9ee744fa013cc21cdd86acd864c936e9e456", "citationCount": 92, "openAccessPdf": "https://aclanthology.org/2022.emnlp-main.130.pdf", "authors": ["Monica Agrawal", "S. Hegselmann", "Hunter Lang", "Yoon Kim", "D. Sontag"], "externalIds": {"ArXiv": "2205.12689", "DBLP": "conf/emnlp/AgrawalHLKS22", "ACL": "2022.emnlp-main.130", "DOI": "10.18653/v1/2022.emnlp-main.130", "CorpusId": 249062918}}}, "e9666ff2-f098-43a5-a5f6-8587f77e9fa6": {"node_ids": ["73b3f92b-56f4-43e3-b69b-24147d2b3c19"], "metadata": {"title": "Towards Reasoning in Large Language Models: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "citationCount": 108, "openAccessPdf": null, "authors": ["Jie Huang", "K. Chang"], "externalIds": {"ArXiv": "2212.10403", "DBLP": "journals/corr/abs-2212-10403", "DOI": "10.48550/arXiv.2212.10403", "CorpusId": 254877753}}}, "6ffd7394-6942-42e7-b01f-ae75612863f6": {"node_ids": ["793ac17d-6bc7-4b4c-bec7-7bca0600ddec"], "metadata": {"title": "Wordcraft: Story Writing With Large Language Models", "venue": "International Conference on Intelligent User Interfaces", "year": 2022, "paperId": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20", "citationCount": 86, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3490099.3511105", "authors": ["Ann Yuan", "Andy Coenen", "Emily Reif", "Daphne Ippolito"], "externalIds": {"DBLP": "conf/iui/YuanCRI22", "DOI": "10.1145/3490099.3511105", "CorpusId": 247585187}}}, "ec2c0581-24a7-4545-9c19-0c22fa48181e": {"node_ids": ["8cc6e23b-27c7-4162-a2f5-93273dbb0cb9"], "metadata": {"title": "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models", "venue": "International Computing Education Research Workshop", "year": 2022, "paperId": "0d08ffccc982781e310bb184397bbe64b9aef157", "citationCount": 89, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3501385.3543957", "authors": ["Sami Sarsa", "Paul Denny", "Arto Hellas", "Juho Leinonen"], "externalIds": {"DBLP": "journals/corr/abs-2206-11861", "ArXiv": "2206.11861", "DOI": "10.1145/3501385.3543957", "CorpusId": 249954011}}}, "4f12021f-c7b5-4d7e-8d43-658752481687": {"node_ids": ["3d4ab8cf-d1c2-4058-b695-cd54ee4cbdfc"], "metadata": {"title": "Faithful Reasoning Using Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "f0a0e8b6e84207f50db4d24cc4016e40601214ef", "citationCount": 64, "openAccessPdf": null, "authors": ["Antonia Creswell", "M. Shanahan"], "externalIds": {"ArXiv": "2208.14271", "DBLP": "journals/corr/abs-2208-14271", "DOI": "10.48550/arXiv.2208.14271", "CorpusId": 251929296}}}, "1f4d4c07-16a2-48ed-8087-208a41dcee02": {"node_ids": ["a1e8d600-2713-44df-89fa-8eeb4d807fd6", "6401cbba-3834-4075-8fea-74a0fd56f11d"], "metadata": {"title": "Can large language models reason about medical questions?", "venue": "arXiv.org", "year": 2022, "paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "citationCount": 82, "openAccessPdf": null, "authors": ["Valentin Li'evin", "C. Hother", "O. Winther"], "externalIds": {"DBLP": "journals/corr/abs-2207-08143", "ArXiv": "2207.08143", "DOI": "10.48550/arXiv.2207.08143", "CorpusId": 250627547}}}, "e8a4883c-2c6d-4ceb-a728-e0a9f4e3f4d6": {"node_ids": ["5d4630c4-4b9a-45c8-a9a0-525d5e2ec4f7"], "metadata": {"title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "8b293973061026d9d0eed90e71e30928e029171e", "citationCount": 67, "openAccessPdf": null, "authors": ["Kushal Tirumala", "Aram H. Markosyan", "Luke Zettlemoyer", "Armen Aghajanyan"], "externalIds": {"DBLP": "journals/corr/abs-2205-10770", "ArXiv": "2205.10770", "DOI": "10.48550/arXiv.2205.10770", "CorpusId": 248986465}}}, "1eef0a61-42eb-420b-90c1-1877dda8c746": {"node_ids": ["623d4355-e3ff-4a3e-ae35-dde9d422de86", "33242c4d-5a7b-4b72-80f1-a158dbd38e39"], "metadata": {"title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e", "citationCount": 59, "openAccessPdf": null, "authors": ["Archiki Prasad", "Peter Hase", "Xiang Zhou", "Mohit Bansal"], "externalIds": {"ArXiv": "2203.07281", "ACL": "2023.eacl-main.277", "DBLP": "conf/eacl/PrasadHZB23", "DOI": "10.48550/arXiv.2203.07281", "CorpusId": 247447170}}}, "0f3300ca-b6fc-42ab-8617-078c5ce3566c": {"node_ids": ["0518ab85-5af8-4f0c-b153-e0d076829f6e"], "metadata": {"title": "Emergent analogical reasoning in large language models", "venue": "Nature Human Behaviour", "year": 2022, "paperId": "3cbffab9d7981da6662d474aaa056dcbd3c1701e", "citationCount": 57, "openAccessPdf": "https://arxiv.org/pdf/2212.09196", "authors": ["Taylor W. Webb", "K. Holyoak", "Hongjing Lu"], "externalIds": {"DBLP": "journals/corr/abs-2212-09196", "ArXiv": "2212.09196", "DOI": "10.1038/s41562-023-01659-w", "CorpusId": 254854575, "PubMed": "37524930"}}}, "d2dfc7f1-a1a2-4b88-8ae7-75aebeaeb47d": {"node_ids": ["2ef9155a-0808-4444-b2a5-d750a2715654"], "metadata": {"title": "Autoformalization with Large Language Models", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5", "citationCount": 56, "openAccessPdf": null, "authors": ["Yuhuai Wu", "Albert Qiaochu Jiang", "Wenda Li", "Markus N. Rabe", "Charles Staats", "M. Jamnik", "Christian Szegedy"], "externalIds": {"DBLP": "journals/corr/abs-2205-12615", "ArXiv": "2205.12615", "DOI": "10.48550/arXiv.2205.12615", "CorpusId": 249063032}}}, "156aa766-fabe-4af1-84b9-1c6f379aa6da": {"node_ids": ["b8a6ff02-102b-4c71-8503-9d8858564732"], "metadata": {"title": "Talking About Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "3eed4de25636ac90f39f6e1ef70e3507ed61a2a6", "citationCount": 54, "openAccessPdf": null, "authors": ["M. Shanahan"], "externalIds": {"DBLP": "journals/corr/abs-2212-03551", "ArXiv": "2212.03551", "DOI": "10.48550/arXiv.2212.03551", "CorpusId": 254366666}}}, "86caca98-8de3-4988-99ae-4be11e028419": {"node_ids": ["52409e45-5230-4e5e-9999-509af585a51a"], "metadata": {"title": "Compositional Semantic Parsing with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "40047a74b707743157051d38f76061ba5ff9aab4", "citationCount": 53, "openAccessPdf": null, "authors": ["Andrew Drozdov", "Nathanael Scharli", "Ekin Akyuurek", "Nathan Scales", "Xinying Song", "Xinyun Chen", "O. Bousquet", "Denny Zhou"], "externalIds": {"ArXiv": "2209.15003", "DBLP": "journals/corr/abs-2209-15003", "CorpusId": 252596001}}}, "1e078f33-1bcf-4262-8754-6e344d200c90": {"node_ids": ["f39152ab-9a48-4bca-98d9-215297cfb494"], "metadata": {"title": "Large Language Models Struggle to Learn Long-Tail Knowledge", "venue": "International Conference on Machine Learning", "year": 2022, "paperId": "75f7e9e2b59fb640ef9d1dff94097175daf46c4d", "citationCount": 49, "openAccessPdf": null, "authors": ["Nikhil Kandpal", "H. Deng", "Adam Roberts", "Eric Wallace", "Colin Raffel"], "externalIds": {"ArXiv": "2211.08411", "DBLP": "journals/corr/abs-2211-08411", "DOI": "10.48550/arXiv.2211.08411", "CorpusId": 253522998}}}, "4b3f6b2c-a193-4d8a-93e0-371072339959": {"node_ids": ["be061a19-abb3-4887-a7c5-fcc96fd623ab", "67800253-f967-4692-bba9-64867bcbb3e7"], "metadata": {"title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412", "citationCount": 85, "openAccessPdf": null, "authors": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "externalIds": {"DBLP": "conf/iclr/0002IWXJ000023", "ArXiv": "2209.10063", "DOI": "10.48550/arXiv.2209.10063", "CorpusId": 252408513}}}, "4dad8280-c405-4ef5-b229-6d47ff7480af": {"node_ids": ["e1b070a8-3dd3-4f0c-8cca-597592d1b1df"], "metadata": {"title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models", "venue": "International Conference on Artificial Intelligence and Statistics", "year": 2022, "paperId": "9dcee248452d84b6bf26911ba6726ae5ce1a46f3", "citationCount": 39, "openAccessPdf": null, "authors": ["S. Hegselmann", "Alejandro Buendia", "Hunter Lang", "Monica Agrawal", "Xiaoyi Jiang", "D. Sontag"], "externalIds": {"DBLP": "journals/corr/abs-2210-10723", "ArXiv": "2210.10723", "DOI": "10.48550/arXiv.2210.10723", "CorpusId": 252992811}}}, "3b4da62b-3fd8-44c7-9c88-d589ff00f28c": {"node_ids": ["98dc8d84-7e4c-46bf-a3cf-e55e51ef8907"], "metadata": {"title": "Evaluating Large Language Models", "venue": "", "year": 2022, "paperId": "4b2137280915ccc0e06e97b604778b05876a34ad", "citationCount": 95, "openAccessPdf": null, "authors": [], "externalIds": {"CorpusId": 247456179}}}, "72cd9b3f-a353-4133-b1e5-fe98326c95d8": {"node_ids": ["b059fe23-a328-4005-a04b-371267840166"], "metadata": {"title": "Large Language Models Are Reasoning Teachers", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44", "citationCount": 64, "openAccessPdf": null, "authors": ["Namgyu Ho", "Laura Schmid", "Se-Young Yun"], "externalIds": {"ACL": "2023.acl-long.830", "DBLP": "journals/corr/abs-2212-10071", "ArXiv": "2212.10071", "DOI": "10.48550/arXiv.2212.10071", "CorpusId": 254877399}}}, "9cf0347a-2685-4c0f-b46e-51517d2b82a5": {"node_ids": ["e214ebbe-a4a2-4577-a1fa-6529171c905d"], "metadata": {"title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "8ee45aeb7c97e3346cc62f216f673b91277ac718", "citationCount": 43, "openAccessPdf": null, "authors": ["Chan Hee Song", "Jiaman Wu", "Clay Washington", "Brian M. Sadler", "Wei-Lun Chao", "Yu Su"], "externalIds": {"ArXiv": "2212.04088", "DBLP": "journals/corr/abs-2212-04088", "DOI": "10.48550/arXiv.2212.04088", "CorpusId": 254408960}}}, "971c9ea2-0ff3-4d54-8971-a2c3dbe03caa": {"node_ids": ["4e335f4c-1af9-48fe-95da-4351af74a1aa"], "metadata": {"title": "Explanations from Large Language Models Make Small Reasoners Better", "venue": "arXiv.org", "year": 2022, "paperId": "7d29a84a589aa5655e5d3fed8d725ea472816599", "citationCount": 41, "openAccessPdf": null, "authors": ["SHIYANG LI", "Jianshu Chen", "Yelong Shen", "Zhiyu Chen", "Xinlu Zhang", "Zekun Li", "Hong Wang", "Jingu Qian", "Baolin Peng", "Yi Mao", "Wenhu Chen", "Xifeng Yan"], "externalIds": {"ArXiv": "2210.06726", "DBLP": "journals/corr/abs-2210-06726", "DOI": "10.48550/arXiv.2210.06726", "CorpusId": 252873123}}}, "cdf9b041-fd18-49af-9797-61ea9bc8af67": {"node_ids": ["fb7fb34c-8b57-421b-99b4-09017b849864"], "metadata": {"title": "Visual Classification via Description from Large Language Models", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "a42b091adaf29b06a092b67192ac07cb93312f2a", "citationCount": 58, "openAccessPdf": null, "authors": ["Sachit Menon", "Carl Vondrick"], "externalIds": {"DBLP": "conf/iclr/MenonV23", "ArXiv": "2210.07183", "DOI": "10.48550/arXiv.2210.07183", "CorpusId": 252872997}}}, "226c7a90-ff29-42bd-8d04-9f3bd74d1ea4": {"node_ids": ["3300d303-99a4-4525-a60d-d1752aeebd7a"], "metadata": {"title": "PromptMaker: Prompt-based Prototyping with Large\u00a0Language\u00a0Models", "venue": "CHI Extended Abstracts", "year": 2022, "paperId": "18a648a2c2d873602708a0a7f96be3ec716f6b1a", "citationCount": 45, "openAccessPdf": null, "authors": ["Ellen Jiang", "Kristen Olson", "Edwin Toh", "A. Molina", "Aaron Donsbach", "Michael Terry", "Carrie J. Cai"], "externalIds": {"DBLP": "conf/chi/JiangOTMDTC22", "DOI": "10.1145/3491101.3503564", "CorpusId": 248419856}}}, "a5eab4f4-87a2-4286-ad4b-eca9b1cc6ab5": {"node_ids": ["95bd1c65-a23b-4c37-b038-e94823a5f0a4", "836e5ae1-10df-4b38-958e-4f88496998f1", "4e80bd3b-2ecd-46cd-838a-292278252d37"], "metadata": {"title": "Language Models are Few-Shot Learners", "venue": "Neural Information Processing Systems", "year": 2020, "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "citationCount": 16056, "openAccessPdf": null, "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "Rewon Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "S. Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "externalIds": {"ArXiv": "2005.14165", "DBLP": "journals/corr/abs-2005-14165", "MAG": "3030163527", "CorpusId": 218971783}}}, "bcc2ee4a-a361-4d0e-90f6-481b167db6d7": {"node_ids": ["42988a74-efa9-4291-bde2-f2e52df2387f"], "metadata": {"title": "The debate over understanding in AI\u2019s large language models", "venue": "Proceedings of the National Academy of Sciences of the United States of America", "year": 2022, "paperId": "e32185936ab3b23f39b1dd93e1507e6d80a71776", "citationCount": 46, "openAccessPdf": null, "authors": ["M. Mitchell", "D. Krakauer"], "externalIds": {"PubMedCentral": "10068812", "ArXiv": "2210.13966", "DBLP": "journals/corr/abs-2210-13966", "DOI": "10.1073/pnas.2215907120", "CorpusId": 253107905, "PubMed": "36943882"}}}, "0c87d542-9d83-4107-9294-e3f7606fc9ab": {"node_ids": ["fe365372-01e6-489b-a56f-7dc9bc921877"], "metadata": {"title": "Evaluating the Text-to-SQL Capabilities of Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "51000d9f79be0eefd7972fe94e3c71dddc90d2c6", "citationCount": 54, "openAccessPdf": null, "authors": ["Nitarshan Rajkumar", "Raymond Li", "Dzmitry Bahdanau"], "externalIds": {"DBLP": "journals/corr/abs-2204-00498", "ArXiv": "2204.00498", "DOI": "10.48550/arXiv.2204.00498", "CorpusId": 247922681}}}, "9612f788-1e71-4259-9bb3-8cbfffe0238e": {"node_ids": ["3bdd2053-dd18-429e-afb5-8da4ceec0311"], "metadata": {"title": "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book", "venue": "Technical Symposium on Computer Science Education", "year": 2022, "paperId": "2abed82162c47a0cc32cd62afcf46b0745541017", "citationCount": 40, "openAccessPdf": "https://arxiv.org/pdf/2211.02265", "authors": ["S. MacNeil", "Andrew Tran", "Arto Hellas", "Joanne Kim", "Sami Sarsa", "Paul Denny", "Seth Bernstein", "Juho Leinonen"], "externalIds": {"DBLP": "journals/corr/abs-2211-02265", "ArXiv": "2211.02265", "DOI": "10.1145/3545945.3569785", "CorpusId": 253370333}}}, "a28b6264-bab4-42a5-be83-1ca8a5d9dda7": {"node_ids": ["89636cfc-5d09-437b-9b35-c8f89bbd3b4b"], "metadata": {"title": "Meaning without reference in large language models", "venue": "arXiv.org", "year": 2022, "paperId": "50296a5814c4ac7f58f3b0177233a8f63c701565", "citationCount": 43, "openAccessPdf": null, "authors": ["S. Piantadosi", "Felix Hill"], "externalIds": {"ArXiv": "2208.02957", "DBLP": "journals/corr/abs-2208-02957", "DOI": "10.48550/arXiv.2208.02957", "CorpusId": 251371595}}}, "df9c9841-0cb0-417c-8738-1a527502e9f6": {"node_ids": ["d53ace1a-ac06-4d66-9474-ffc7a89f4aca"], "metadata": {"title": "Leveraging Large Language Models for Multiple Choice Question Answering", "venue": "International Conference on Learning Representations", "year": 2022, "paperId": "ed38c6b157c11476939c426ec6871c926f2f3524", "citationCount": 34, "openAccessPdf": null, "authors": ["Joshua Robinson", "Christopher Rytting", "D. Wingate"], "externalIds": {"ArXiv": "2210.12353", "DBLP": "conf/iclr/RobinsonW23", "DOI": "10.48550/arXiv.2210.12353", "CorpusId": 253098700}}}, "9ade8cfe-c14b-4766-b7b9-7da722b55530": {"node_ids": ["645024ce-9b96-45ff-b5c9-3c509c1e32cf"], "metadata": {"title": "Large Language Models are reasoners with Self-Verification", "venue": "arXiv.org", "year": 2022, "paperId": "f02e8f1c9b5ab12ddfb1977570f9f5445a99a973", "citationCount": 31, "openAccessPdf": null, "authors": ["Yixuan Weng", "Minjun Zhu", "Bin Li", "Shizhu He", "Kang Liu", "Jun Zhao"], "externalIds": {"DBLP": "journals/corr/abs-2212-09561", "DOI": "10.48550/arXiv.2212.09561", "CorpusId": 254854206}}}, "0a2f9da6-ebbf-47d0-9f32-21325ea11075": {"node_ids": ["4810a3a5-ffb3-4f38-ab8e-9a45a5df5d7e"], "metadata": {"title": "Enabling Conversational Interaction with Mobile UI using Large Language Models", "venue": "International Conference on Human Factors in Computing Systems", "year": 2022, "paperId": "99070fb6df9e8d11e30f7aaefcc9f0b0c5a73789", "citationCount": 29, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3544548.3580895", "authors": ["Bryan Wang", "Gang Li", "Yang Li"], "externalIds": {"ArXiv": "2209.08655", "DBLP": "journals/corr/abs-2209-08655", "DOI": "10.1145/3544548.3580895", "CorpusId": 252367445}}}, "4c0fc807-c715-426f-9647-b320bdeda630": {"node_ids": ["d4eb84e9-227d-4603-9d86-56ffbde4e943", "0e858d36-dd07-4ecd-9906-9fde5dd3c8d2"], "metadata": {"title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "citationCount": 53, "openAccessPdf": null, "authors": ["Eldar Kurtic", "Daniel Fernando Campos", "Tuan Nguyen", "Elias Frantar", "Mark Kurtz", "Ben Fineran", "M. Goin", "Dan Alistarh"], "externalIds": {"ACL": "2022.emnlp-main.279", "DBLP": "conf/emnlp/KurticCNFKFGA22", "ArXiv": "2203.07259", "DOI": "10.48550/arXiv.2203.07259", "CorpusId": 247446572}}}, "24deb67b-4795-4a2a-9203-89b7fefd583e": {"node_ids": ["0501062b-74f1-4602-9fd4-c970c0cff831"], "metadata": {"title": "SKILL: Structured Knowledge Infusion for Large Language Models", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2022, "paperId": "eea2129457fcd78c4071a9020355a2fe1da4d2fd", "citationCount": 27, "openAccessPdf": "https://aclanthology.org/2022.naacl-main.113.pdf", "authors": ["Fedor Moiseev", "Zhe Dong", "Enrique Alfonseca", "Martin Jaggi"], "externalIds": {"DBLP": "journals/corr/abs-2205-08184", "ArXiv": "2205.08184", "ACL": "2022.naacl-main.113", "DOI": "10.18653/v1/2022.naacl-main.113", "CorpusId": 248834551}}}, "dd9ab116-ba7f-4b46-b567-115a805d403d": {"node_ids": ["5382996a-1111-4639-a45a-20d037308d79", "f7fdca32-024d-436c-ab9d-87a0c8f56200"], "metadata": {"title": "Prompting Is Programming: A Query Language for Large Language Models", "venue": "Proc. ACM Program. Lang.", "year": 2022, "paperId": "c2329c685f11efa25c562f97be71ff03103423fd", "citationCount": 25, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3591300", "authors": ["Luca Beurer-Kellner", "Marc Fischer", "Martin T. Vechev"], "externalIds": {"DBLP": "journals/corr/abs-2212-06094", "ArXiv": "2212.06094", "DOI": "10.1145/3591300", "CorpusId": 254564450}}}, "900f35db-fc9d-4fd1-bf21-8f466e18d253": {"node_ids": ["cf0a99d4-e2a4-4989-8f57-b3cf3d48286b"], "metadata": {"title": "Learning Video Representations from Large Language Models", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "paperId": "933b37b21e9d61139660088adb032ff3fdf56d86", "citationCount": 23, "openAccessPdf": "https://arxiv.org/pdf/2212.04501", "authors": ["Yue Zhao", "Ishan Misra", "Philipp Krahenbuhl", "Rohit Girdhar"], "externalIds": {"ArXiv": "2212.04501", "DBLP": "conf/cvpr/0006MKG23", "DOI": "10.1109/CVPR52729.2023.00637", "CorpusId": 254408789}}}, "33555ccf-3b2c-4463-a1ba-dbf354d2b7d5": {"node_ids": ["0aec158f-2af8-4b02-8188-33bb287bba3e", "ae9138c8-f473-4bc8-aa3d-0b84845c0bd5"], "metadata": {"title": "Understanding HTML with Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "6dd4743ee5430157c981a8dfe9a7434d99be2e8b", "citationCount": 17, "openAccessPdf": null, "authors": ["Izzeddin Gur", "Ofir Nachum", "Yingjie Miao", "Mustafa Safdari", "Austin Huang", "Aakanksha Chowdhery", "Sharan Narang", "Noah Fiedel", "Aleksandra Faust"], "externalIds": {"DBLP": "journals/corr/abs-2210-03945", "ArXiv": "2210.03945", "DOI": "10.48550/arXiv.2210.03945", "CorpusId": 252780086}}}, "15740998-e241-46ad-a84e-948b02bf809b": {"node_ids": ["de84f76a-2bfb-4f8c-a018-25270138caaa"], "metadata": {"title": "Using Large Language Models to Enhance Programming Error Messages", "venue": "Technical Symposium on Computer Science Education", "year": 2022, "paperId": "668faca09fcefd18a46ab5ce4eab765c065e1d5e", "citationCount": 36, "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3545945.3569770", "authors": ["Juho Leinonen", "Arto Hellas", "Sami Sarsa", "B. Reeves", "Paul Denny", "J. Prather", "Brett A. Becker"], "externalIds": {"ArXiv": "2210.11630", "DBLP": "journals/corr/abs-2210-11630", "DOI": "10.1145/3545945.3569770", "CorpusId": 253080791}}}, "1162a76f-1d11-4771-8655-e8c108f4cf1c": {"node_ids": ["094946a2-d061-41ee-ab11-a29b2e6c2f68"], "metadata": {"title": "Capturing Failures of Large Language Models via Human Cognitive Biases", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "76f023c3a819fc58989a064a1b50825b11fce95d", "citationCount": 35, "openAccessPdf": null, "authors": ["Erik Jones", "J. Steinhardt"], "externalIds": {"ArXiv": "2202.12299", "DBLP": "journals/corr/abs-2202-12299", "CorpusId": 247084098}}}, "2d698339-0250-40db-b79d-28caddeab50d": {"node_ids": ["8780dc0c-b52d-483f-bb05-a37c44ab8431", "4e26d892-407e-4cad-b511-79efd956a7f5"], "metadata": {"title": "A Survey of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "1d29334cfbe9a1a943082058876f0c22d44c62fd", "citationCount": 443, "openAccessPdf": null, "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "externalIds": {"DBLP": "journals/corr/abs-2303-18223", "ArXiv": "2303.18223", "DOI": "10.48550/arXiv.2303.18223", "CorpusId": 257900969}}}, "a595946b-5120-4796-a7db-4ec3b76aece2": {"node_ids": ["901c1fca-489b-4f9a-b0aa-c8ac24b67a31"], "metadata": {"title": "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "05d77715d49714506a920f26c5432b92078cd37c", "citationCount": 29, "openAccessPdf": null, "authors": ["Bernd Bohnet", "Vinh Q. Tran", "Pat Verga", "Roee Aharoni", "D. Andor", "Livio Baldini Soares", "Jacob Eisenstein", "Kuzman Ganchev", "Jonathan Herzig", "Kai Hui", "T. Kwiatkowski", "Ji Ma", "Jianmo Ni", "Tal Schuster", "William W. Cohen", "Michael Collins", "Dipanjan Das", "Donald Metzler", "Slav Petrov", "Kellie Webster"], "externalIds": {"DBLP": "journals/corr/abs-2212-08037", "ArXiv": "2212.08037", "DOI": "10.48550/arXiv.2212.08037", "CorpusId": 254685584}}}, "59318c71-e193-4755-b4c9-a266d0c7c5a8": {"node_ids": ["e9a94b93-dc0d-4a09-9e18-fe7888463686", "520d449b-f0a4-409c-9a1b-9153f4148ab6"], "metadata": {"title": "Automated Repair of Programs from Large Language Models", "venue": "International Conference on Software Engineering", "year": 2022, "paperId": "0bcd59da541fdae66884afba8d25475a54a9da1a", "citationCount": 28, "openAccessPdf": "https://arxiv.org/pdf/2205.10583", "authors": ["Zhiyu Fan", "Xiang Gao", "M. Mirchev", "Abhik Roychoudhury", "Shin Hwei Tan"], "externalIds": {"ArXiv": "2205.10583", "DBLP": "conf/icse/FanGMRT23", "DOI": "10.1109/ICSE48619.2023.00128", "CorpusId": 255372224}}}, "5ddb59b9-8a4c-4a1a-8ffb-9834a8a136d4": {"node_ids": ["2d4380c8-a715-4864-ab4c-d5527b17ed6e", "0555ff75-23f0-4fa0-b4cc-d44c5b7d14b9"], "metadata": {"title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models", "venue": "International Symposium on Software Testing and Analysis", "year": 2022, "paperId": "66476832701361c9f9b2a7eb2354ee8cd9f72e67", "citationCount": 27, "openAccessPdf": "https://arxiv.org/pdf/2212.14834", "authors": ["Yinlin Deng", "Chun Xia", "Haoran Peng", "Chenyuan Yang", "Lingming Zhang"], "externalIds": {"DBLP": "conf/issta/DengXPY023", "ArXiv": "2212.14834", "DOI": "10.1145/3597926.3598067", "CorpusId": 255340904}}}, "c0a4f260-a296-4bbd-bd6d-4e4df8b3bee0": {"node_ids": ["108dee8e-0588-4e37-8379-1b1ac801fdfa", "9083decc-bb9a-4c29-bba7-0d11ead008f6"], "metadata": {"title": "Large Language Models and the Reverse Turing Test", "venue": "Neural Computation", "year": 2022, "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d", "citationCount": 28, "openAccessPdf": "https://direct.mit.edu/neco/article-pdf/35/3/309/2071839/neco_a_01563.pdf", "authors": ["T. Sejnowski"], "externalIds": {"ArXiv": "2207.14382", "DBLP": "journals/neco/Sejnowski23", "DOI": "10.1162/neco_a_01563", "CorpusId": 251196636, "PubMed": "36746144"}}}, "a7991df4-901e-4302-bb9d-11fdadb86650": {"node_ids": ["0d10250a-8c80-4a6f-bf9a-0a0dc4cfc7c3"], "metadata": {"title": "Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models", "venue": "Minds and Machines", "year": 2022, "paperId": "d072b46a0504ac023d5035d8ec0c7876151245c4", "citationCount": 28, "openAccessPdf": "https://link.springer.com/content/pdf/10.1007/s11023-022-09602-0.pdf", "authors": ["Adam Sobieszek", "Tadeusz Price"], "externalIds": {"DBLP": "journals/mima/SobieszekP22", "DOI": "10.1007/s11023-022-09602-0", "CorpusId": 248634256}}}, "e74b3d2e-2beb-40fe-9ef0-3fc75e9daaa0": {"node_ids": ["d86640df-d07c-423c-ae35-8ebf441d4cd2"], "metadata": {"title": "Do Large Language Models Understand Us?", "venue": "Daedalus", "year": 2022, "paperId": "39a45eba627199ee12c168dffcead45e138e9a01", "citationCount": 25, "openAccessPdf": "https://direct.mit.edu/daed/article-pdf/151/2/183/2060575/daed_a_01909.pdf", "authors": ["Blaise Ag\u00fcera y Arcas"], "externalIds": {"DOI": "10.1162/daed_a_01909", "CorpusId": 248377874}}}, "cb6a87f4-e6f2-45e0-82d2-2066ec7c2771": {"node_ids": ["be66680d-0829-4aee-948f-cd0c54c959d5"], "metadata": {"title": "Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey", "venue": "arXiv.org", "year": 2022, "paperId": "475c3014a68d545f1d2319f94fd3ab99fc3f6bec", "citationCount": 18, "openAccessPdf": null, "authors": ["Mengnan Du", "Fengxiang He", "Na Zou", "Dacheng Tao", "Xia Hu"], "externalIds": {"DBLP": "journals/corr/abs-2208-11857", "ArXiv": "2208.11857", "DOI": "10.48550/arXiv.2208.11857", "CorpusId": 251800110}}}, "abbb03a1-5cbe-423e-8b58-9374443c1584": {"node_ids": ["80cd099c-ed94-4564-ae7d-6a2987abf609", "8e5ddb88-7d7f-455a-a1ef-81ff31069171"], "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "paperId": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "citationCount": 17, "openAccessPdf": "https://aclanthology.org/2023.acl-long.411.pdf", "authors": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "externalIds": {"ACL": "2023.acl-long.411", "ArXiv": "2212.09420", "DBLP": "conf/acl/ZanCZLWGWL23", "DOI": "10.18653/v1/2023.acl-long.411", "CorpusId": 258557362}}}, "549c635a-fff8-47fa-8d18-b71cb0b716c9": {"node_ids": ["ac4c69b5-04e8-45de-a61b-f07ea0b89e1c"], "metadata": {"title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm", "venue": "CHI Extended Abstracts", "year": 2021, "paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "citationCount": 305, "openAccessPdf": "https://arxiv.org/pdf/2102.07350", "authors": ["Laria Reynolds", "Kyle McDonell"], "externalIds": {"DBLP": "journals/corr/abs-2102-07350", "ArXiv": "2102.07350", "DOI": "10.1145/3411763.3451760", "CorpusId": 231925131}}}, "3809e1ba-81f3-4c4b-8903-9447067af8ff": {"node_ids": ["8be8af7a-2e77-4b7d-bdd8-0ba9e9712dba"], "metadata": {"title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models", "venue": "arXiv.org", "year": 2021, "paperId": "f577654d9dd29d88c6db9ee39a4fd831573b8770", "citationCount": 108, "openAccessPdf": null, "authors": ["Alex Tamkin", "Miles Brundage", "Jack Clark", "Deep Ganguli"], "externalIds": {"ArXiv": "2102.02503", "DBLP": "journals/corr/abs-2102-02503", "CorpusId": 231802467}}}, "1c132b5b-24ec-47c6-a405-10017b6c0274": {"node_ids": ["23ea0f97-2a4b-4130-9df7-6a01b247612b"], "metadata": {"title": "A Recipe for Arbitrary Text Style Transfer with Large Language Models", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "paperId": "7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd", "citationCount": 75, "openAccessPdf": "https://aclanthology.org/2022.acl-short.94.pdf", "authors": ["Emily Reif", "Daphne Ippolito", "Ann Yuan", "Andy Coenen", "Chris Callison-Burch", "Jason Wei"], "externalIds": {"ACL": "2022.acl-short.94", "DBLP": "journals/corr/abs-2109-03910", "ArXiv": "2109.03910", "DOI": "10.18653/v1/2022.acl-short.94", "CorpusId": 237263305}}}, "8c1c9973-9d55-471b-9863-a6cea419fef8": {"node_ids": ["2711496c-7b6c-401d-8066-4c0f0f0630ec"], "metadata": {"title": "Jigsaw: Large Language Models meet Program Synthesis", "venue": "International Conference on Software Engineering", "year": 2021, "paperId": "d095f9ffcb5905bf0858ad1769d3d90e2e8737e2", "citationCount": 79, "openAccessPdf": null, "authors": ["Naman Jain", "Skanda Vaidyanath", "Arun Shankar Iyer", "Nagarajan Natarajan", "Suresh Parthasarathy", "S. Rajamani", "Rahul Sharma"], "externalIds": {"DBLP": "conf/icse/JainVINPR022", "ArXiv": "2112.02969", "DOI": "10.1145/3510003.3510203", "CorpusId": 244908632}}}, "4e6b2eff-c2f6-47f0-8787-b27fcb059eb9": {"node_ids": ["f4cb3e47-1364-40ed-8dc2-bbe11b000d79", "5cd979f0-c9af-43ba-a3c6-c30cb89e6a19", "3e6f12d6-3cc1-4de8-afed-1404ff93e418", "0f8026a5-5204-4eb9-9ff4-46e8870ca444", "8c1374ab-fec3-4d6c-9f9a-2aa55e724243"], "metadata": {"title": "Measuring Progress on Scalable Oversight for Large Language Models", "venue": "arXiv.org", "year": 2022, "paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "citationCount": 27, "openAccessPdf": null, "authors": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "C. Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeff Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova DasSarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "S. Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "externalIds": {"ArXiv": "2211.03540", "DBLP": "journals/corr/abs-2211-03540", "DOI": "10.48550/arXiv.2211.03540", "CorpusId": 253384413}}}, "cd11a346-f5e8-4319-b698-af78eb861136": {"node_ids": ["6adbd397-4b40-4df8-b866-f3c46272092a"], "metadata": {"title": "Persistent Anti-Muslim Bias in Large Language Models", "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "year": 2021, "paperId": "4c2733d191e347753bb28afa46a1c55c65e085be", "citationCount": 214, "openAccessPdf": "https://arxiv.org/pdf/2101.05783", "authors": ["Abubakar Abid", "Maheen Farooqi", "James Y. Zou"], "externalIds": {"DBLP": "conf/aies/AbidF021", "ArXiv": "2101.05783", "DOI": "10.1145/3461702.3462624", "CorpusId": 231603388}}}, "0130dfa3-a254-4f8f-adfa-d021ec30798c": {"node_ids": ["bd735b8d-8ddc-470b-bacd-6a6630252f05"], "metadata": {"title": "Large language models associate Muslims with violence", "venue": "Nature Machine Intelligence", "year": 2021, "paperId": "4eda2b9eaef3ae892382acc21593eed6f56f2ea1", "citationCount": 74, "openAccessPdf": null, "authors": ["Abubakar Abid", "Maheen Farooqi", "James Zou"], "externalIds": {"MAG": "3170344956", "DBLP": "journals/natmi/AbidFZ21", "DOI": "10.1038/s42256-021-00359-2", "CorpusId": 236384212}}}, "dfb1163f-e732-40ed-9253-f227f0dff022": {"node_ids": ["001ccbe4-a7ba-4511-a35f-0aeadae961a5"], "metadata": {"title": "Structured Pruning of Large Language Models", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2019, "paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "citationCount": 155, "openAccessPdf": "https://arxiv.org/pdf/1910.04732", "authors": ["Ziheng Wang", "Jeremy Wohlwend", "Tao Lei"], "externalIds": {"MAG": "2979691890", "ArXiv": "1910.04732", "ACL": "2020.emnlp-main.496", "DBLP": "journals/corr/abs-1910-04732", "DOI": "10.18653/v1/2020.emnlp-main.496", "CorpusId": 204009154}}}, "6925cf83-6ee8-4cc9-96cb-b69c650af276": {"node_ids": ["1c70b554-a085-491d-a67b-14ef84bd4425"], "metadata": {"title": "Examining Zero-Shot Vulnerability Repair with Large Language Models", "venue": "IEEE Symposium on Security and Privacy", "year": 2021, "paperId": "a5731122200fbb8b37f048010a1e1ca4474aa606", "citationCount": 53, "openAccessPdf": "https://arxiv.org/pdf/2112.02125", "authors": ["H. Pearce", "Benjamin Tan", "Baleegh Ahmad", "R. Karri", "Brendan Dolan-Gavitt"], "externalIds": {"ArXiv": "2112.02125", "DBLP": "conf/sp/PearceTAKD23", "DOI": "10.1109/SP46215.2023.10179324", "CorpusId": 251563966}}}, "a36974b6-77f6-4c00-b9af-e56ea621bfb3": {"node_ids": ["8ba77af6-6aff-4b64-9206-8895a271a929", "b37a6fc9-c135-44b1-8c3c-df0bac34ad8b", "342e0ad6-4569-49c5-906e-1fe303e5f668"], "metadata": {"title": "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "venue": "JMIR Medical Education", "year": 2023, "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c", "citationCount": 294, "openAccessPdf": "https://mededu.jmir.org/2023/1/e45312/PDF", "authors": ["Aidan Gilson", "C. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "externalIds": {"PubMedCentral": "9947764", "DOI": "10.2196/45312", "CorpusId": 256663603, "PubMed": "36753318"}}}, "dbeb5079-4654-4901-98e2-a47c333d1066": {"node_ids": ["6ef7a3d8-859e-4536-b0cb-87da8a690803"], "metadata": {"title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "citationCount": 150, "openAccessPdf": null, "authors": ["Stella Rose Biderman", "Hailey Schoelkopf", "Quentin G. Anthony", "Herbie Bradley", "Kyle O'Brien", "Eric Hallahan", "Mohammad Aflah Khan", "Shivanshu Purohit", "USVSN Sai Prashanth", "Edward Raff", "Aviya Skowron", "Lintang Sutawika", "Oskar van der Wal"], "externalIds": {"DBLP": "conf/icml/BidermanSABOHKP23", "ArXiv": "2304.01373", "DOI": "10.48550/arXiv.2304.01373", "CorpusId": 257921893}}}, "cc7cdf4d-72a7-40e4-a387-416a320e7a77": {"node_ids": ["ccba7ccf-a07c-4c2a-b833-d714a143b17b", "8cc7c4ee-2c73-4664-9226-71600afb3e2a"], "metadata": {"title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "venue": "arXiv.org", "year": 2023, "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "citationCount": 116, "openAccessPdf": null, "authors": ["Qinghao Ye", "Haiyang Xu", "Guohai Xu", "Jiabo Ye", "Ming Yan", "Yi Zhou", "Junyan Wang", "Anwen Hu", "Pengcheng Shi", "Yaya Shi", "Chenliang Li", "Yuanhong Xu", "Hehong Chen", "Junfeng Tian", "Qiang Qi", "Ji Zhang", "Feiyan Huang"], "externalIds": {"ArXiv": "2304.14178", "DBLP": "journals/corr/abs-2304-14178", "DOI": "10.48550/arXiv.2304.14178", "CorpusId": 258352455}}}, "fc10c197-ca0c-454c-9715-847fc89f5753": {"node_ids": ["af3a62d1-20ab-41c3-86fe-b6a3859b448a"], "metadata": {"title": "A Watermark for Large Language Models", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "cb5b71a622aff47014d4f28a958679629a8b6363", "citationCount": 125, "openAccessPdf": null, "authors": ["John Kirchenbauer", "Jonas Geiping", "Yuxin Wen", "Jonathan Katz", "Ian Miers", "T. Goldstein"], "externalIds": {"DBLP": "journals/corr/abs-2301-10226", "ArXiv": "2301.10226", "DOI": "10.48550/arXiv.2301.10226", "CorpusId": 256194179}}}, "92196516-0ccb-43f6-9292-15a8044b5b4b": {"node_ids": ["d9620c25-abf4-4b19-8c60-e2be6bd035f2", "87bcc8c2-7705-4f82-83ee-83c7400b0a60"], "metadata": {"title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions", "venue": "arXiv.org", "year": 2023, "paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "citationCount": 122, "openAccessPdf": null, "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "externalIds": {"ArXiv": "2304.12244", "DBLP": "journals/corr/abs-2304-12244", "DOI": "10.48550/arXiv.2304.12244", "CorpusId": 258298159}}}, "3234fd39-7653-4a79-a5c1-53a2c59f3b13": {"node_ids": ["7eea075b-2dd2-44de-9f85-ad7e42a6a039", "a0259824-9d8b-45eb-80c2-5e501c61e451"], "metadata": {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "citationCount": 78, "openAccessPdf": null, "authors": ["Pan Lu", "Baolin Peng", "Hao Cheng", "Michel Galley", "Kai-Wei Chang", "Y. Wu", "Song-Chun Zhu", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2304-09842", "ArXiv": "2304.09842", "DOI": "10.48550/arXiv.2304.09842", "CorpusId": 258212542}}}, "5dc919c1-7b45-4d76-a07b-b4fed3e88eb4": {"node_ids": ["21de50ed-52ad-4757-811b-a011348864bb", "8eb10aee-8654-4adf-9274-f9e6bc2c095d"], "metadata": {"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "citationCount": 81, "openAccessPdf": null, "authors": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "externalIds": {"DBLP": "journals/corr/abs-2303-08896", "ArXiv": "2303.08896", "DOI": "10.48550/arXiv.2303.08896", "CorpusId": 257557820}}}, "b879d8cd-e884-4787-b79d-18819ec7477d": {"node_ids": ["c40f5dad-f3b7-40e3-8a77-797bb40e0fb2"], "metadata": {"title": "Theory of Mind Might Have Spontaneously Emerged in Large Language Models", "venue": "", "year": 2023, "paperId": "da358fea1b9e23ebe027edec36ffbb56bd7d33f0", "citationCount": 74, "openAccessPdf": null, "authors": ["Michal Kosinskihttps://www.semanticscholar.org/me/account"], "externalIds": {"ArXiv": "2302.02083", "CorpusId": 256616268}}}, "da5f1c01-d813-43ae-9217-2d0346f39ca4": {"node_ids": ["09861dcc-9906-4806-b515-15c80ef781b6", "36e58545-d8a8-43df-90e4-ffb3c6e6457b"], "metadata": {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "venue": "arXiv.org", "year": 2023, "paperId": "9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3", "citationCount": 69, "openAccessPdf": null, "authors": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "externalIds": {"ArXiv": "2302.01560", "DBLP": "journals/corr/abs-2302-01560", "DOI": "10.48550/arXiv.2302.01560", "CorpusId": 256598146}}}, "55e6e486-bc6e-4073-bba5-89639e708e8c": {"node_ids": ["f8150398-a7b7-46b3-bcf5-8a2783bb228d", "236bba97-317c-442d-b369-ddb085356bee"], "metadata": {"title": "Teaching Large Language Models to Self-Debug", "venue": "arXiv.org", "year": 2023, "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "citationCount": 96, "openAccessPdf": null, "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Sch\u00e4rli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2304-05128", "ArXiv": "2304.05128", "DOI": "10.48550/arXiv.2304.05128", "CorpusId": 258059885}}}, "c5c97630-d99a-4175-bc31-b30aa39d8535": {"node_ids": ["8f1c72ae-ed18-466f-ae6d-55600385b60e", "b595587e-a0a9-4374-bfd0-0fd465e80743"], "metadata": {"title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "5501d00310b06e00351295529498cc684187148d", "citationCount": 118, "openAccessPdf": null, "authors": ["Tyna Eloundou", "Sam Manning", "Pamela Mishkin", "Daniel Rock"], "externalIds": {"ArXiv": "2303.10130", "DBLP": "journals/corr/abs-2303-10130", "CorpusId": 257622601}}}, "a73bcb04-a9cd-4978-b5f1-1fc1fe019048": {"node_ids": ["1ef81d84-76c3-49b6-a697-0f1aacb7b67a", "9ac10e17-e2d0-4606-9d34-d55dd28e076d"], "metadata": {"title": "Dissociating language and thought in large language models: a cognitive perspective", "venue": "arXiv.org", "year": 2023, "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d", "citationCount": 96, "openAccessPdf": null, "authors": ["Kyle Mahowald", "Anna A. Ivanova", "I. Blank", "N. Kanwisher", "J. Tenenbaum", "Evelina Fedorenko"], "externalIds": {"DBLP": "journals/corr/abs-2301-06627", "ArXiv": "2301.06627", "DOI": "10.48550/arXiv.2301.06627", "CorpusId": 255941592}}}, "67b87faa-cc33-4f08-863a-92ed7b442b87": {"node_ids": ["b030a5f8-ac32-4683-a6f6-d1442e6f1aa5"], "metadata": {"title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "venue": "arXiv.org", "year": 2023, "paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805", "citationCount": 55, "openAccessPdf": null, "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "externalIds": {"ArXiv": "2306.08568", "DBLP": "journals/corr/abs-2306-08568", "DOI": "10.48550/arXiv.2306.08568", "CorpusId": 259164815}}}, "ffc24877-e43c-40fc-893c-aa6f9a1c917f": {"node_ids": ["c73b9439-1f3e-42d3-8b50-f4958199bfde", "9c9e7d52-0514-4f79-b5ab-586cbde1f5d3"], "metadata": {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "citationCount": 213, "openAccessPdf": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "externalIds": {"ArXiv": "2305.10601", "DBLP": "journals/corr/abs-2305-10601", "DOI": "10.48550/arXiv.2305.10601", "CorpusId": 258762525}}}, "037cd110-ee3a-4926-8674-4817122f5327": {"node_ids": ["8c2d0994-4bd0-41f7-ad53-31422c268a09"], "metadata": {"title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality", "venue": "European Association for Machine Translation Conferences/Workshops", "year": 2023, "paperId": "4161ad2d2495d8af1d62dc5e71882bde642cd1c1", "citationCount": 68, "openAccessPdf": null, "authors": ["Tom Kocmi", "C. Federmann"], "externalIds": {"ArXiv": "2302.14520", "DBLP": "journals/corr/abs-2302-14520", "ACL": "2023.eamt-1.19", "DOI": "10.48550/arXiv.2302.14520", "CorpusId": 257232490}}}, "1e3e953f-e419-4782-8eae-dc40d3bb8f26": {"node_ids": ["bb5ea7d5-a2af-45cc-83d0-ea2ac11f1fe7"], "metadata": {"title": "ChatGPT and a new academic reality: Artificial Intelligence\u2010written research papers and the ethics of the large language models in scholarly publishing", "venue": "J. Assoc. Inf. Sci. Technol.", "year": 2023, "paperId": "da9683e826c37a6383c124b5c6cddefcb35ee8fd", "citationCount": 82, "openAccessPdf": "https://arxiv.org/pdf/2303.13367", "authors": ["Brady D. Lund", "Ting Wang", "Nishith Reddy Mannuru", "Bing Nie", "S. Shimray", "Ziang Wang"], "externalIds": {"DBLP": "journals/corr/abs-2303-13367", "ArXiv": "2303.13367", "DOI": "10.1002/asi.24750", "CorpusId": 257463753}}}, "26316f62-a24c-46fb-afff-b354925bc1c8": {"node_ids": ["0497ec8e-ad55-4ba1-a0e2-8c8fe4429b1f"], "metadata": {"title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "venue": "International Conference on Machine Learning", "year": 2023, "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e", "citationCount": 67, "openAccessPdf": null, "authors": ["Freda Shi", "Xinyun Chen", "Kanishka Misra", "Nathan Scales", "David Dohan", "E. Chi", "Nathanael Scharli", "Denny Zhou"], "externalIds": {"DBLP": "journals/corr/abs-2302-00093", "ArXiv": "2302.00093", "DOI": "10.48550/arXiv.2302.00093", "CorpusId": 256459776}}}, "12b644db-be9e-45c7-91f3-d6a67efa7afa": {"node_ids": ["7129cae3-93a6-45be-a1ab-0104a6c46987", "c92b16b4-8633-4dd4-bce0-f8eceef1ebad"], "metadata": {"title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "venue": "arXiv.org", "year": 2023, "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "citationCount": 108, "openAccessPdf": null, "authors": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lid\u00e9n", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "externalIds": {"DBLP": "journals/corr/abs-2302-12813", "ArXiv": "2302.12813", "DOI": "10.48550/arXiv.2302.12813", "CorpusId": 257205781}}}, "af725450-3369-4ed4-8f24-a8c80adb25dd": {"node_ids": ["0ced9a6f-0b73-4247-9619-dfc9de229cc0", "9187459a-bfae-4caa-b74e-815145561e7b"], "metadata": {"title": "Large Language Models are not Fair Evaluators", "venue": "arXiv.org", "year": 2023, "paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "citationCount": 74, "openAccessPdf": null, "authors": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "externalIds": {"ArXiv": "2305.17926", "DBLP": "journals/corr/abs-2305-17926", "DOI": "10.48550/arXiv.2305.17926", "CorpusId": 258960339}}}, "9c1d738c-8a33-4b6f-b7dd-b357f04f01bb": {"node_ids": ["832ae626-c45b-49f1-80bb-218120a7f0d9", "c0b99e15-2625-4f6d-84cd-1c60de929e27", "a10f5859-717a-4f94-a2be-5771da77b16e", "6b21224f-cfc0-4f90-a1c0-8076867d148e"], "metadata": {"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "venue": "arXiv.org", "year": 2023, "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "citationCount": 68, "openAccessPdf": null, "authors": ["K. Singhal", "Tao Tu", "Juraj Gottweis", "R. Sayres", "Ellery Wulczyn", "Le Hou", "Kevin Clark", "S. Pfohl", "H. Cole-Lewis", "Darlene Neal", "M. Schaekermann", "Amy Wang", "Mohamed Amin", "S. Lachgar", "P. A. Mansfield", "Sushant Prakash", "Bradley Green", "Ewa Dominowska", "B. A. Y. Arcas", "Nenad Toma\u0161ev", "Yun Liu", "Renee C Wong", "Christopher Semturs", "S. S. Mahdavi", "J. Barral", "D. Webster", "G. Corrado", "Y. Matias", "Shekoofeh Azizi", "A. Karthikesalingam", "Vivek Natarajan"], "externalIds": {"DBLP": "journals/corr/abs-2305-09617", "ArXiv": "2305.09617", "DOI": "10.48550/arXiv.2305.09617", "CorpusId": 258715226}}}, "a023287c-0fb4-4b10-a1c4-9a94b7d4ba4f": {"node_ids": ["6ec5bc0c-2754-49b8-b738-fc4edcb04697"], "metadata": {"title": "Benchmarking Large Language Models for News Summarization", "venue": "arXiv.org", "year": 2023, "paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29", "citationCount": 67, "openAccessPdf": null, "authors": ["Tianyi Zhang", "Faisal Ladhak", "Esin Durmus", "Percy Liang", "K. McKeown", "Tatsunori Hashimoto"], "externalIds": {"DBLP": "journals/corr/abs-2301-13848", "ArXiv": "2301.13848", "DOI": "10.48550/arXiv.2301.13848", "CorpusId": 256416014}}}, "ba222a86-8474-4ab0-b55f-65ef9ef4acfb": {"node_ids": ["6b259adb-188d-48a3-98ca-8689991857c2", "83ca4193-6149-4f16-9344-1f144888073e"], "metadata": {"title": "Training language models to follow instructions with human feedback", "venue": "Neural Information Processing Systems", "year": 2022, "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "citationCount": 2856, "openAccessPdf": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "J. Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "P. Welinder", "P. Christiano", "J. Leike", "Ryan J. Lowe"], "externalIds": {"DBLP": "journals/corr/abs-2203-02155", "ArXiv": "2203.02155", "DOI": "10.48550/arXiv.2203.02155", "CorpusId": 246426909}}}}}