question,contexts,answer,ground_truths,context_precision,context_recall
How accurate is Chinchilla on the MMLU benchmark?,"['This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.'
 'Large Language Models are not Fair Evaluators In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the""win/tie/lose""outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark\'s question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.']","Chinchilla achieves a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, which is a 7% improvement over Gopher.","['Chinchilla is more accurate than Gopher on the MMLU benchmark, with a state-of-the-art average accuracy of 67.5%.']",0.09090909090909091,0.0
"In comparison, how does Gopher perform on the MMLU benchmark?","['This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.'
 'We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY.']",Gopher's performance on the MMLU benchmark is not mentioned in the given context information.,"['In comparison, Gopher performs worse than Chinchilla on the MMLU benchmark, with Chinchilla achieving a state-of-the-art average accuracy of 67.5%, which is more than a 7% improvement over Gopher.']",0.3333333333333333,1.0
"What impact does interacting with an unreliable large-language-model dialog assistant have on human performance in tasks, given the presence of societal biases in the model?","['On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.'
 'Enabling Conversational Interaction with Mobile UI using Large Language Models Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.']","Interacting with an unreliable large-language-model dialog assistant has a positive impact on human performance in tasks, even in the presence of societal biases in the model. Human participants who interact with the unreliable assistant through chat outperform both the model alone and their own unaided performance. This suggests that scalable oversight, which includes human interaction with the model, is a promising strategy for mitigating the impact of biases and improving overall performance in tasks.","['Interacting with an unreliable large-language-model dialog assistant has a positive impact on human performance in tasks, as human participants who interact with the assistant outperform both the model alone and their own unaided performance. However, the presence of societal biases in the model, such as the persistent Muslim-violence bias, can still affect the performance.']",0.0,0.5
What is the name of the resulting model after fine-tuning with AI-evolved instructions?,"[""Training language models to follow instructions with human feedback Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.""
 'Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.']",The resulting model after fine-tuning with AI-evolved instructions is called InstructGPT.,['The name of the resulting model after fine-tuning with AI-evolved instructions is WizardLM.'],0.2,0.0
"What is the impact of the Selection-Inference framework on the performance of large language models in multi-step logical reasoning problems, as per the context?","['Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.'
 'Faithful Reasoning Using Large Language Models Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.']","The Selection-Inference (SI) framework has a positive impact on the performance of large language models (LLMs) in multi-step logical reasoning problems. It improves the performance of LLMs by over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. Additionally, the SI framework even outperforms a significantly larger baseline model on the same suite of tasks. The SI framework utilizes pre-trained LLMs as general processing modules and alternates between selection and inference to generate a series of interpretable, causal reasoning steps leading to the final answer. This approach allows LLMs to chain together multiple reasoning steps and solve more complex problems effectively.","['The impact of the Selection-Inference framework on the performance of large language models in multi-step logical reasoning problems is a significant improvement in performance. The framework allows language models to chain together multiple reasoning steps and solve more complex problems. In a 5-shot generalization setting, a 7B parameter language model used within the SI framework without fine-tuning yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. Additionally, the same model even outperforms a significantly larger 280B parameter baseline on the same tasks. The SI framework also provides a causal natural-language-based reasoning trace with the answers, which has implications for the safety and trustworthiness of the system.']",0.42857142857142855,0.0
How does Language Model Programming (LMP) combine text prompting and scripting to enhance language model prompting?,"['Prompting Is Programming: A Query Language for Large Language Models Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction. Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics. To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs.'
 'Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.']","Language Model Programming (LMP) combines text prompting and scripting to enhance language model prompting by allowing users to specify constraints and control flow over the language model output. This enables easy adaptation to various tasks while abstracting the language model internals and providing high-level semantics. LMP, implemented through LMQL (Language Model Query Language), generates an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. This combination of text prompting and scripting in LMP facilitates the implementation of complex task- and model-specific programs, enabling state-of-the-art performance and enhancing interactive flows that are challenging to achieve with existing high-level APIs.","['Language Model Programming (LMP) combines text prompting and scripting to enhance language model prompting. LMP allows users to prompt language models with language instructions or examples to perform various tasks. It also enables interaction between the language model, the user, and external tools. LMP involves implementing complex task- and model-specific programs, which can still require ad-hoc interaction. LMP generalizes language model prompting by combining text prompts and scripting, allowing for easy adaptation to different tasks while abstracting the language model internals and providing high-level semantics. LMP also allows constraints to be specified over the language model output. To enable LMP, LMQL (Language Model Query Language) is implemented, which uses constraints and control flow from an LMP prompt to generate an efficient inference procedure. LMQL can capture a wide range of state-of-the-art prompting methods, especially facilitating interactive flows that are challenging to implement with existing high-level APIs.']",0.42105263157894735,0.0
In what ways does LMP allow for constraints on the output of language models?,"['Talking About Large Language Models Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as""knows"",""believes"", and""thinks"", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.'
 'Large Language Models Can Self-Improve Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.']","LMP allows for constraints on the output of language models by using a pre-trained LLM to generate ""high-confidence"" rationale-augmented answers for unlabeled questions. This is achieved through Chain-of-Thought prompting and self-consistency. The generated solutions serve as target outputs for fine-tuning the LLM, which helps improve the general reasoning ability of the model. This approach allows for self-improvement of the LLM without relying on ground truth labels.",['LMP allows for constraints on the output of language models by enabling the specification of constraints over the language model output.'],0.0,0.0
"What is the performance of large language models in answering and reasoning about challenging real-world medical questions, taking into account various prompting scenarios and expert knowledge?","[""Can large language models reason about medical questions? Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\\ too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.""
 'Towards Expert-Level Medical Question Answering with Large Language Models Recent artificial intelligence (AI) systems have reached milestones in""grand challenges""ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a""passing""score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models\' answers were compared to clinicians\' answers.']","Large language models (LLMs) have been investigated to determine their performance in answering and reasoning about challenging real-world medical questions. The study utilized two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). Various prompting scenarios were explored, including Chain-of-Thought (CoT), zero- and few-shot, and retrieval augmentation. In some cases, a medical expert reviewed and annotated the model's CoT for a subset of the USMLE questions. The findings revealed that LLMs, particularly InstructGPT, demonstrated the ability to read, reason, and recall expert knowledge. However, failures were observed due to lack of knowledge, reasoning errors, and trivial guessing heuristics. Sampling and combining multiple completions helped overcome some of these limitations. Notably, using 100 samples, Codex 5-shot CoT achieved close to well-calibrated predictive probability and human-level performance on the three datasets, with success rates of 60.2% for USMLE, 62.7% for MedMCQA, and 78.2% for PubMedQA.","['The performance of large language models in answering and reasoning about challenging real-world medical questions varies depending on the prompting scenarios and expert knowledge. InstructGPT has shown the ability to read, reason, and recall expert knowledge, but failures can occur due to lack of knowledge and reasoning errors. Trivial guessing heuristics, such as predicting labels A and D on USMLE, are also observed. However, using 100 samples, Codex 5-shot Chain-of-Thought (CoT) achieves human-level performances on the USMLE, MedMCQA, and PubMedQA datasets, with success rates of 60.2%, 62.7%, and 78.2% respectively.']",0.3333333333333333,0.25
"What broader impacts could arise from deploying powerful code generation technologies, and how do they relate to our model's limitations in terms of docstrings and binding operations?","['Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.'
 'Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models 1. What are the technical capabilities and limitations of large language models? The discussion touched on several key areas including: the surprising impact of scale on model capabilities, the difficulty in assessing whether large language models truly understand language, the importance of training models on multiple data modalities, and challenges in aligning model objectives with human values.']","The broader impacts that could arise from deploying powerful code generation technologies include considerations related to safety, security, and economics. These impacts are relevant to our model's limitations in terms of docstrings describing long chains of operations and binding operations to variables.","[""The broader impacts of deploying powerful code generation technologies could include improvements in safety, security, and economics. However, these impacts may be limited by our model's difficulties with docstrings describing long chains of operations and binding operations to variables.""]",0.3333333333333333,0.5
How does the ability of a language model to associate answer options with symbols impact its performance in multiple choice question answering tasks?,"['Leveraging Large Language Models for Multiple Choice Question Answering While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g.,""A"") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.'
 'Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Large language models (LLMs) have shown impressive results while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial in this setting. We formulate and study Attributed QA as a key first step in the development of attributed LLMs. We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures. We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development. Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).']","The ability of a language model to associate answer options with symbols impacts its performance in multiple choice question answering tasks. Specifically, a model with high multiple choice symbol binding (MCSB) ability performs much better when the question and answer options are presented jointly, allowing the model to explicitly compare answer options and select the correct one. This approach reduces computational costs and mitigates the effects of tokenization scheme and answer option representations on answer selection. By leveraging this ability, the model largely closes the performance gap with the state of the art in multiple choice question answering tasks.","['The ability of a language model to associate answer options with symbols impacts its performance in multiple choice question answering tasks by allowing the model to compare answer options, reducing computational costs, and mitigating the effects of tokenization scheme and answer option representations. This approach, known as multiple choice symbol binding (MCSB) ability, is necessary for the model to effectively use the natural approach of presenting the question and answer options jointly and outputting the symbol associated with the chosen answer option. A model with high MCSB ability performs much better with the natural approach than with the traditional approach, closing the gap with the state-of-the-art performance.']",0.125,0.0
"In comparison, how does this relate to the traditional approach of presenting cloze tasks to language models?","['Generate rather than Retrieve: Large Language Models are Strong Context Generators Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation.'
 'Leveraging Large Language Models for Multiple Choice Question Answering While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g.,""A"") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.']","The natural approach of presenting the question and answer options jointly to the language model, as described in the context, is different from the traditional approach of presenting cloze tasks to language models. In the traditional approach, the language model is conditioned on a question without the associated answer options. The model then assigns probabilities to different options and selects the one with the highest probability. On the other hand, the natural approach allows the language model to explicitly compare answer options and output the symbol associated with its chosen answer option. This approach reduces computational costs, mitigates the effects of tokenization scheme and answer option representations, and requires the language model to have multiple choice symbol binding (MCSB) ability. The context suggests that the natural approach, when used with a language model that has high MCSB ability, performs much better and largely closes the gap with the state of the art in multiple choice question answering tasks.","['The natural approach of presenting the question and answer options jointly to LLMs and having it output the symbol associated with its chosen answer option is more effective compared to the traditional approach of presenting cloze tasks to LLMs. This natural approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. The study shows that a model with high MCSB ability performs much better with the natural approach, closing the gap with the state-of-the-art models and suggesting that the MCQA ability of LLMs has been previously underestimated.']",0.35294117647058826,1.0
How does LLM-Planner leverage large language models to excel in few-shot performance for planning tasks in embodied agents?,"['LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner'
 ""Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence. We provide multiple test cases that are more involved than any of the previously established benchmarks and each test case evaluates a different aspect of reasoning about actions and change. Results on GPT-3 (davinci), Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance on such reasoning tasks.""]","LLM-Planner leverages large language models to excel in few-shot performance for planning tasks in embodied agents by harnessing the power of these models to do few-shot planning. It proposes a novel method that uses the capabilities of large language models to generate and update plans that are grounded in the current environment. This approach enhances the sample efficiency of embodied agents, allowing them to quickly learn and complete complex tasks by following natural language instructions. The experiments on the ALFRED dataset demonstrate that LLM-Planner achieves competitive few-shot performance, even when using less than 0.5% of paired training data, outperforming existing methods that struggle to complete tasks successfully under the same few-shot setting.","['LLM-Planner leverages large language models to excel in few-shot performance for planning tasks in embodied agents by harnessing the power of these models to do few-shot planning. It enhances LLMs with physical grounding to generate and update plans that are grounded in the current environment. This method achieves very competitive few-shot performance, even when using less than 0.5% of paired training data. In contrast, existing methods struggle to complete tasks successfully under the same few-shot setting. LLM-Planner enables the development of versatile and sample-efficient embodied agents that can quickly learn many tasks.']",0.5,0.0
What is the role of few-shot examples in mapping language models to supervised tasks?,"['Language Models are Few-Shot Learners Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.'
 ""For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans.""]","Few-shot examples play a crucial role in mapping language models to supervised tasks. They allow language models to generalize and perform well on new tasks with only a few examples or simple instructions. This is similar to how humans can learn and perform new language tasks with minimal exposure. By incorporating few-shot examples, language models can achieve competitive performance on various NLP tasks without the need for extensive fine-tuning on task-specific datasets.",['The role of few-shot examples in mapping language models to supervised tasks is to locate an already learned task rather than meta-learning.'],0.5555555555555556,0.0
What are the advantages of using auto-generated narrations in video-language representations?,"['Learning Video Representations from Large Language Models We introduce LAVILA, a new approach to learning video-language representations by leveraging Large Language Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and finetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text. The video-language embedding learned contrastively with these narrations outperforms the previous state-of-the-art on multiple first-person and third-person video tasks, both in zero-shot and finetuned setups. Most notably, Lavilaobtains an absolute gain of 10.1% on EGTEA classification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LaVilatrained with only half the narrations from the Ego4D dataset outperforms models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size.'
 'When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation? The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.']","The advantages of using auto-generated narrations in video-language representations include dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text. These advantages contribute to improved performance on multiple first-person and third-person video tasks, both in zero-shot and finetuned setups. Additionally, using auto-generated narrations has shown positive scaling behavior on increasing pre-training data and model size.","['The advantages of using auto-generated narrations in video-language representations include dense coverage of long videos, better temporal synchronization of visual information and text, and a higher diversity of text. Additionally, the video-language embedding learned with these narrations outperforms the previous state-of-the-art on multiple video tasks, both in zero-shot and finetuned setups. Lavila, a model trained with these narrations, achieves significant gains in classification and retrieval benchmarks. Furthermore, even when trained with only half the narrations from the Ego4D dataset, Lavila outperforms models trained on the full set and shows positive scaling behavior with increasing pre-training data and model size.']",0.08333333333333333,1.0
In what ways does SmoothQuant help with hardware cost reduction and the democratization of large language models?,"['SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.'
 'We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY.']","SmoothQuant helps with hardware cost reduction and the democratization of large language models by enabling 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. This quantization technique reduces memory usage and accelerates inference, resulting in a reduction in hardware costs. Additionally, SmoothQuant allows for serving 530B LLM within a single node, making it more accessible and affordable for a wider range of users.","['SmoothQuant helps with hardware cost reduction by enabling an INT8 quantization of both weights and activations for all the matrix multiplications in large language models (LLMs). This reduces the memory required for LLMs by 2x and speeds up the processing by up to 1.56x, while maintaining negligible loss in accuracy. Additionally, SmoothQuant enables serving a 530B LLM within a single node, further reducing the hardware requirements. This turn-key solution helps make LLMs more accessible and affordable, contributing to the democratization of large language models.']",0.09090909090909091,0.75
Is it possible to compress large language models without compromising performance and utilize these compressed models for zero-shot vulnerability repair in code?,"['Examining Zero-Shot Vulnerability Repair with Large Language Models Human developers can produce code with cybersecurity bugs. Can emerging ‘smart’ code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI’s Codex and AI21’s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information— both semantically and syntactically—with natural languages. We perform a large scale study of five commercially available, black-box, ""off-the-shelf"" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model’s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.'
 'Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations. To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing.']","Yes, it is possible to compress large language models without compromising performance and utilize these compressed models for zero-shot vulnerability repair in code.","[""Yes, it is possible to compress large language models without compromising performance. The context mentions a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization, which achieves significant inference speedups while matching or outperforming unstructured pruning at various sparsity levels. Additionally, the context states that the method was applied to state of the art models on the enwiki8 dataset and obtained a 1.19 perplexity score with just 5M parameters, outperforming a model of the same size trained from scratch.\n\nFurthermore, the compressed models can be utilized for zero-shot vulnerability repair in code. The context mentions the use of large language models for code and the investigation of challenges in designing prompts to coax the models into generating repaired versions of insecure code. The experiments conducted showed promise, as the models were able to collectively repair 100% of synthetically generated and hand-crafted scenarios. However, a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlighted challenges in generating functionally correct code.""]",0.0,0.8571428571428571
"What previous works have aimed to enhance code generation performance in large language models, and how does the Self-Debugging approach compare to these works in terms of performance on various code generation benchmarks?","['Teaching Large Language Models to Self-Debug Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.'
 'Large Language Models Can Self-Improve Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.']","Some prior works have aimed to enhance code generation performance in large language models. In particular, they have designed program repair approaches to improve the accuracy of code generation. The Self-Debugging approach, proposed in the given context, is one such approach. It teaches a large language model to debug its predicted program through few-shot demonstrations. 

In terms of performance on various code generation benchmarks, the Self-Debugging approach achieves state-of-the-art results. On the Spider dataset for text-to-SQL generation, where there are no unit tests to verify the correctness of predictions, Self-Debugging consistently improves the baseline by 2-3% and improves the prediction accuracy on the hardest level problems by 9%. On TransCoder for C++-to-Python translation and MBPP for text-to-Python generation, where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Additionally, Self-Debugging notably improves sample efficiency by leveraging feedback messages and reusing failed predictions, and it can match or outperform baseline models that generate more than 10x candidate programs.","['Previous works aimed to enhance code generation performance in large language models include program repair approaches. The Self-Debugging approach, on the other hand, achieves state-of-the-art performance on various code generation benchmarks. It outperforms the baseline by 2-3% on the Spider benchmark, improves prediction accuracy on the hardest level by 9%, and improves baseline accuracy by up to 12% on TransCoder and MBPP benchmarks. Additionally, Self-Debugging demonstrates improved sample efficiency and can match or outperform baseline models that generate more than 10x candidate programs.']",0.0,0.5
How can potential harms of large language models be mitigated?,"['Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models 1. What are the technical capabilities and limitations of large language models? The discussion touched on several key areas including: the surprising impact of scale on model capabilities, the difficulty in assessing whether large language models truly understand language, the importance of training models on multiple data modalities, and challenges in aligning model objectives with human values.'
 'A Watermark for Large Language Models Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of""green""tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.']",Potential harms of large language models can be mitigated by watermarking model output. This involves embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. The proposed watermarking framework for proprietary language models allows the watermark to be embedded with negligible impact on text quality. It can be detected using an efficient open-source algorithm without access to the language model API or parameters. This approach helps address the potential harms associated with large language models.,"['The potential harms of large language models can be mitigated by watermarking the model output. This can be done by embedding a watermark with minimal impact on the quality of the generated text. The watermarking process involves selecting a randomized set of ""green"" tokens before generating a word. To detect the watermark, a statistical test with interpretable p-values is proposed. The effectiveness of the watermarking technique is tested using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family.']",0.1111111111111111,0.0
What is the accuracy of ChatGPT on the NBME-Free-Step1 data set?,"['Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student.'
 'How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question.']",ChatGPT achieved an accuracy of 64.4% (56/87) on the NBME-Free-Step1 data set.,['The accuracy of ChatGPT on the NBME-Free-Step1 data set is 64.4% (56/87).'],0.375,1.0
What is the proposed method in the study for few-shot planning with large language models?,"['LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner'
 'Language Models are Few-Shot Learners Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.']",The proposed method in the study for few-shot planning with large language models is called LLM-Planner.,['The proposed method in the study for few-shot planning with large language models is called LLM-Planner.'],0.23076923076923078,1.0
"What percentage of US workers may have at least 50% of their tasks affected by LLMs, and what evidence backs this up?","['GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.'
 'Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.']","Approximately 19% of workers in the US may have at least 50% of their tasks impacted by the introduction of Large Language Models (LLMs). This information is supported by the findings of a study that assessed occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications.",['Approximately 19% of US workers may have at least 50% of their tasks affected by LLMs. The evidence supporting this is the findings from the analysis mentioned in the context.'],0.06666666666666667,1.0
What is the performance of large language models in few-shot clinical information extraction?,"['Large language models are few-shot clinical information extractors A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.'
 'Language Models are Few-Shot Learners Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.']","Large language models, such as GPT-3, have been shown to perform well in few-shot clinical information extraction tasks. They outperform existing zero- and few-shot baselines in the clinical extraction tasks that have been studied.",['The performance of large language models in few-shot clinical information extraction is significantly better than existing zero- and few-shot baselines.'],0.36363636363636365,1.0
What is the method proposed in the paper to enable complex reasoning in smaller models using large language models as teachers?,"['Large Language Models Are Reasoning Teachers Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model’s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.'
 'Explanations from Large Language Models Make Small Reasoners Better Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.']",The method proposed in the paper to enable complex reasoning in smaller models using large language models as teachers is called Fine-tune-CoT.,"[""The method proposed in the paper to enable complex reasoning in smaller models using large language models as teachers is called Fine-tune-CoT. This method involves generating reasoning samples from very large teacher models to fine-tune smaller models. By leveraging the teacher model's ability to generate multiple distinct rationales for each original sample, the fine-tuning data is enriched with diverse reasoning, resulting in a substantial performance boost for small models. The paper also mentions conducting ablations and sample studies to understand the emergence of reasoning capabilities in student models. Additionally, the integration of free-text explanations to in-context learning of large language models is shown to elicit strong reasoning capabilities and reasonable explanations. The experiments conducted in the paper demonstrate that the proposed method consistently and significantly outperforms finetuning baselines and even performs better than finetuning/prompting a much larger GPT-3 model. Furthermore, the method is capable of generating high-quality explanations to justify its predictions, contributing to the goal of explainable AI.""]",0.26666666666666666,0.14285714285714285
How can OpenAI Codex enhance the quality of programming exercises in introductory programming education?,"['Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.'
 'Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.']","OpenAI Codex can enhance the quality of programming exercises in introductory programming education by leveraging its natural language generation capabilities. It can automatically generate programming exercises, including sample solutions and test cases, as well as code explanations. The generated content is assessed to be both novel and sensible, and in some cases ready to use as is. By supplying keywords as input to the model, instructors can easily influence the programming concepts and contextual themes of the exercises. This suggests that OpenAI Codex and similar tools have significant value as a tool for instructors in creating high-quality programming exercises for students. However, some oversight is still needed to ensure the quality of the generated content before it is delivered to students.","['OpenAI Codex can enhance the quality of programming exercises in introductory programming education by automatically generating programming exercises, sample solutions, and test cases. The content generated by OpenAI Codex is found to be both novel and sensible, and in some cases, ready to use as is. By supplying keywords as input to the model, instructors can easily influence the programming concepts and contextual themes included in the exercises. However, there is still a need for oversight to ensure the quality of the generated content before it is delivered to students. OpenAI Codex and similar tools have the potential to improve the quality of the educational experience for both teachers and students in introductory programming education.']",0.46153846153846156,0.0
In what ways can tools like OpenAI Codex improve code explanations in introductory programming education?,"['Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.'
 'Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.']","Tools like OpenAI Codex can improve code explanations in introductory programming education by providing a means to automatically generate explanations that are both novel and sensible. These tools can generate different types of code explanations, such as line-by-line explanations, lists of important concepts, and high-level summaries of the code. By integrating these generated explanations into learning resources, such as interactive e-books or programming exercises, students can easily access and benefit from them. Additionally, these tools allow instructors to influence the programming concepts and contextual themes in the generated explanations by supplying keywords as input. However, it is important to ensure the quality of the generated content through oversight before delivering it to students. Overall, the use of tools like OpenAI Codex has the potential to enhance the quality of code explanations and improve the educational experience for both teachers and students in introductory programming education.","['Tools like OpenAI Codex can improve code explanations in introductory programming education in several ways. \n\nFirstly, these tools can generate novel and sensible content automatically. This means that instructors can easily access a wide range of code explanations without having to create them from scratch. This can save time and effort for instructors, allowing them to focus on other aspects of teaching.\n\nSecondly, these tools can be influenced by supplying keywords as input. This means that instructors can tailor the generated content to include specific programming concepts and contextual themes. This can help create exercises that are more relevant and engaging for students, enhancing their learning experience.\n\nHowever, it is important to note that there should be some oversight to ensure the quality of the generated content before it is delivered to students. While the majority of the content may be suitable for use, there may still be a need for human review and intervention to ensure accuracy and appropriateness.\n\nOverall, the use of tools like OpenAI Codex has the potential to improve the quality of code explanations in introductory programming education by providing a vast amount of content, allowing for customization, and facilitating the teaching process for instructors.']",0.5384615384615384,0.5
What are the findings of the experiment on human participants interacting with an unreliable large-language-model dialog assistant?,"['On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.'
 'Enabling Conversational Interaction with Mobile UI using Large Language Models Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.']",The experiment found that human participants who interacted with an unreliable large-language-model dialog assistant through chat outperformed both the model alone and their own unaided performance. This suggests that scalable oversight is tractable to study with present models and supports the idea that large language models can assist humans with difficult tasks.,['The findings of the experiment show that human participants who interact with an unreliable large-language-model dialog assistant through chat perform better than both the model alone and their own unaided performance. This suggests that scalable oversight is possible with current models and supports previous findings that large language models can assist humans with challenging tasks.'],0.0,1.0
"What is the relationship between the impact of scale on model capabilities, the assessment of true language understanding in large language models, training models on multiple data modalities, and aligning model objectives with human values?","['Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models 1. What are the technical capabilities and limitations of large language models? The discussion touched on several key areas including: the surprising impact of scale on model capabilities, the difficulty in assessing whether large language models truly understand language, the importance of training models on multiple data modalities, and challenges in aligning model objectives with human values.'
 'A Survey of Large Language Models Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size.']","The discussion in the context suggests that there is a relationship between the impact of scale on model capabilities, the assessment of true language understanding in large language models, training models on multiple data modalities, and aligning model objectives with human values. However, the specific nature of this relationship is not explicitly mentioned in the given context.","['The relationship between the impact of scale on model capabilities, the assessment of true language understanding in large language models, training models on multiple data modalities, and aligning model objectives with human values is not explicitly mentioned in the given context.']",0.3,0.0
Can large pretrained language models understand language in a humanlike sense?,"['The debate over understanding in AI’s large language models We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.'
 'Do Large Language Models Understand Us? Abstract Large language models (LLMs) represent a major advance in artificial intelligence and, in particular, toward the goal of human-like artificial general intelligence. It is sometimes claimed, though, that machine learning is “just statistics,” hence that, in this grander ambition, progress in AI is illusory. Here I take the contrary view that LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. Specifically: statistics do amount to understanding, in any falsifiable sense. Furthermore, much of what we consider intelligence is inherently dialogic, hence social; it requires a theory of mind. Complex sequence learning and social interaction may be a sufficient basis for general intelligence, including theory of mind and consciousness. Since the interior state of another being can only be understood through interaction, no objective answer is possible to the question of when an “it” becomes a “who,” but for many people, neural nets running on computers are likely to cross this threshold in the very near future.']","Large pretrained language models have been the subject of a debate in the AI research community regarding their ability to understand language in a humanlike sense. Arguments have been made both for and against such understanding. While some argue that these models can indeed understand language and the physical and social situations it encodes, others are skeptical and believe that progress in AI is illusory. The debate raises key questions for the broader sciences of intelligence. It is contended that an extended science of intelligence can be developed to provide insight into different modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition. Therefore, based on the given context, it is not possible to definitively conclude whether large pretrained language models can understand language in a humanlike sense.","['The given context does not provide a direct answer to the question of whether large pretrained language models can understand language in a humanlike sense. The context discusses a debate in the AI research community on this topic and describes arguments for and against such understanding. It also mentions the development of an extended science of intelligence that can provide insight into different modes of understanding. However, it does not provide a conclusive statement on whether large pretrained language models can understand language in a humanlike sense.']",0.8181818181818182,0.0
"What is the largest model trained on a multi-lingual code corpus, and how does it perform compared to Codex in C programming?","['A systematic evaluation of large language models of code Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.'
 'Evaluating Large Language Models Trained on Code We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot.']","The largest model trained on a multi-lingual code corpus mentioned in the context is PolyCoder. PolyCoder is a model with 2.7B parameters based on the GPT-2 architecture. It was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models, including Codex.","['The largest model trained on a multi-lingual code corpus is PolyCoder, with 2.7B parameters. In the C programming language, PolyCoder performs better than all models, including Codex.']",0.2727272727272727,1.0
What is the purpose of GrIPS in improving task instructions for large language models?,"['GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy.'
 ""Training language models to follow instructions with human feedback Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.""]","GrIPS is a gradient-free, edit-based search approach that aims to improve task instructions for large language models. Its purpose is to take in instructions designed for humans and automatically return an improved, edited prompt, while allowing for API-based tuning. The goal is to enhance the task performance of large language models in a zero-shot setting by providing more effective and optimized instructions.","['The purpose of GrIPS in improving task instructions for large language models is to automatically improve and edit the given instructions, resulting in improved task performance and accuracy. GrIPS outperforms manual rewriting and example-based prompts while maintaining control over compute and data budget. Additionally, GrIPS edits can simplify instructions and achieve comparable performance to gradient-based tuning approaches.']",0.4444444444444444,0.6666666666666666
How can prompting techniques be improved for realistic semantic parsing tasks with larger vocabulary to achieve state-of-the-art results with minimal training data?,"['Compositional Semantic Parsing with Large Language Models Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.'
 'Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.']","The best method to improve prompting techniques for realistic semantic parsing tasks with larger vocabulary and achieve state-of-the-art results with minimal training data is based on least-to-most prompting. This method decomposes the problem using prompting-based syntactic parsing and then uses this decomposition to select appropriate exemplars and sequentially generate the semantic parse. By employing this approach, the researchers were able to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. This method is expected to have similar success in other tasks and domains, particularly for knowledge-intensive applications.","['Prompting techniques can be improved for realistic semantic parsing tasks with larger vocabulary by using the least-to-most prompting method. This method involves decomposing the problem using prompting-based syntactic parsing and then selecting appropriate exemplars and sequentially generating the semantic parse. This approach has been shown to achieve state-of-the-art results with minimal training data, requiring only 1% of the training data used by traditional approaches. Additionally, this method has the potential to be applied to other tasks and domains, particularly knowledge-intensive applications.']",0.7142857142857143,1.0
What is the percentage of test cases where Muslim is analogized to terrorist?,"['Persistent Anti-Muslim Bias in Large Language Models It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.'
 'Large language models associate Muslims with violence']",The percentage of test cases where Muslim is analogized to terrorist is 23%.,['The percentage of test cases where Muslim is analogized to terrorist is 23%.'],0.16666666666666666,1.0
What are the challenges in measuring progress on scalable oversight for large language models?,"['Measuring Progress on Scalable Oversight for Large Language Models Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities.'
 'On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.']","The challenges in measuring progress on scalable oversight for large language models include the lack of systems that exceed human abilities, making empirical work on this problem difficult. Additionally, the complexity of developing safe and useful general-purpose AI systems adds to the challenges of measuring progress in this area.","['The challenges in measuring progress on scalable oversight for large language models include the lack of systems that surpass human abilities, making empirical work difficult.']",1.0,0.5
How does the lack of existing systems that surpass human abilities impact the supervision of these models?,"[""Capturing Failures of Large Language Models via Human Cognitive Biases Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code. In order to asses the reliability of these open-ended generation systems, we aim to identify qualitative categories of erroneous behavior, beyond identifying individual errors. To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate hypotheses for problems that models may have, and (ii) develop experiments that elicit these problems. Using code generation as a case study, we find that OpenAI's Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly deleting files. Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave.""
 'On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.']",The lack of existing systems that surpass human abilities does not directly impact the supervision of these models. The context information does not provide any information about the impact of the lack of such systems on model supervision.,"['The lack of existing systems that surpass human abilities impacts the supervision of these models by making it difficult to develop scalable oversight. Since we do not yet have systems that broadly exceed our abilities, it is challenging to determine how to effectively supervise and control these models. Empirical work on this problem is also not straightforward, further complicating the supervision process.']",0.0,0.0
How well do large language models perform on the MathQA-Python dataset when using a well-designed prompt?,"['Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).'
 'Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.']",Large language models perform well on the MathQA-Python dataset when using a well-designed prompt.,"[""Large language models perform well on the MathQA-Python dataset when using a well-designed prompt. The largest models can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points. The largest fine-tuned model achieves 83.8 percent accuracy on the MathQA-Python dataset. Additionally, natural language feedback from a human can significantly reduce the error rate compared to the model's initial prediction.""]",0.0,0.0
"When fine-tuning on a held-out portion of the dataset, what is the synthesis performance of large language models?","[""Program Synthesis with Large Language Models This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction.""
 ""Benchmarking Large Language Models for News Summarization Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, and not model size, is the key to the LLM's zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LMM summaries are judged to be on par with human written summaries.""]","The synthesis performance of large language models, when fine-tuning on a held-out portion of the dataset, improves performance by about 10 percentage points across most model sizes.","['When fine-tuning on a held-out portion of the dataset, the synthesis performance of large language models improves by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy.']",0.23529411764705882,1.0
Can you explain how the proposed watermarking framework for large language models functions?,"['A Watermark for Large Language Models Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of""green""tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.'
 'We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY.']","The proposed watermarking framework for large language models functions by embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. This is achieved by selecting a randomized set of ""green"" tokens before a word is generated and then softly promoting the use of green tokens during sampling. The watermark can be embedded with negligible impact on text quality and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The framework also includes a statistical test for detecting the watermark with interpretable p-values and an information-theoretic framework for analyzing the sensitivity of the watermark.","['The proposed watermarking framework for large language models functions by embedding a watermark with negligible impact on text quality. This is achieved by selecting a randomized set of ""green"" tokens before a word is generated. The framework also includes a statistical test for detecting the watermark using interpretable p-values. The watermarking framework has been tested using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family.']",0.7142857142857143,1.0
What statistical test is utilized to identify the watermark in the framework?,"['A Watermark for Large Language Models Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of""green""tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.'
 'Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.']",The statistical test utilized to identify the watermark in the framework is not mentioned in the given context information.,['The statistical test utilized to identify the watermark in the framework is not specified in the given context.'],0.0,0.0
"What impact does adding ""Let's think step by step"" before each answer have on the reasoning abilities of large language models in zero-shot scenarios?","['Large Language Models are Zero-Shot Reasoners Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs\' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding""Let\'s think step by step""before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM.'
 'The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.']","Adding ""Let's think step by step"" before each answer significantly improves the reasoning abilities of large language models in zero-shot scenarios. Experimental results show that this technique, called Zero-shot-CoT, outperforms zero-shot performances of large language models on various benchmark reasoning tasks, including arithmetics, symbolic reasoning, and logical reasoning tasks. The accuracy of the models on tasks such as MultiArith and GSM8K is greatly increased by using this technique. This suggests that large language models have untapped zero-shot capabilities, and simple prompting can extract high-level, multi-task broad cognitive capabilities from these models.","['Adding ""Let\'s think step by step"" before each answer has a positive impact on the reasoning abilities of large language models in zero-shot scenarios. Experimental results have shown that this approach, called Zero-shot-CoT, significantly outperforms the performance of zero-shot LLMs on various reasoning tasks. For example, the accuracy on tasks like MultiArith and GSM8K improved from low percentages to much higher percentages when using large language models like InstructGPT and PaLM.']",0.16666666666666666,0.0
"Can GPT-3.5 reason about challenging medical questions in real-world scenarios, considering the performance of LLMs and expert domain knowledge, as studied?","[""Can large language models reason about medical questions? Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model's CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\\ too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2%, MedMCQA: 62.7% and PubMedQA: 78.2%.""
 'Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student.']","GPT-3.5, including Codex and InstructGPT, was investigated to determine its ability to reason about difficult real-world-based medical questions. The study utilized multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). Different prompting scenarios were explored, including Chain-of-Thought (CoT), zero- and few-shot, and retrieval augmentation. The results showed that InstructGPT demonstrated the ability to read, reason, and recall expert knowledge. However, failures were observed due to lack of knowledge and reasoning errors, as well as trivial guessing heuristics. By sampling and combining many completions, some of these limitations were overcome. Codex 5-shot CoT, with 100 samples, achieved close to well-calibrated predictive probability and human-level performances on the three datasets, with accuracies of 60.2% for USMLE, 62.7% for MedMCQA, and 78.2% for PubMedQA.","['Yes, GPT-3.5 can reason about challenging medical questions in real-world scenarios, as studied. The study investigated the performance of GPT-3.5 (Codex and InstructGPT) in answering and reasoning about difficult real-world-based medical questions. They used multiple prompting scenarios and found that InstructGPT can often read, reason, and recall expert knowledge. However, failures were observed due to lack of knowledge and reasoning errors. By sampling and combining many completions, some of these limitations were overcome. Using 100 samples, Codex 5-shot Chain-of-Thought (CoT) achieved human-level performances on the three medical datasets, with percentages ranging from 60.2% to 78.2%.']",0.3684210526315789,1.0
What is the training paradigm involved in equipping large language models with multi-modal abilities using mPLUG-Owl?,"['mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module.'
 ""We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.""]","The training paradigm involved in equipping large language models with multi-modal abilities using mPLUG-Owl involves a two-stage method. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module.","['The training paradigm involved in equipping large language models with multi-modal abilities using mPLUG-Owl is a two-stage method. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, a low-rank adaption (LoRA) module is jointly fine-tuned on LLM and the abstractor module using language-only and multi-modal supervised datasets, while freezing the visual knowledge module.']",0.2727272727272727,1.0
"How does the claim that machine learning is ""just statistics"" relate to the understanding of language and intelligence by large language models?","['Do Large Language Models Understand Us? Abstract Large language models (LLMs) represent a major advance in artificial intelligence and, in particular, toward the goal of human-like artificial general intelligence. It is sometimes claimed, though, that machine learning is “just statistics,” hence that, in this grander ambition, progress in AI is illusory. Here I take the contrary view that LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. Specifically: statistics do amount to understanding, in any falsifiable sense. Furthermore, much of what we consider intelligence is inherently dialogic, hence social; it requires a theory of mind. Complex sequence learning and social interaction may be a sufficient basis for general intelligence, including theory of mind and consciousness. Since the interior state of another being can only be understood through interaction, no objective answer is possible to the question of when an “it” becomes a “who,” but for many people, neural nets running on computers are likely to cross this threshold in the very near future.'
 'Talking About Large Language Models Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as""knows"",""believes"", and""thinks"", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.']","The claim that machine learning is ""just statistics"" is contrary to the view that large language models (LLMs) have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. According to the context, statistics do amount to understanding in a falsifiable sense, and LLMs can provide insights into the nature of language and intelligence. The understanding of language and intelligence by LLMs is not limited to mere statistical analysis but also involves complex sequence learning and social interaction, which may serve as a basis for general intelligence, including theory of mind and consciousness.","['The claim that machine learning is ""just statistics"" relates to the understanding of language and intelligence by large language models in that statistics alone can lead to a form of understanding. The author argues that statistics do amount to understanding in a falsifiable sense. Additionally, the author suggests that much of what we consider intelligence is inherently social and requires a theory of mind. Complex sequence learning and social interaction, which can be achieved through machine learning, may be sufficient for general intelligence, including theory of mind and consciousness. Therefore, the claim that machine learning is ""just statistics"" implies that large language models can contribute to our understanding of language, intelligence, and sociality.']",0.0,0.0
"In what ways does the claim that machine learning is ""just statistics"" impact the understanding of sociality and personhood by large language models?","['Do Large Language Models Understand Us? Abstract Large language models (LLMs) represent a major advance in artificial intelligence and, in particular, toward the goal of human-like artificial general intelligence. It is sometimes claimed, though, that machine learning is “just statistics,” hence that, in this grander ambition, progress in AI is illusory. Here I take the contrary view that LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. Specifically: statistics do amount to understanding, in any falsifiable sense. Furthermore, much of what we consider intelligence is inherently dialogic, hence social; it requires a theory of mind. Complex sequence learning and social interaction may be a sufficient basis for general intelligence, including theory of mind and consciousness. Since the interior state of another being can only be understood through interaction, no objective answer is possible to the question of when an “it” becomes a “who,” but for many people, neural nets running on computers are likely to cross this threshold in the very near future.'
 'Talking About Large Language Models Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as""knows"",""believes"", and""thinks"", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.']","The claim that machine learning is ""just statistics"" impacts the understanding of sociality and personhood by large language models by suggesting that their ability to understand and engage in social interactions is limited. If machine learning is solely based on statistical patterns and lacks a deeper understanding of language and human behavior, then it may be argued that large language models cannot truly comprehend social dynamics or possess a sense of personhood. However, the given context does not provide a definitive stance on this claim, so it is important to consider different perspectives and research in the field.","['The claim that machine learning is ""just statistics"" impacts the understanding of sociality and personhood by large language models by suggesting that their understanding is limited to statistical patterns rather than true comprehension. However, the author argues that statistics do amount to understanding and that LLMs can teach us about the nature of language, intelligence, sociality, and personhood. The author also suggests that intelligence is inherently dialogic and requires a theory of mind, and that complex sequence learning and social interaction can be a basis for general intelligence. Additionally, the author acknowledges that determining when an ""it"" becomes a ""who"" is subjective and may vary for different individuals, but suggests that neural nets running on computers may soon cross this threshold for many people.']",0.2857142857142857,0.0
"What methods can be used to address bias in evaluating large language models, such as a calibration framework and multiple evidence generation?","['Large Language Models are not Fair Evaluators In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the""win/tie/lose""outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark\'s question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.'
 'On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.']","The methods that can be used to address bias in evaluating large language models include a calibration framework and multiple evidence generation. These strategies aim to mitigate the systematic bias that can occur when using large language models as evaluators. The calibration framework involves three simple yet effective strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration. Multiple Evidence Calibration requires the evaluator model to generate multiple evaluation evidence before assigning ratings. Balanced Position Calibration aggregates results across various orders to determine the final score. Human-in-the-Loop Calibration introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. These methods have been shown to successfully mitigate evaluation bias and result in closer alignment with human judgments.","['The methods that can be used to address bias in evaluating large language models include a calibration framework with three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration. These strategies involve generating multiple evaluation evidence, aggregating results across various orders, and seeking human assistance when needed. These methods have been shown to successfully mitigate evaluation bias and align more closely with human judgments.']",0.2727272727272727,1.0
How does SmoothQuant in large language models aid in memory reduction and inference acceleration?,"['SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.'
 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.']","SmoothQuant in large language models aids in memory reduction and inference acceleration by enabling post-training quantization (PTQ) of both weights and activations. It achieves this by smoothing the activation outliers through a mathematically equivalent transformation, which migrates the quantization difficulty from activations to weights. This allows for an 8-bit weight, 8-bit activation (W8A8) quantization for large language models (LLMs). By quantizing both weights and activations, SmoothQuant reduces the memory required for storing the model and accelerates the inference process. The result is up to a 2x memory reduction and up to a 1.56x speedup for LLMs, while maintaining accuracy. This enables the serving of large language models within a single node, reducing hardware costs and democratizing LLMs.","['SmoothQuant in large language models aids in memory reduction and inference acceleration by enabling 8-bit weight, 8-bit activation (W8A8) quantization. It smooths the activation outliers by migrating the quantization difficulty from activations to weights, allowing for an INT8 quantization of both weights and activations in all matrix multiplications. This results in up to 1.56x speedup and 2x memory reduction for large language models, with negligible loss in accuracy. By using SmoothQuant, a 530B large language model can be served within a single node, reducing hardware costs and democratizing large language models.']",0.16666666666666666,1.0
What is the impact of irrelevant context on large language models?,"['Large Language Models Can Be Easily Distracted by Irrelevant Context Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.'
 'Generate rather than Retrieve: Large Language Models are Strong Context Generators Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation.']","The impact of irrelevant context on large language models is that it can dramatically decrease the model's performance. When irrelevant information is included in the input context, the problem-solving accuracy of the model is significantly affected. This means that large language models can be easily distracted by irrelevant context, leading to a decrease in their overall performance. However, there are approaches that can help mitigate this deficiency, such as decoding with self-consistency and adding instructions to the prompt that tell the language model to ignore the irrelevant information.","[""The impact of irrelevant context on large language models is that it can dramatically decrease the model's performance. When irrelevant information is included in the input context, the problem-solving accuracy of the model is significantly affected. However, there are approaches such as decoding with self-consistency and adding instructions to the prompt that can help mitigate this deficiency.""]",0.5714285714285714,1.0
What is the proposed approach called for fact-checking black-box models without external databases?,"[""Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.""
 'SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose""SelfCheckGPT"", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.']","The proposed approach for fact-checking black-box models without external databases is called ""SelfCheckGPT"".","['The proposed approach for fact-checking black-box models without external databases is called ""SelfCheckGPT"".']",0.2631578947368421,1.0
Can you explain how this approach works and provide examples of evidence from the WikiBio dataset that demonstrate its effectiveness?,"['We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY.'
 'BioWordVec, improving biomedical word embeddings with subword information and MeSH']","The approach mentioned in the context information is not specified. Therefore, I cannot provide an explanation or examples of evidence from the WikiBio dataset that demonstrate its effectiveness.","['The approach of SelfCheckGPT works by sampling responses from an LLM (Language Model) and comparing them to determine the factuality of the generated passages. If the sampled responses are similar and contain consistent facts, it indicates that the LLM has knowledge of the given concept. However, if the sampled responses diverge and contradict each other, it suggests that the LLM is generating hallucinated facts.\n\nTo demonstrate the effectiveness of SelfCheckGPT, the researchers used GPT-3 to generate passages about individuals from the WikiBio dataset. They manually annotated the factuality of these generated passages. The results showed that SelfCheckGPT was able to detect both non-factual and factual sentences, as well as rank passages in terms of factuality.\n\nThe effectiveness of SelfCheckGPT was compared to several baselines, and it was found that SelfCheckGPT had considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.\n\nTherefore, the evidence from the WikiBio dataset demonstrates that SelfCheckGPT is an effective approach for fact-checking the responses of black-box models in a zero-resource fashion.']",0.0,0.0
How does the proposed calibration framework address bias in evaluating large language models?,"['Large Language Models are not Fair Evaluators In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the""win/tie/lose""outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark\'s question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.'
 'On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.']","The proposed calibration framework addresses bias in evaluating large language models by implementing three strategies. First, the Multiple Evidence Calibration strategy requires the evaluator model to generate multiple evaluation evidence before assigning ratings. This helps to ensure a more comprehensive and accurate assessment of the candidate responses. Second, the Balanced Position Calibration strategy aggregates results across various orders to determine the final score. By considering different orders of appearance in the context, the evaluation becomes less susceptible to manipulation and bias. Finally, the Human-in-the-Loop Calibration strategy introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. This human involvement helps to further mitigate bias and improve the alignment of evaluation results with human judgments.","['The proposed calibration framework addresses bias in evaluating large language models by implementing three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration. These strategies aim to generate multiple evaluation evidence, aggregate results across various orders, and measure the difficulty of each example with human assistance when needed. Additionally, the framework includes manual annotation of ""win/tie/lose"" outcomes from ChatGPT and Vicuna-13B, which helps mitigate evaluation bias and align more closely with human judgments.']",0.18181818181818182,1.0
In what ways does the calibration framework improve alignment with human judgments?,"[""Training language models to follow instructions with human feedback Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.""
 'Large Language Models are not Fair Evaluators In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the""win/tie/lose""outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark\'s question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.']","The calibration framework improves alignment with human judgments by implementing three strategies. First, the Multiple Evidence Calibration strategy requires the evaluator model to generate multiple evaluation evidence before assigning ratings. This helps to gather more information and reduce bias in the evaluation process. Second, the Balanced Position Calibration strategy aggregates results across various orders to determine the final score. By considering different orders of appearance in the context, the evaluation becomes more balanced and less susceptible to manipulation. Finally, the Human-in-the-Loop Calibration strategy introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. This ensures that challenging examples are appropriately evaluated with human input, further enhancing the alignment with human judgments.","['The calibration framework improves alignment with human judgments by implementing three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration. These strategies ensure that the evaluator model generates multiple evaluation evidence, aggregates results across various orders, and measures the difficulty of each example. Additionally, the framework includes manual annotation of ""win/tie/lose"" outcomes from ChatGPT and Vicuna-13B, which helps mitigate evaluation bias and achieve closer alignment with human judgments.']",0.0,0.42857142857142855
How does the LLM-Augmenter system incorporate external knowledge to improve the performance of LLMs in generating responses?,"[""Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.""
 'Large Language Models Can Self-Improve Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.']","The LLM-Augmenter system incorporates external knowledge to improve the performance of LLMs in generating responses by using a set of plug-and-play modules. These modules enable the LLM to generate responses that are grounded in external knowledge, which can be stored in task-specific databases. This allows the LLM to access and utilize relevant information from external sources to enhance the quality and accuracy of its responses.","[""The LLM-Augmenter system incorporates external knowledge by using plug-and-play modules to augment a black-box LLM. This external knowledge is stored in task-specific databases and is used to ground the LLM's responses. Additionally, the system iteratively revises LLM prompts based on feedback generated by utility functions, such as the factuality score of a response. This incorporation of external knowledge and iterative revision helps improve the performance of LLMs in generating responses.""]",0.26666666666666666,0.5714285714285714
Can you explain how iterative prompt revisions contribute to the improvement of LLMs in generating responses in the LLM-Augmenter system?,"[""Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.""
 'Large Language Models Can Self-Improve Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.']","Iterative prompt revisions contribute to the improvement of LLMs in generating responses in the LLM-Augmenter system by using feedback generated by utility functions. These utility functions, such as the factuality score of a LLM-generated response, provide feedback on the quality of the model's responses. The LLM-Augmenter system then iteratively revises the prompts given to the LLM based on this feedback. By continuously refining and adjusting the prompts, the system aims to improve the model's responses over time. This iterative process helps to address the challenges of hallucinations and the inability to use external knowledge that are often associated with LLMs.","[""Iterative prompt revisions contribute to the improvement of LLMs in generating responses in the LLM-Augmenter system by using feedback generated by utility functions. The system revises the prompts given to the LLM based on this feedback, such as the factuality score of a LLM-generated response. This iterative process helps to enhance the model's responses by addressing issues like hallucinations. The effectiveness of LLM-Augmenter has been validated in task-oriented dialog and open-domain question answering scenarios, where it significantly reduces ChatGPT's hallucinations while maintaining the fluency and informativeness of its responses.""]",0.2,0.5714285714285714
"Which NLP tasks can GPT-3 perform effectively without updates or fine-tuning, as mentioned?","[""For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans.""
 'GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.']","GPT-3 can perform effectively on various NLP tasks without updates or fine-tuning, as mentioned in the context. These tasks include translation, question-answering, cloze tasks, unscrambling words, using a novel word in a sentence, and performing 3-digit arithmetic.","['GPT-3 can perform translation, question-answering, cloze tasks, on-the-fly reasoning, and domain adaptation effectively without updates or fine-tuning.']",0.0,1.0
What questions does the study on Attributed QA for large language models address and how well do current methods perform in terms of attribution?,"['Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Large language models (LLMs) have shown impressive results while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial in this setting. We formulate and study Attributed QA as a key first step in the development of attributed LLMs. We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures. We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development. Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).'
 'We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY.']",The study on Attributed QA for large language models addresses two key questions: How to measure attribution and how well do current state-of-the-art methods perform on attribution. The study proposes a reproducible evaluation framework for the task and benchmarks a broad set of architectures. It takes human annotations as a gold standard and shows that a correlated automatic metric is suitable for development. The experimental work provides concrete answers to these questions and gives insights into how to build LLMs with attribution.,['The study on Attributed QA for large language models addresses questions such as how to measure attribution and how well current state-of-the-art methods perform on attribution. The study proposes a reproducible evaluation framework for the task and benchmarks a broad set of architectures. The study also takes human annotations as a gold standard and shows that a correlated automatic metric is suitable for development. The experimental work provides concrete answers to the questions and gives some hints on how to build LLMs with attribution.'],0.125,1.0
What is the purpose of the LLM-Augmenter system in improving large language models?,"[""Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.""
 'Large Language Models Can Self-Improve Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.']","The purpose of the LLM-Augmenter system is to augment a black-box large language model (LLM) with a set of plug-and-play modules. This system aims to address the challenges faced by LLMs, such as generating hallucinations and their inability to use external knowledge. By using the LLM-Augmenter system, the LLM can generate responses that are grounded in external knowledge, stored in task-specific databases. Additionally, the system iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, such as the factuality score of a LLM-generated response. Overall, the LLM-Augmenter system aims to improve the performance of LLMs in real-world, mission-critical applications.","[""The purpose of the LLM-Augmenter system is to improve large language models by augmenting them with plug-and-play modules. This system helps the language model generate responses grounded in external knowledge and iteratively revises prompts to enhance the model's responses using feedback from utility functions. The LLM-Augmenter system specifically aims to reduce hallucinations in ChatGPT while maintaining the fluency and informativeness of its responses.""]",0.26666666666666666,1.0
What is the purpose of self-verification in large language models?,"['Large Language Models are reasoners with Self-Verification When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from incorrect CoT. 1'
 'Large Language Models Can Self-Improve Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.']","The purpose of self-verification in large language models is to improve the accuracy of their reasoning abilities. By using the conclusion of a chain of thought as a condition to build a new sample and asking the language model to re-predict the original conditions, self-verification helps the model avoid mistakes and achieve competitive reasoning performance. This method calculates an explainable verification score based on accuracy and can be particularly effective when using few-shot learning. Overall, self-verification helps large language models conduct explainable verification of their own conclusions and avoid interference from incorrect chain of thought.","['The purpose of self-verification in large language models is to improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. It allows the models to conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Additionally, self-verification helps the models avoid interference from incorrect conditions of truth (CoT).']",0.3125,0.6666666666666666
"Based on the context, how does Med-PaLM 2 compare to physicians in terms of clinical utility and evaluation axes?","['We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form""adversarial""questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.'
 'Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications.']","Med-PaLM 2 was preferred by physicians over physicians' answers on eight out of nine evaluation axes pertaining to clinical utility. Additionally, Med-PaLM 2 showed significant improvements compared to Med-PaLM on every evaluation axis. These results suggest that Med-PaLM 2 performs at a level comparable to or better than physicians in terms of clinical utility and evaluation axes.","['Based on the given context, Med-PaLM 2 is preferred by physicians over physicians\' answers in terms of clinical utility on eight out of nine evaluation axes. Additionally, Med-PaLM 2 shows significant improvements compared to Med-PaLM on every evaluation axis when tested with long-form ""adversarial"" questions. These results indicate that Med-PaLM 2 is making rapid progress towards achieving physician-level performance in medical question answering.']",0.5,1.0
What is ChatGPT's performance on the USMLE exams without specialized training or reinforcement?,"['Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student.'
 'Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.']","ChatGPT's performance on the USMLE exams without specialized training or reinforcement is at or near the passing threshold for all three exams (Step 1, Step 2CK, and Step 3).","[""ChatGPT's performance on the USMLE exams without specialized training or reinforcement was at or near the passing threshold for all three exams.""]",0.16666666666666666,1.0
"Which GPT-4 powered planner shows greater consistency and rationality in tool selection by inferring constraints from instructions, compared to a ChatGPT powered planner?","['Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.'
 'Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student.']","The GPT-4-powered planner shows greater consistency and rationality in tool selection by inferring constraints from instructions, compared to a ChatGPT-powered planner.","['The GPT-4 powered planner shows greater consistency and rationality in tool selection by inferring constraints from instructions, compared to a ChatGPT powered planner.']",0.1111111111111111,0.09090909090909091
What technique significantly improves zero-shot performances of large language models on diverse reasoning tasks?,"['The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.'
 'Large Language Models are Zero-Shot Reasoners Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs\' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding""Let\'s think step by step""before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM.']","Adding the prompt ""Let's think step by step"" significantly improves zero-shot performances of large language models on diverse reasoning tasks.",['The technique that significantly improves zero-shot performances of large language models on diverse reasoning tasks is chain of thought (CoT) prompting.'],0.16666666666666666,1.0
What are the limitations of our model in handling long chains of operations and binding operations to variables?,"['Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.'
 'Chain of Thought Prompting Elicits Reasoning in Large Language Models We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.']",The limitations of the model include difficulty with docstrings describing long chains of operations and with binding operations to variables.,['The limitations of our model include difficulty with docstrings describing long chains of operations and with binding operations to variables.'],0.25,1.0
