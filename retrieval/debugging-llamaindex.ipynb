{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1409d882",
   "metadata": {},
   "source": [
    "# Debugging LlamaIndex\n",
    "\n",
    "I've been developing RAG(Retrieval Augmented Generation) apps with Llamaindex and helping develop/contributing to the main project as well. Over time I've been learning a few tricks here and there to helps debug the pipelines I build more effectively. This is a collection of all my tips and tricks\n",
    "\n",
    "\n",
    "## Content\n",
    "### 1. [Tracing your steps with `CallbackManager`](#Tracing-your-steps-with-CallbackManager)\n",
    "### 2. [Furthur Explorations with `LlamaDebugHandler`](#Furthur-Explorations-with-LlamaDebugHandler)\n",
    "### 3. [Setting up wandb for experiment tracking and tracing](#Setting-up-wandb-for-experiment-tracking-and-tracing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ae349",
   "metadata": {},
   "source": [
    "# Tracing your steps with `CallbackManager`\n",
    "\n",
    "Using the `CallbackManager` allows you to trace the steps llamaindex takes to generate the response and the time each step took. \n",
    "\n",
    "For this example lets compare 2 indices `ListIndex` and `VectorIndex` and see how the outputs are.\n",
    "\n",
    "but first things first, lets import everything and init a service_context that uses the callback_manager we created. Then we use `set_global_service_context` to use that service context throught."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98938490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ListIndex, VectorStoreIndex, ServiceContext, LLMPredictor\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler, CBEventType\n",
    "\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    ")\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    callback_manager=callback_manager, \n",
    "    llm_predictor=llm_predictor\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "310d0182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "docs = SimpleDirectoryReader(\"./what_i_worked_on_pg/\").load_data()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986e918",
   "metadata": {},
   "source": [
    "Now lets create the first index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f0ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.126203 seconds\n",
      "      |_chunking ->  0.125512 seconds\n",
      "    |_embedding ->  1.223936 seconds\n",
      "    |_embedding ->  1.190142 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47a9b0",
   "metadata": {},
   "source": [
    "and vola! you can see the traces in actions. Here you can see the different steps the index construction took. You can see that the LlamaIndex made 2 calls to the embedding endpoint to create the embeddings for the chunks.\n",
    "\n",
    "Now lets try `ListIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7546bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.125731 seconds\n",
      "      |_chunking ->  0.125057 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "list_index = ListIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65bf9c",
   "metadata": {},
   "source": [
    "And we have a different output. If you know `ListIndexes` you know that they don't have embeddings instead just chunk the docs and store.\n",
    "\n",
    "That was index creation but how about query time? Lets find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4b5f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util func to help me :)\n",
    "def query(question, index):\n",
    "    qe = index.as_query_engine()\n",
    "    r = qe.query(question)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1575cca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.269069 seconds\n",
      "      |_retrieve ->  0.70633 seconds\n",
      "        |_embedding ->  0.696948 seconds\n",
      "      |_synthesize ->  1.562535 seconds\n",
      "        |_llm ->  1.542432 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "r = query(\"what did the author do growing up?\", vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713201e",
   "metadata": {},
   "source": [
    "Here you can see the steps LlamaIndex took.\n",
    "\n",
    "Lets try something a bit more complicated, like our `ListIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46a7422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  206.851148 seconds\n",
      "      |_retrieve ->  0.019339 seconds\n",
      "      |_synthesize ->  206.831567 seconds\n",
      "        |_llm ->  5.387623 seconds\n",
      "        |_llm ->  11.923166 seconds\n",
      "        |_llm ->  15.879682 seconds\n",
      "        |_llm ->  11.992212 seconds\n",
      "        |_llm ->  20.846005 seconds\n",
      "        |_llm ->  22.670804 seconds\n",
      "        |_llm ->  36.684416 seconds\n",
      "        |_llm ->  25.109205 seconds\n",
      "        |_llm ->  26.02517 seconds\n",
      "        |_llm ->  30.040593 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "r = query(\"what did the author do growing up?\", list_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a48b86",
   "metadata": {},
   "source": [
    "As you can imagine this is very neat tool to get a better understanding of what are the steps that happening internally with LlamaIndex. Especially usefull when debugging, something I've turned to time and time again when building complex indexes with LlamaIndex.\n",
    "\n",
    "but what if you want a bit more info?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333edf66",
   "metadata": {},
   "source": [
    "# Furthur Explorations with `LlamaDebugHandler`\n",
    "\n",
    "(tomorrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eedaa3",
   "metadata": {},
   "source": [
    "# Setting up wandb for experiment tracking and tracing\n",
    "\n",
    "(soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23625f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
