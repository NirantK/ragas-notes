{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81dc5",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "* Input set of documents\n",
    "    - number of samples to generate\n",
    "    - maximum token length (input + context + answer)\n",
    "    - difficulty distribution \n",
    "    - Seed questions? \n",
    "    - randomize answer output formats\n",
    "* Output : test set with questions,contexts,answer\n",
    "\n",
    "1. Select document and part of document to frame question from (random)\n",
    "2. Formulate a question,context pair contrained on output format. \n",
    "3. Identify difficulty type \n",
    "4. Evol question using evol-instruct like paradigm to improve difficulty \n",
    "    - ask for improved reasoning - (Evol instruct C Increased Reasoning Steps Prompt)\n",
    "    - Add more contexts and frame question for multi hop\n",
    "        - Adding more context\n",
    "           1. Identiy entity from current context and retrive paras with same entity : formulate new question\n",
    "           2. Identify similar paras using sentence similarity : formulate new question\n",
    "           \n",
    "           \n",
    "#### Ideas to increase complextity\n",
    "\n",
    "- Extend question by using info in the newly added contexts.\n",
    "\n",
    "\n",
    "- Concretizing\n",
    "\n",
    "    In RAG case,this should yeild an instruction that is related to real world use-case on concept decribed in the context. \n",
    "    For example, if context speaks about different types of instructions and their use-cases\n",
    "    question will be like \"I want to train a chatbot, how to form training dataset?\"\n",
    "\n",
    "\n",
    "- Improved reasoning\n",
    "           \n",
    "- Reasoning over multiple contexts\n",
    "    - add context1, context2, context3, etc untill max tokens - x. And then ask model to formulate a question that would require reasoning over multiple contexts. \n",
    "    \n",
    "- Condition the question - introduce new conditions\n",
    "    \n",
    "### Question filtering\n",
    "- Follow question elimination trick from evol-instruct\n",
    "- Remove phrases like 'according to given question,etc from'\n",
    "- Simplify question and decrease length \n",
    "- Length upper cut of questions (~20)\n",
    "    \n",
    "#### Open-issues\n",
    "1. How to ensure inter-document dependancy is null\n",
    "\n",
    "A question framed might have possible answers from different documents\n",
    "\n",
    "2. Questions framed by LLM are easily answerable. \n",
    "\n",
    "It feels like LLM first identifies a candidate sentence and frames question based on it. So by default the answer to questions can be located easily.\n",
    "\n",
    "3. GPT 3.5 tends to add or increase length of question when asked to create difficult question\n",
    "\n",
    "often this ends up as a unsually long question that contains two questions\n",
    "\n",
    "4. Bad content : some pages / parts of document will contain bad contents like aknowledgements, examples, etc. Those should be deprioritized.\n",
    "    - This can be partly handled by a good chunking mechanism - each part should contains atlest 2k tokens. \n",
    "    - Some overlap - split intelligently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e9980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91ccf9e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4420177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open(\"/Users/shahules/openai-key.json\"))[\"ikka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0dcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d67a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "Embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcbd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbour(text, text_list, min_similarity=0.8, top_k=3):\n",
    "        text_vec = np.asarray(Embedding.embed_query(text)).reshape(1, -1)\n",
    "        text_list_vec = np.asarray(\n",
    "            Embedding.embed_documents(text_list)\n",
    "        )\n",
    "        norm = np.linalg.norm(text_list_vec, axis=1) * np.linalg.norm(\n",
    "            text_vec, axis=1\n",
    "        )\n",
    "        similarity =  (\n",
    "            np.dot(text_list_vec, text_vec.T).reshape(\n",
    "                -1,\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        similarity  = similarity[similarity>min_similarity]\n",
    "        return similarity.argsort().tolist()[-top_k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd578ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "split long documents to max length chunks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def doc_filter(corpus,top_k=20):\n",
    "    vectorizer = TfidfVectorizer(min_df=0.05,max_df=0.5)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X = X.mean(axis=1).reshape(1,-1).argsort().tolist()[0]\n",
    "    return X[-top_k:][::-1]\n",
    "\n",
    "def split_text(text, max_length=1000):\n",
    "        chunks = []\n",
    "        corpus = text.split('\\n')\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        for chunk in corpus:\n",
    "            \n",
    "            if len(current_chunk + chunk)//4 < max_length:\n",
    "                current_chunk = \"\\n\".join([current_chunk,chunk])\n",
    "            else:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = chunk\n",
    "            \n",
    "        return chunks\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff761f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e91820",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb673dd",
   "metadata": {},
   "source": [
    "- random select document \n",
    "- Identify sections from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dadf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKLIST = [\"Based on the provided context\",'this context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../arxiv-llm/textdata\"\n",
    "text = read_doc(\"../arxiv-llm/textdata/2303.18223v11.A_Survey_of_Large_Language_Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7034b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = split_text(text)\n",
    "top_docs = doc_filter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d594b8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 13, 83, 8, 84, 3, 0, 6, 82, 33, 69, 77, 80, 19, 63, 52, 42, 23, 18]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2403987",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_formulate = \"\"\"\n",
    "Your task is to formulate a question from given context satifying the rules given below:\n",
    "    1.The question should be fully answered from the given context and make of use of abbreviation wherever possible.\n",
    "    2.The question should be framed from a part of context that contains important information. It can also be from tables,code,etc.\n",
    "    3.The answer to the question should not contain any links. \n",
    "    4.The question should be of moderate difficulty.\n",
    "    5.The question must be reasonable and must be understood and responded by humans.\n",
    "    6.Do no use phrases like 'provided context',etc in the question\n",
    "    7.Avoid framing question using word \"and\" that can be decomposed into more than one question.\n",
    "    \n",
    "context:{context}\n",
    "\"\"\"\n",
    "\n",
    "context_formulate =  \"\"\"\\\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract sentences from given context that can be used to answer the question. \n",
    "Rules to follow while doing this task:\n",
    "    1. Your task is not to answer the question using given context but to extract sentences from given context that can be used to answer given question.\n",
    "    2. The sentences could be anywhere in the provided context, pay attention to the whole context.\n",
    "    3. While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "    4. If the answer is not present in context, you should only return the empty string.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\" \n",
    "\n",
    "context_formulatev1 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information.\"\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "context_formulatev2 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "answer_formulate = \"\"\"\n",
    "Asnwer the question using the information from the qiven context. \n",
    "question:{question}\n",
    "context:{context}\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "context_from_answer = \"\"\"\n",
    "Given question, context and answer. Locate the relevant information in the context from context that was to used to form the given answer. \n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "answer:\\n{answer}\n",
    "extracted context:\"\"\"\n",
    "\n",
    "\n",
    "content_filtering = \"\"\"\n",
    "Identify if the following content is suitable for framing a high quality question. The content should be deemed unsuitable if the content is dominated by credits, examples, disclaimer, references,acknowledgments.etc.\n",
    "content:\\n{context}\n",
    "\"\"\"\n",
    "# answer_index_formulate = \"\"\"\n",
    "# Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
    "# You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
    "# \\n\\n\n",
    "# question:\\nWhen was Einstein born?\n",
    "# context:\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "# answer: the answer to the given question can be located between character index 0 and 155 of given context.  \n",
    "# question:\\n{question}\n",
    "# context:\\n{context}\n",
    "# answer:\n",
    "# \"\"\"\n",
    "\n",
    "resoning_single_context = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context. Ensure that the question can be answered entirely from the information present in the context. Do not frame questions that contains more than 15 words.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "resoning_multi_context = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context1 and context2. Ensure that the question can be answered entirely from the information present in the contexts. Do not frame questions that contains more than 15 words.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_multi_context = \"\"\"\n",
    " You are a prompt rewriter. You will be provided with a question and two set of contexts namely context1 and context2.\n",
    " Your task to is to complicate the given question to improve the difficulty of answering. You should do it by rewriting it into a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context1 and context2. Ensure that the question can be answered entirely from the information present in the contexts. Do not frame questions that contains more than 15 words.\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "reasoning = \"\"\"\n",
    "You are a prompt rewriter. You will be provided with a question and a long context.Your task to is to complicate the given question to improve the difficulty of answering. \n",
    "You should do complicate the question by rewriting question into a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in given context. \n",
    "Here are some strategies to create multi-hop questions:\n",
    "\n",
    "   - Bridge related entities: Identify information that relates specific entities and frame question that can be answered only by analysing information of both entities.\n",
    "   \n",
    "   - Use Pronouns: identify (he, she, it, they) that refer to same entity or concepts in the context, and ask questions that would require the reader to figure out what pronouns refer to.\n",
    "\n",
    "   - Refer to Specific Details: Mention specific details or facts from different parts of the context including tables, code, etc and ask how they are related.\n",
    "\n",
    "   - Pose Hypothetical Scenarios: Present a hypothetical situation or scenario that requires combining different elements from the context to arrive at an answer.\n",
    "\n",
    "Rules to follow when rewriting question:\n",
    "1. Ensure that the rewritten question can be answered entirely from the information present in the contexts.\n",
    "2. Do not frame questions that contains more than 15 words. Use abbreviation wherever possible.\n",
    "3. Make sure the question is clear and unambiguous. \n",
    "4. phrases like 'based on the provided context','according to the context',etc are not allowed to appear in the question.\n",
    "\n",
    "question: {question}\n",
    "CONTEXTS:\n",
    "{context}\n",
    "\n",
    "Multi-hop Reasoning Question:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reasoning_question_reformulte = \"\"\"\n",
    "Rewrite the following question to make it more indirect while retaining the essence of the original question. The goal is to create a question that conveys the same meaning but in a less direct manner.\n",
    "\n",
    "Original Question:\n",
    "{question}\n",
    "\n",
    "Indirectly Rewritten Question:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4fc606e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = corpus[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6280e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = question_formulate.format(context=context)\n",
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "41845c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bdb2f964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are three major quantization libraries for LLMs and what are their main features?'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a23527f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neigh = find_neighbour(context,corpus[top_docs[i]+1:top_docs[i]+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b45227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context2 = corpus[top_docs[i]+top_neigh[0]]\n",
    "# full_context = \"\".join([f\"\\n\\ncontext{i+1}\\n\\n{ctx}\" for i,ctx in enumerate([context,context2])])\n",
    "full_context = \"\\n\\n\".join([context,context2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "921580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning.format(question=question,context=full_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2e211024",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "af8f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionv1 = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "52c7cb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the three important kinds of general data used for pre-training LLMs, and how do they affect the performance of LLMs on downstream tasks?'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fba30167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "techniques in BPE, such as NFKC [182], may degrade the\n",
      "tokenization performance [34, 59, 69]. When extending\n",
      "existing LLMs ( i.e., continual pre-training or instruction\n",
      "tuning), we should be also aware of the potential side effect\n",
      "with customized tokenizers. For example, LLaMA trains\n",
      "the BPE tokenizer based on a pre-training corpus mainly\n",
      "consisting of English texts, and the derived vocabulary\n",
      "might be less capable in processing non-English data, e.g.,\n",
      "taking longer inference latency to generate Chinese texts.\n",
      "4.1.3 Effect of Pre-training Data on LLMs\n",
      "Unlike small-scale PLMs, it is usually infeasible to iterate\n",
      "the pre-training of LLMs multiple times, due to the huge\n",
      "demand for computational resources. Thus, it is particularly\n",
      "important to construct a well-prepared pre-training corpus\n",
      "before training a LLM. In this part, we discuss how the qual-\n",
      "ity and distribution of the pre-training corpus potentially\n",
      "influence the performance of LLMs.\n",
      "Mixture of Sources. As discussed before, pre-training data\n",
      "from different domains or scenarios has distinct linguistic\n",
      "characteristics or semantic knowledge. By pre-training on a\n",
      "mixture of text data from diverse sources, LLMs can acquire\n",
      "a broad scope of knowledge and may exhibit a strong gen-\n",
      "eralization capacity. Thus, when mixing different sources, it\n",
      "is suggested to include as many high-quality data sources\n",
      "as possible, and carefully set the distribution of pre-training\n",
      "data, since it is also likely to affect the performance of LLMs\n",
      "on downstream tasks [59]. Gopher [59] conducts the ablation\n",
      "experiment on data distribution to examine the impact of\n",
      "mixed sources on downstream tasks. Experimental results\n",
      "on the LAMBADA dataset [183] show that increasing the\n",
      "proportion of books data can improve the capacity of the\n",
      "model in capturing long-term dependencies from text, and\n",
      "increasing the proportion of the C4 dataset [73] leads to\n",
      "performance improvement on the C4 validation dataset [59].\n",
      "However, as a side effect, training on excessive data about a\n",
      "certain domain would affect the generalization capability ofLLMs on other domains [35, 59]. Therefore, it is suggested\n",
      "that researchers should carefully determine the proportion\n",
      "of data from different domains in the pre-training corpus, in\n",
      "order to develop LLMs that better meet their specific needs.\n",
      "The readers can refer to Figure 5 for a comparison of the\n",
      "data sources for different LLMs.\n",
      "Amount of Pre-training Data. For pre-training an effective\n",
      "LLM, it is important to collect sufficient high-quality data\n",
      "that satisfies the data quantity demand of the LLM. Exist-\n",
      "ing studies have found that with the increasing parameter\n",
      "scale in the LLM, more data is also required to train the\n",
      "model [34, 57]: a similar scaling law as model size is also\n",
      "observed in data size, with respect to model performance.\n",
      "A recent study has shown that a number of existing LLMs\n",
      "suffer from sub-optimal training due to inadequate pre-\n",
      "training data [34]. By conducting extensive experiments, it\n",
      "further demonstrates increasing the model size and data size\n",
      "in equal scales can lead to a more compute-efficient model\n",
      "(i.e., the Chinchilla model), for a given compute budget.\n",
      "More recently, LLaMA [57] shows that with more data\n",
      "and longer training, smaller models can also achieve good\n",
      "performance. Overall, it is suggested that researchers should\n",
      "pay more attention to the amount of high-quality data for\n",
      "adequately training the model, especially when scaling the\n",
      "model parameters.\n",
      "Quality of Pre-training Data. Existing work has shown\n",
      "that pre-training on the low-quality corpus, such as noisy,\n",
      "toxic, and duplicate data, may hurt the performance of\n",
      "models [59, 169, 171, 174]. For developing a well-performing\n",
      "LLM, it is crucial to consider both the quantity and the\n",
      "quality of the collected training data. Recent studies, such\n",
      "as T5 [73], GLaM [96], and Gopher [59], have investigated\n",
      "the influence of data quality on the performance of down-\n",
      "stream tasks. By comparing the performance of models\n"
     ]
    }
   ],
   "source": [
    "print(context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "290bdf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questionv1.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0e7d6e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "context1\n",
      "\n",
      "representations) and then fine-tuning the biLSTM network\n",
      "according to specific downstream tasks. Further, based on\n",
      "the highly parallelizable Transformer architecture [22] with\n",
      "self-attention mechanisms, BERT [23] was proposed by pre-\n",
      "training bidirectional language models with specially de-\n",
      "signed pre-training tasks on large-scale unlabeled corpora.\n",
      "These pre-trained context-aware word representations are\n",
      "very effective as general-purpose semantic features, which\n",
      "have largely raised the performance bar of NLP tasks. This\n",
      "study has inspired a large number of follow-up work, which\n",
      "sets the “ pre-training and fine-tuning ” learning paradigm.\n",
      "Following this paradigm, a great number of studies on\n",
      "PLMs have been developed, introducing either different\n",
      "architectures [24, 25] ( e.g., GPT-2 [26] and BART [24]) or\n",
      "improved pre-training strategies [27–29]. In this paradigm, it\n",
      "often requires fine-tuning the PLM for adapting to different\n",
      "downstream tasks.\n",
      "•Large language models (LLM) . Researchers find that\n",
      "scaling PLM ( e.g., scaling model size or data size) often\n",
      "leads to an improved model capacity on downstream tasks\n",
      "(i.e.,following the scaling law [30]). A number of studies\n",
      "have explored the performance limit by training an ever\n",
      "larger PLM ( e.g., the 175B-parameter GPT-3 and the 540B-\n",
      "parameter PaLM). Although scaling is mainly conducted\n",
      "in model size (with similar architectures and pre-training\n",
      "tasks), these large-sized PLMs display different behaviors\n",
      "from smaller PLMs ( e.g., 330M-parameter BERT and 1.5B-\n",
      "parameter GPT-2) and show surprising abilities (called emer-\n",
      "gent abilities [31]) in solving a series of complex tasks. For\n",
      "example, GPT-3 can solve few-shot tasks through in-contextlearning , whereas GPT-2 cannot do well. Thus, the research\n",
      "community coins the term “ large language models (LLM) ”1\n",
      "for these large-sized PLMs [32–35], which attract increasing\n",
      "research attention (See Figure 1). A remarkable application\n",
      "of LLMs is ChatGPT2that adapts the LLMs from the GPT\n",
      "series for dialogue, which presents an amazing conversation\n",
      "ability with humans. We can observe a sharp increase of the\n",
      "arXiv papers that are related to LLMs after the release of\n",
      "ChatGPT in Figure 1.\n",
      "In the existing literature, PLMs have been widely dis-\n",
      "cussed and surveyed [36–39], while LLMs are seldom re-\n",
      "viewed in a systematic way. To motivate our survey, we first\n",
      "highlight three major differences between LLMs and PLMs.\n",
      "First, LLMs display some surprising emergent abilities that\n",
      "may not be observed in previous smaller PLMs. These abili-\n",
      "ties are key to the performance of language models on com-\n",
      "plex tasks, making AI algorithms unprecedently powerful\n",
      "and effective. Second, LLMs would revolutionize the way\n",
      "that humans develop and use AI algorithms. Unlike small\n",
      "PLMs, the major approach to accessing LLMs is through\n",
      "the prompting interface ( e.g., GPT-4 API). Humans have to\n",
      "understand how LLMs work and format their tasks in a way\n",
      "that LLMs can follow. Third, the development of LLMs no\n",
      "longer draws a clear distinction between research and en-\n",
      "gineering. The training of LLMs requires extensive practical\n",
      "experiences in large-scale data processing and distributed\n",
      "parallel training. To develop capable LLMs, researchers\n",
      "have to solve complicated engineering issues, working with\n",
      "engineers or being engineers.\n",
      "Nowadays, LLMs are posing a significant impact on\n",
      "the AI community, and the advent of ChatGPT and GPT-4\n",
      "leads to the rethinking of the possibilities of artificial general\n",
      "intelligence (AGI). OpenAI has published a technical article\n",
      "entitled “ Planning for AGI and beyond ”, which discusses\n",
      "the short-term and long-term plans to approach AGI [40],\n",
      "1. Note that a LLM is not necessarily more capable than a small PLM,\n",
      "and emergent abilities may not occur in some LLMs.\n",
      "2. https://openai.com/blog/chatgpt/\n",
      "\n",
      "3\n",
      "and a more recent paper has argued that GPT-4 might be\n",
      "considered as an early version of an AGI system [41]. The\n",
      "\n",
      "context2\n",
      "\n",
      "marks) when applied to PaLM and LaMDA variants with\n",
      "a model size larger than 60B, while its advantage over\n",
      "the standard prompting becomes more evident when the\n",
      "model size exceeds 100B. Furthermore, the performance\n",
      "7. It is difficult to accurately examine the critical size for emergent\n",
      "abilities of LLMs ( i.e.,the minimum size to possess an ability), since it\n",
      "might vary for different models or tasks. Also, existing studies often\n",
      "test emergent abilities on very limited model sizes for a specific LLM.\n",
      "For example, PaLM is often tested with three sizes of 8B, 62B and 540B.\n",
      "It is unclear about the model performance of the untested sizes.\n",
      "8. In a recent study [60], it also shows that in-context learning implic-\n",
      "itly performs meta-optimization through the attention mechanism.\n",
      "\n",
      "5\n",
      "improvement with CoT prompting seems to be also varied\n",
      "for different tasks, e.g., GSM8K >MAWPS >SWAMP for\n",
      "PaLM [33].\n",
      "Key Techniques for LLMs . It has been a long way that\n",
      "LLMs evolve into the current state: general and capable\n",
      "learners. In the development process, a number of impor-\n",
      "tant techniques are proposed, which largely improve the\n",
      "capacity of LLMs. Here, we briefly list several important\n",
      "techniques that (potentially) lead to the success of LLMs, as\n",
      "follows.\n",
      "•Scaling . As discussed in previous parts, there exists\n",
      "an evident scaling effect in Transformer language mod-\n",
      "els: larger model/data sizes and more training compute\n",
      "typically lead to an improved model capacity [30, 34]. As\n",
      "two representative models, GPT-3 and PaLM explored the\n",
      "scaling limits by increasing the model size to 175B and\n",
      "540B, respectively. Since compute budget is usually limited,\n",
      "scaling laws can be further employed to conduct a more\n",
      "compute-efficient allocation of the compute resources. For\n",
      "example, Chinchilla (with more training tokens) outper-\n",
      "forms its counterpart model Gopher (with a larger model\n",
      "size) by increasing the data scale with the same compute\n",
      "budget [34]. In addition, data scaling should be with careful\n",
      "cleaning process, since the quality of pre-training data plays\n",
      "a key role in the model capacity.\n",
      "•Training . Due to the huge model size, it is very chal-\n",
      "lenging to successfully train a capable LLM. Distributed\n",
      "training algorithms are needed to learn the network param-\n",
      "eters of LLMs, in which various parallel strategies are of-\n",
      "ten jointly utilized. To support distributed training, several\n",
      "optimization frameworks have been released to facilitate\n",
      "the implementation and deployment of parallel algorithms,\n",
      "such as DeepSpeed [65] and Megatron-LM [66–68]. Also, op-\n",
      "timization tricks are also important for training stability and\n",
      "model performance, e.g., restart to overcome training loss\n",
      "spike [56] and mixed precision training [69]. More recently,\n",
      "GPT-4 [46] proposes to develop special infrastructure and\n",
      "optimization methods that reliably predict the performance\n",
      "of large models with much smaller models.\n",
      "•Ability eliciting . After being pre-trained on large-scale\n",
      "corpora, LLMs are endowed with potential abilities as\n",
      "general-purpose task solvers. These abilities might not be\n",
      "explicitly exhibited when LLMs perform some specific tasks.\n",
      "As the technical approach, it is useful to design suitable task\n",
      "instructions or specific in-context learning strategies to elicit\n",
      "such abilities. For instance, chain-of-thought prompting has\n",
      "been shown to be useful to solve complex reasoning tasks\n",
      "by including intermediate reasoning steps. Furthermore,\n",
      "we can perform instruction tuning on LLMs with task\n",
      "descriptions expressed in natural language, for improving\n",
      "the generalizability of LLMs on unseen tasks. These eliciting\n",
      "techniques mainly correspond to the emergent abilities of\n",
      "LLMs, which may not show the same effect on small lan-\n",
      "guage models.\n",
      "•Alignment tuning . Since LLMs are trained to capture\n",
      "the data characteristics of pre-training corpora (including\n",
      "both high-quality and low-quality data), they are likely to\n",
      "generate toxic, biased, or even harmful content for humans.\n"
     ]
    }
   ],
   "source": [
    "print(full_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dfd6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7503b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = answer_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "889d4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cada2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "303fbf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Python, variable names can be written by using alphanumeric characters and underscores. Variable names cannot start with a number and cannot contain spaces or special characters.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b752c",
   "metadata": {},
   "source": [
    "### Complicating instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af24ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1\n",
    "## includes info in context2 and adds an 'and' part to the original question\n",
    "\n",
    "multi_context = \"\"\"\n",
    "You are a prompt rewriter. You will be provided with a question and two set of contexts namely context1 and context2. \n",
    "Your task is to complicate the given question in a way that answering it requires information derived from both context1 and context2. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be very long. Use abbreviation wherever possible.\n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present in context1 and context2. \n",
    "    4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\n",
    "    5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "\n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_part = \"\"\"\n",
    "Please rephrase the following multi-part question into a single, shortened and comprehensive question that encompasses all the key elements.\n",
    "question:\\n{question}\n",
    "rewritten question:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "65432c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [137]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt_input \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_context\u001b[49m\u001b[38;5;241m.\u001b[39mformat(question\u001b[38;5;241m=\u001b[39mquestion,context1\u001b[38;5;241m=\u001b[39mextracted_context,context2\u001b[38;5;241m=\u001b[39mcorpus[all_contexts[\u001b[38;5;241m2\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_context' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_input = multi_context.format(question=question,context1=extracted_context,context2=all_contexts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "240a8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionv1 = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53689adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs, and how do factors such as scaling the instructions and formatting design impact their performance?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a95156",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(single_part.format(question=questionv1),temperature=0)\n",
    "questionv1_compact = output['choices'][0]['message']['content']\n",
    "questionv1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af504e",
   "metadata": {},
   "source": [
    "### reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3238e227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 13, 83, 8, 84, 3, 0, 6, 82, 33, 69, 77, 80, 19, 63, 52, 42, 23, 18]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b2884069",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8397919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning.format(reasoning_type=resoning_single_context,context=corpus[top_docs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8f5a690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bb70ce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7w8iLF1QO2bXaFdwQtLb8iuMefLmL at 0x7fc9e9e3c5e0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are some deficiencies of existing prompting approaches and what strategies have been proposed to address them?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1694090877,\n",
       "  \"id\": \"chatcmpl-7w8iLF1QO2bXaFdwQtLb8iuMefLmL\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 18,\n",
       "    \"prompt_tokens\": 1299,\n",
       "    \"total_tokens\": 1317\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2caeae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in order to detect early abnormal issues during training.\n",
      "Furthermore, it also calls for more flexible mechanisms of\n",
      "hardware support or resource schedule, so as to better\n",
      "organize and utilize the resources in a computing cluster.\n",
      "Since it is very costly to pre-train a LLM from scratch, it\n",
      "is important to design a suitable mechanisms for continu-\n",
      "ally pre-training or fine-tuning the LLM based on publicly\n",
      "available model checkpoints ( e.g., LLaMA [57] and Flan-\n",
      "T5 [64]). For this purpose, a number of technical issues\n",
      "have to be resolved, e.g., catastrophic forgetting and task\n",
      "specialization. However, to date, there still lack open-source\n",
      "model checkpoints for LLMs with complete pre-processing\n",
      "and training logs ( e.g., the scripts to prepare the pre-training\n",
      "data) for reproduction. We believe that it will be of great\n",
      "value to report more technical details in open-source models\n",
      "for the research of LLMs. Furthermore, it is also important\n",
      "\n",
      "57\n",
      "to develop more improvement tuning strategies that effec-\n",
      "tively elicits the model abilities.\n",
      "Model Utilization. Since fine-tuning is very costly in real\n",
      "applications, prompting has become the prominent approach\n",
      "to using LLMs. By combining task descriptions and demon-\n",
      "stration examples into prompts, in-context learning (a spe-\n",
      "cial form of prompting) endows LLMs with the ability to\n",
      "perform well on new tasks, even outperforming full-data\n",
      "fine-tuned models in some cases. Furthermore, to enhance\n",
      "the ability of complex reasoning, advanced prompting tech-\n",
      "niques have been proposed, exemplified by the chain-of-\n",
      "thought (CoT) strategy, which includes the intermediate\n",
      "reasoning steps into prompts. However, existing prompt-\n",
      "ing approaches still have several deficiencies described as\n",
      "follows. Firstly, it involves considerable human efforts in\n",
      "the design of prompts. It would be quite useful to au-\n",
      "tomatically generate effective prompts for solving various\n",
      "tasks. Secondly, some complex tasks ( e.g., formal proof and\n",
      "numerical computation) require specific knowledge or logic\n",
      "rules, which may not be well expressed in natural language\n",
      "or demonstrated by examples. Thus, it is important to\n",
      "develop more informative, flexible task formatting methods\n",
      "for prompts46. Thirdly, existing prompting strategies mainly\n",
      "focus on single-turn performance. It is useful to develop\n",
      "interactive prompting mechanisms ( e.g., through natural\n",
      "language conversations) for solving complex tasks, which\n",
      "have been demonstrated to be very useful by ChatGPT.\n",
      "Safety and Alignment. Despite their capacities, LLMs pose\n",
      "similar safety challenges as small language models. For\n",
      "example, LLMs exhibit a tendency to generate hallucina-\n",
      "tions [448], which are texts that seem plausible but may be\n",
      "factually incorrect. What is worse, LLMs might be elicited by\n",
      "intentional instructions to produce harmful, biased, or toxic\n",
      "texts for malicious systems, leading to the potential risks of\n",
      "misuse [55, 61]. To have a detailed discussion of the safety\n",
      "issues of LLMs ( e.g., privacy, overreliance, disinformation,\n",
      "and influence operations), the readers can refer to the GPT-\n",
      "3/4 technical reports [46, 55]. As the major approach to\n",
      "averting these issues, reinforcement learning from human\n",
      "feedback (RLHF) [61, 100] has been widely used by in-\n",
      "corporating humans in the training loop for developing\n",
      "well-aligned LLMs. To improve the model safety, it is also\n",
      "important to include safety-relevant prompts during RLHF,\n",
      "as shown by GPT-4 [46]. However, RLHF heavily relies\n",
      "on high-quality human feedback data from professional\n",
      "labelers, making it difficult to be properly implemented in\n",
      "practice. Therefore, it is necessary to improve the RLHF\n",
      "framework for reducing the efforts of human labelers and\n",
      "seek a more efficient annotation approach with guaranteed\n",
      "data quality, e.g., LLMs can be employed to assist the\n",
      "labeling work. More recently, red teaming [115, 269] has\n",
      "been adopted for improving the model safety of LLMs,\n",
      "which utilizes the collected adversarial prompts to refine\n"
     ]
    }
   ],
   "source": [
    "print(corpus[top_docs[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c226a42",
   "metadata": {},
   "source": [
    "## Concretizing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "245ab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concretizing \n",
    "Concretize_prompt = \"\"\"\n",
    "Concretize the following question by providing a specific, real-world scenario or example that illustrates the concept or situation described in the question. The goal is to make the question more tangible and relatable.\n",
    "\n",
    "question:\\n{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1f318f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "for i in np.random.randint(0,len(top_docs),2):\n",
    "    print(i)\n",
    "    context = corpus[top_docs[i]]\n",
    "    prompt_input = question_formulate.format(context=context)\n",
    "    output = llm2(prompt_input)\n",
    "    question = output['choices'][0]['message']['content']\n",
    "    prompt_input = Concretize_prompt.format(question=question,context=context)\n",
    "    output = llm2(prompt_input)\n",
    "    question_c = output['choices'][0]['message']['content']\n",
    "    data_list.append({\n",
    "        \"question\":question,\n",
    "        \"complex\":question_c\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ec7eb6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35,  0])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,len(corpus),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fbb40602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are the commonly used model architectures for pre-training LLMs?',\n",
       "  'complex': \"In the field of natural language processing, one commonly used model architecture for pre-training Language Model (LLMs) is the Transformer model. For example, let's say a team of researchers is working on developing a chatbot that can understand and generate human-like responses. They decide to pre-train their LLM using a Transformer architecture. They feed the model with a large corpus of text data, such as books, articles, and internet forums, to learn the patterns and structures of language. This pre-training phase helps the model to capture the nuances of language and develop a strong language understanding capability. Once the pre-training is complete, the researchers fine-tune the model on a specific task, such as chatbot response generation, to make it more specialized and accurate in generating appropriate responses in a conversational context.\"},\n",
       " {'question': 'What is the dominant method for generating human feedback data in existing work?',\n",
       "  'complex': \"In the field of product development, companies often rely on user feedback to improve their products. One dominant method for generating human feedback data is through online surveys. For example, a popular social media platform may release a new feature and want to gather user opinions on its usability and satisfaction. They could create a survey with questions about the feature's functionality, ease of use, and overall experience. By distributing the survey to a sample of their user base, they can collect valuable feedback data to inform future iterations and improvements to the feature.\"}]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "03691f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = Concretize_prompt.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f938f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concretize the following question by providing a specific, real-world scenario or example that illustrates the concept or situation described in the question. The goal is to make the question more tangible and relatable.\n",
      "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
      "Relate the original question to a real-life scenario and reframe the question. \n",
      "Follow the rules given below while rewriting the question.\n",
      "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
      "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
      "    3. The rewritten question should not ask for any external links. \n",
      "    4. The rewritten question must not be more than 15 words. Keep the overall length of the question short by using abbreviations wherever possible.\n",
      "\n",
      "question:\n",
      "What are the three typical emergent abilities of LLMs mentioned in the context?\n",
      "context:\n",
      "sizes should be increased in equal scales, i.e.,having similar\n",
      "values for aandbin Equation (3).\n",
      "Though with some restricted assumptions, these scaling\n",
      "laws provide an intuitive understanding of the scaling ef-\n",
      "fect, making it feasible to predict the performance of LLMs\n",
      "during training [46]. However, some abilities ( e.g.,in-context\n",
      "learning [55]) are unpredictable according to the scaling law,\n",
      "which can be observed only when the model size exceeds a\n",
      "certain level (as discussed below).\n",
      "Emergent Abilities of LLMs . In the literature [31], emergent\n",
      "abilities of LLMs are formally defined as “the abilities that\n",
      "are not present in small models but arise in large models”,\n",
      "which is one of the most prominent features that distin-\n",
      "guish LLMs from previous PLMs. It further introduces a\n",
      "6. Here, Nc,Dcand Ccare measured in the number of non-\n",
      "embedding parameters, the number of training tokens and the number\n",
      "of FP-days, respectively. According to the original paper [30], CcandC\n",
      "should be denoted by Cmin\n",
      "c andCmin, corresponding to the optimal\n",
      "use of compute. We use the simplified notations for ease of discussions.notable characteristic when emergent abilities occur [31]:\n",
      "performance rises significantly above random when the\n",
      "scale reaches a certain level. By analogy, such an emergent\n",
      "pattern has close connections with the phenomenon of phase\n",
      "transition in physics [31, 58]. In principle, emergent abilities\n",
      "can be defined in relation to some complex tasks [31, 59],\n",
      "while we are more concerned with general abilities that\n",
      "can be applied to solve a variety of tasks. Here, we briefly\n",
      "introduce three typical emergent abilities for LLMs and\n",
      "representative models that possess such an ability7.\n",
      "•In-context learning. The in-context learning (ICL) ability\n",
      "is formally introduced by GPT-3 [55]: assuming that the\n",
      "language model has been provided with a natural language\n",
      "instruction and/or several task demonstrations, it can gen-\n",
      "erate the expected output for the test instances by com-\n",
      "pleting the word sequence of input text, without requiring\n",
      "additional training or gradient update8. Among the GPT-\n",
      "series models, the 175B GPT-3 model exhibited a strong ICL\n",
      "ability in general, but not the GPT-1 and GPT-2 models. Such\n",
      "an ability also depends on the specific downstream task. For\n",
      "example, the ICL ability can emerge on the arithmetic tasks\n",
      "(e.g., the 3-digit addition and subtraction) for the 13B GPT-3,\n",
      "but 175B GPT-3 even cannot work well on the Persian QA\n",
      "task [31].\n",
      "•Instruction following. By fine-tuning with a mixture of\n",
      "multi-task datasets formatted via natural language descrip-\n",
      "tions (called instruction tuning ), LLMs are shown to perform\n",
      "well on unseen tasks that are also described in the form\n",
      "of instructions [28, 61, 62]. With instruction tuning, LLMs\n",
      "are enabled to follow the task instructions for new tasks\n",
      "without using explicit examples, thus having an improved\n",
      "generalization ability. According to the experiments in [62],\n",
      "instruction-tuned LaMDA-PT [63] started to significantly\n",
      "outperform the untuned one on unseen tasks when the\n",
      "model size reached 68B, but not for 8B or smaller model\n",
      "sizes. A recent study [64] found that a model size of 62B is\n",
      "at least required for PaLM to perform well on various tasks\n",
      "in four evaluation benchmarks ( i.e.,MMLU, BBH, TyDiQA\n",
      "and MGSM), though a much smaller size might suffice for\n",
      "some specific tasks ( e.g., MMLU).\n",
      "•Step-by-step reasoning. For small language models, it\n",
      "is usually difficult to solve complex tasks that involve\n",
      "multiple reasoning steps, e.g., mathematical word problems.\n",
      "In contrast, with the chain-of-thought (CoT) prompting\n",
      "strategy [33], LLMs can solve such tasks by utilizing the\n",
      "prompting mechanism that involves intermediate reasoning\n",
      "steps for deriving the final answer. This ability is speculated\n",
      "to be potentially obtained by training on code [33, 47]. An\n",
      "empirical study [33] has shown that CoT prompting can\n",
      "bring performance gains (on arithmetic reasoning bench-\n",
      "rewritten question:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bd05de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "83b0a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the three typical emergent abilities of LLMs mentioned in the context?'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5083c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_c = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1dd38446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are three emergent abilities of LLMs mentioned in the context, such as in-context learning?'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "4c2ff172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C at 0x7fe9d180cf40> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The specific sources of data used for pre-training Language Models (LLMs) such as GPT-3 and GPT-NeoX include webpages, conversation data, books and news, scientific data, and code.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693402013,\n",
       "  \"id\": \"chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 44,\n",
       "    \"prompt_tokens\": 1263,\n",
       "    \"total_tokens\": 1307\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(answer_formulate.format(question=question_c,context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ee2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context. Ensure that the question can be answered entirely from the information present in the context. Do not frame questions that contains more than 15 words.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Detailed Description and Rules:\n",
    "1. Understand the Context: Read the provided context carefully and understand the information it contains. Identify key pieces of information that are scattered or indirectly related to each othe\n",
    "2. Identify Multiple Steps: Think about how you can construct a question that requires the reader to connect pieces of information from different parts of the context. This may involve drawing connections between various facts or concepts.\n",
    "3. Formulate a Multi-hop Question: Craft a question that necessitates the reader to make multiple logical connections or inferences based on the information provided in the context. Here are some strategies to create multi-hop questions:\n",
    "\n",
    "   - Bridge related entities: Identify information that relates specific entities and frame question that can be answered only by analysing information of both entities.\n",
    "   \n",
    "   - Use Pronouns: identify (he, she, it, they) that refer to same entity or concepts in the context, and ask questions that would require the reader to figure out what pronouns refer to.\n",
    "\n",
    "   - Refer to Specific Details: Mention specific details or facts from different parts of the context and ask how they are related.\n",
    "\n",
    "   - Pose Hypothetical Scenarios: Present a hypothetical situation or scenario that requires combining different elements from the context to arrive at an answer.\n",
    "\n",
    "   - Ask About Cause and Effect: Inquire about the causes or effects of specific events or actions mentioned in the context, which may involve reasoning through multiple steps.\n",
    "\n",
    "4. Ensure Clarity: Make sure the question is clear and unambiguous. It should be evident to the reader that they need to connect multiple pieces of information to answer the question.\n",
    "5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "Multi-hop Reasoning Question:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3200248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(test_prompt.format(context=context))\n",
    "question_r = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc8b2241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_r.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab87ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some challenges that need to be addressed when applying LLMs to real-world scenarios?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9b58f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "some recent work has studied the human-like characteristics\n",
      "of LLMs, such as self-awareness, theory of mind (ToM), and\n",
      "affective computing [603, 604]. In particular, an empirical\n",
      "evaluation of ToM conducted on two classic false-belief\n",
      "tasks speculates that LLMs may have ToM-like abilities\n",
      "since the model in the GPT-3.5 series achieves comparable\n",
      "performance with nine-year-old children in ToM task [603].\n",
      "In addition, another line of work has investigated applying\n",
      "LLMs into the software development domain, e.g., code\n",
      "suggestion [605], code summarization [606], and automated\n",
      "program repair [607]. To summarize, to assist humans by\n",
      "LLMs in real-world tasks has become a significant area of\n",
      "research. However, it also presents challenges. Ensuring the\n",
      "accuracy of LLM-generated content, addressing biases, and\n",
      "maintaining user privacy and data security are crucial con-\n",
      "siderations when applying LLMs to real-world scenarios.\n",
      "10 C ONCLUSION AND FUTURE DIRECTIONS\n",
      "In this survey, we have reviewed the recent progress of\n",
      "large language models (LLMs), and introduced the key\n",
      "concepts, findings, and techniques for understanding and\n",
      "utilizing LLMs. We focus on the large-sized models ( i.e.,\n",
      "having a size larger than 10B) while excluding the contents\n",
      "of early pre-trained language models ( e.g., BERT and GPT-\n",
      "2) that have been well covered in the existing literature. In\n",
      "particular, our survey has discussed four important aspects\n",
      "of LLMs, i.e., pre-training, adaptation tuning, utilization,\n",
      "and evaluation. For each aspect, we highlight the techniques\n",
      "or findings that are key to the success of LLMs. Furthermore,\n",
      "we also summarize the available resources for developing\n",
      "LLMs and discuss important implementation guidelines for\n",
      "reproducing LLMs. This survey tries to cover the most\n",
      "recent literature about LLMs and provides a good reference\n",
      "resource on this topic for both researchers and engineers.\n",
      "Next, we summarize the discussions of this survey, and\n",
      "introduce the challenges and future directions for LLMs, in\n",
      "the following aspects.\n",
      "Theory and Principle. To understand the underlying work-\n",
      "ing mechanism of LLMs, one of the greatest mysteries\n",
      "is how information is distributed, organized, and utilized\n",
      "through the very large, deep neural network. It is important\n",
      "to reveal the basic principles or elements that establish the\n",
      "foundation of the abilities of LLMs. In particular, scaling\n",
      "seems to play an important role in increasing the capacity\n",
      "of LLMs [31, 55, 59]. It has been shown that some emergent\n",
      "abilities would occur in an unexpected way (a sudden per-\n",
      "formance leap) when the parameter scale of language mod-\n",
      "els increases to a critical size ( e.g., 10B) [31, 33], typically in-\n",
      "cluding in-context learning, instruction following, and step-\n",
      "by-step reasoning. These emergent abilities are fascinating\n",
      "yet perplexing: when and how they are obtained by LLMs\n",
      "are not yet clear. Recent studies either conduct extensive\n",
      "experiments for investigating the effect of emergent abilities\n",
      "and the contributing factors to such abilities [307, 608, 609],\n",
      "or explain some specific abilities with existing theoretical\n",
      "frameworks [60, 317]. An insightful technical post also spe-\n",
      "cially discusses this topic [47], taking the GPT-series models\n",
      "as the target. However, more formal theories and principles\n",
      "to understand, characterize, and explain the abilities orbehaviors of LLMs are still missing. Since emergent abilities\n",
      "bear a close analogy to phase transitions in nature [31, 58],\n",
      "cross-discipline theories or principles ( e.g., whether LLMs\n",
      "can be considered as some kind of complex systems) might\n",
      "be useful to explain and understand the behaviors of LLMs.\n",
      "These fundamental questions are worth exploring for the\n",
      "research community, which are important for developing\n",
      "the next-generation LLMs.\n",
      "Model Architecture. Due to the scalability and effective-\n",
      "ness, Transformer, consisting of stacked multi-head self-\n",
      "attention layers, has become the de facto architecture for\n",
      "building LLMs. Various strategies have been proposed to\n",
      "improve the performance of this architecture, such as neural\n",
      "network configuration and scalable parallel training (see\n",
      "discussions in Section 4.2.2). To enhance the model capacity\n",
      "(e.g., the multi-turn conversation ability), existing LLMs\n",
      "typically maintain a long context window, e.g., GPT-4-32k\n",
      "has an extremely large context length of 32,768 tokens. Thus,\n",
      "a practical consideration is to reduce the time complexity\n",
      "(originally to be quadratic costs) incurred by the standard\n",
      "self-attention mechanism. It is important to investigate the\n",
      "effect of more efficient Transformer variants in building\n",
      "LLMs [610], e.g., sparse attention has been used in GPT-\n",
      "3 [55]. Besides, catastrophic forgetting has been a long-\n",
      "standing challenge for neural networks, which also has a\n",
      "negative impact on LLMs. When tuning LLMs with new\n",
      "data, the originally learned knowledge is likely to be dam-\n",
      "aged, e.g., fine-tuning a LLM according to some specific\n",
      "tasks will affect the general ability of LLMs. A similar case\n",
      "occurs when LLMs are aligned with human values (called\n",
      "alignment tax [61, 268]). Thus, it is necessary to consider\n",
      "extending existing architectures with more flexible mech-\n",
      "anisms or modules that can effectively support data update\n",
      "and task specialization.\n",
      "Model Training. In practice, it is very difficult to pre-\n",
      "train capable LLMs, due to the huge computation con-\n",
      "sumption and the sensitivity to data quality and training\n",
      "tricks [69, 83]. Thus, it becomes particularly important to\n",
      "develop more systemic, economical pre-training approaches\n",
      "for optimizing LLMs, considering the factors of model ef-\n",
      "fectiveness, efficiency optimization, and training stability.\n",
      "More model checking or performance diagnosis methods\n",
      "(e.g., predictable scaling in GPT-4 [46]) should be developed\n",
      "in order to detect early abnormal issues during training.\n",
      "Furthermore, it also calls for more flexible mechanisms of\n",
      "hardware support or resource schedule, so as to better\n",
      "organize and utilize the resources in a computing cluster.\n",
      "Since it is very costly to pre-train a LLM from scratch, it\n",
      "is important to design a suitable mechanisms for continu-\n",
      "ally pre-training or fine-tuning the LLM based on publicly\n",
      "available model checkpoints ( e.g., LLaMA [57] and Flan-\n",
      "T5 [64]). For this purpose, a number of technical issues\n",
      "have to be resolved, e.g., catastrophic forgetting and task\n",
      "specialization. However, to date, there still lack open-source\n",
      "model checkpoints for LLMs with complete pre-processing\n",
      "and training logs ( e.g., the scripts to prepare the pre-training\n",
      "data) for reproduction. We believe that it will be of great\n",
      "value to report more technical details in open-source models\n",
      "for the research of LLMs. Furthermore, it is also important\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6864e1",
   "metadata": {},
   "source": [
    "## COntent filteting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "16153e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_prompt = \"\"\"\n",
    "Rewrite the provided question to increase its complexity by introducing a conditional element. Rewrite the provided question while maintaining the essence of the original question, but without including the original question as part of it\n",
    "The goal is to make the question more intricate by incorporating a scenario or condition that affects the context of the question.\n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be very long. Use abbreviation wherever possible.\n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present context.\n",
    "    4. phrases like 'provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "for example,\n",
    "question: What are the general principles for designing prompts in LLMs?\n",
    "Rewritten Question:how to apply prompt designing principles to improve LLMs performance in reasoning tasks\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "Rewritten Question with added condiatoin and without including the original question\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "de6f9d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are three major quantization libraries for LLMs and what are their main features?'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9b013705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are three major quantization libraries for LLMs and what are their main features?'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d2c7fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = condition_prompt.format(question=question,context=context)\n",
    "output = llm2(prompt_input,temperature=0)\n",
    "question_cond = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6f72cc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the three major quantization libraries for LLMs and what are their main features, considering the benefits of efficient fine-tuning enhanced quantization in improving the performance of quantized LLMs?'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "how to apply prompt designing principles to improve it's performance in reasoning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af31a1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7wTXeogoap5L9TqI2PvCsXJbSlHf1 at 0x7f82ec37d3b0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are the implications of using LLMs in educational settings and what potential issues can arise from their application?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1694170938,\n",
       "  \"id\": \"chatcmpl-7wTXeogoap5L9TqI2PvCsXJbSlHf1\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 22,\n",
       "    \"prompt_tokens\": 71,\n",
       "    \"total_tokens\": 93\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(single_part.format(question=question_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb3c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
