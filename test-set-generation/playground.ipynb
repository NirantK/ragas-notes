{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81dc5",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "* Input set of documents\n",
    "    - number of samples to generate\n",
    "    - maximum token length (input + context + answer)\n",
    "    - difficulty distribution \n",
    "    - Seed questions? \n",
    "    - randomize answer output formats\n",
    "* Output : test set with questions,contexts,answer\n",
    "\n",
    "1. Select document and part of document to frame question from (random)\n",
    "2. Formulate a question,context pair contrained on output format. \n",
    "3. Identify difficulty type \n",
    "4. Evol question using evol-instruct like paradigm to improve difficulty \n",
    "    - ask for improved reasoning - (Evol instruct C Increased Reasoning Steps Prompt)\n",
    "    - Add more contexts and frame question for multi hop\n",
    "        - Adding more context\n",
    "           1. Identiy entity from current context and retrive paras with same entity : formulate new question\n",
    "           2. Identify similar paras using sentence similarity : formulate new question\n",
    "           \n",
    "           \n",
    "           \n",
    "#### Open-issues\n",
    "1. How to ensure inter-document dependancy is null\n",
    "\n",
    "A question framed might have possible answers from different documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e9980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91ccf9e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "744262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cbc55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open(\"/Users/shahules/openai-key.json\"))[\"ikka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0dcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "008e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e91820",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb673dd",
   "metadata": {},
   "source": [
    "- random select document \n",
    "- Identify sections from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69052348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31fe1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e45b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_doc(\"../arxiv-llm/textdata/2303.18223v11.A_Survey_of_Large_Language_Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42a236b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^(?:\\d+\\.\\d+\\s+)?[A-Z][A-Z-\\s]+$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81d1744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "374f4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "21537c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f2403987",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_formulate = \"\"\"\n",
    "Your task is to formulate a question from given context satifying the rules given below:\n",
    "    1.The question should be fully answered from the given context. \n",
    "    2.The question should be framed from a part that contains non-trivial information. \n",
    "    3.The answer should to the question must be in {answer_format} format. \n",
    "    4.The answer should not contain any links. \n",
    "    5.The question should be of {difficulty} difficulty.\n",
    "    6.The question must be reasonable and must be understood and responded by humans.\n",
    "{context}:context\n",
    "\"\"\"\n",
    "\n",
    "answer_formulate = \"\"\"\n",
    "Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
    "You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
    "\\n\\n\n",
    "question:\\nWhen was Einstein born?\n",
    "context:\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "start_index:0\n",
    "end_index:155\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "start_index:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e3435f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = text.split('\\n\\n')[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c07bc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = question_formulate.format(answer_format=\"text\",difficulty=\"hard\",context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6280e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "41845c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4d0d804b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the primary technical issues in training large language models (LLMs) and what are some approaches to address these challenges?\\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "921580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = answer_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e6295220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
      "You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
      "\n",
      "\n",
      "\n",
      "question:\n",
      "When was Einstein born?\n",
      "context:\n",
      "Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
      "start_index:0\n",
      "end_index:155\n",
      "question:\n",
      "What are the primary technical issues in training large language models (LLMs) and what are some approaches to address these challenges?\n",
      "\n",
      "context:\n",
      "22\n",
      "TABLE 5: Detailed optimization settings of several existing LLMs.\n",
      "ModelBatch Size\n",
      "(#tokens)Learning\n",
      "RateWarmup Decay Method OptimizerPrecision\n",
      "TypeWeight\n",
      "DecayGrad\n",
      "ClipDropout\n",
      "GPT3 (175B) 32K →3.2M 6×10−5yes cosine decay to 10% Adam FP16 0.1 1.0 -\n",
      "PanGu- α(200B) - 2×10−5- - Adam - 0.1 - -\n",
      "OPT (175B) 2M 1.2×10−4yes manual decay AdamW FP16 0.1 - 0.1\n",
      "PaLM (540B) 1M →4M 1×10−2no inverse square root Adafactor BF16 lr21.0 0.1\n",
      "BLOOM (176B) 4M 6×10−5yes cosine decay to 10% Adam BF16 0.1 1.0 0.0\n",
      "MT-NLG (530B) 64 K →3.75M 5×10−5yes cosine decay to 10% Adam BF16 0.1 1.0 -\n",
      "Gopher (280B) 3M →6M 4×10−5yes cosine decay to 10% Adam BF16 - 1.0 -\n",
      "Chinchilla (70B) 1.5M →3M 1×10−4yes cosine decay to 10% AdamW BF16 - - -\n",
      "Galactica (120B) 2M 7×10−6yes linear decay to 10% AdamW - 0.1 1.0 0.1\n",
      "LaMDA (137B) 256K - - - - BF16 - - -\n",
      "Jurassic-1 (178B) 32 K →3.2M 6×10−5yes - - - - - -\n",
      "LLaMA (65B) 4M 1.5×10−4yes cosine decay to 10% AdamW - 0.1 1.0 -\n",
      "GLM (130B) 0.4M →8.25M 8×10−5yes cosine decay to 10% AdamW FP16 0.1 1.0 0.1\n",
      "T5 (11B) 64K 1×10−2no inverse square root AdaFactor - - - 0.1\n",
      "ERNIE 3.0 Titan (260B) - 1×10−4- - Adam FP16 0.1 1.0 -\n",
      "PanGu- Σ(1.085T) 0.5M 2×10−5yes - Adam FP16 - - -\n",
      "4.3.2 Scalable Training Techniques\n",
      "As the model and data sizes increase, it has become chal-\n",
      "lenging to efficiently train LLMs under a limited compu-\n",
      "tational resource. Especially, two primary technical issues\n",
      "are required to be resolved, i.e.,increasing training through-\n",
      "put and loading larger models into GPU memory. In this\n",
      "part, we review several widely used approaches in existing\n",
      "work to address the above two challenges, namely 3D\n",
      "parallelism [66, 229, 230], ZeRO [231], and mixed precision\n",
      "training [232], and also give general suggestions about how\n",
      "to utilize them for training.\n",
      "3D Parallelism. 3D parallelism is actually a combination of\n",
      "three commonly used parallel training techniques, namely\n",
      "data parallelism, pipeline parallelism [229, 230], and tensor\n",
      "parallelism [66]21. We next introduce the three parallel train-\n",
      "ing techniques.\n",
      "•Data parallelism. Data parallelism is one of the most\n",
      "fundamental approaches to improving the training through-\n",
      "put. It replicates the model parameters and optimizer states\n",
      "across multiple GPUs and then distributes the whole train-\n",
      "ing corpus into these GPUs. In this way, each GPU only\n",
      "needs to process the assigned data for it, and performs\n",
      "the forward and backward propagation to obtain the gra-\n",
      "dients. The computed gradients on different GPUs will be\n",
      "further aggregated to obtain the gradients of the entire batch\n",
      "for updating the models in all GPUs. In this way, as the\n",
      "calculations of gradients are independently performed on\n",
      "different GPUs, the data parallelism mechanism is highly\n",
      "scalable, enabling the way that increases the number of\n",
      "GPUs to improve training throughput. Furthermore, this\n",
      "technique is simple in implementation, and most of existing\n",
      "popular deep learning libraries have already implemented\n",
      "data parallelism, such as TensorFlow and PyTorch.\n",
      "•Pipeline parallelism. Pipeline parallelism aims to dis-\n",
      "tribute the different layers of a LLM into multiple GPUs.\n",
      "Especially, in the case of a Transformer model, pipeline\n",
      "parallelism loads consecutive layers onto the same GPU, to\n",
      "reduce the cost of transmitting the computed hidden states\n",
      "or gradients between GPUs. However, a naive implemen-\n",
      "21. Model parallelism is a more broader term that includes tensor\n",
      "parallelism and pipeline parallelism in some work [66].tation of pipeline parallelism may result in a lower GPU\n",
      "utilization rate as each GPU has to wait for the previous\n",
      "one to complete the computation, leading to the unneces-\n",
      "sary cost of bubbles overhead [229]. To reduce these bubbles\n",
      "in pipeline parallelism, GPipe [229] and PipeDream [230]\n",
      "propose the techniques of padding multiple batches of data\n",
      "and asynchronous gradient update to improve the pipeline\n",
      "efficiency.\n",
      "•Tensor parallelism. Tensor parallelism is also a com-\n",
      "monly used technique that aims to decompose the LLM for\n",
      "multi-GPU loading. Unlike pipeline parallelism, tensor par-\n",
      "allelism focuses on decomposing the tensors (the parameter\n",
      "matrices) of LLMs. For a matrix multiplication operation\n",
      "Y=XA in the LLM, the parameter matrix Acan be\n",
      "split into two submatrices, A1andA2, by column, which\n",
      "can be expressed as Y= [XA 1, XA 2]. By placing matrices\n",
      "A1andA2on different GPUs, the matrix multiplication\n",
      "operation would be invoked at two GPUs in parallel, and\n",
      "the final result can be obtained by combining the outputs\n",
      "from the two GPUs through across-GPU communication.\n",
      "Currently, tensor parallelism has been supported in several\n",
      "open-source libraries, e.g., Megatron-LM [66], and can be\n",
      "extended to higher-dimensional tensors. Also, Colossal-AI\n",
      "has implemented tensor parallelism for higher-dimensional\n",
      "tensors [233–235] and proposed sequence parallelism [236]\n",
      "especially for sequence data, which can further decompose\n",
      "the attention operation of the Transformer model.\n",
      "ZeRO. ZeRO [231] technique, proposed by the Deep-\n",
      "Speed [65] library, focuses on the issue of memory re-\n",
      "dundancy in data parallelism. As mentioned before, data\n",
      "parallelism requires each GPU to store the same copy of\n",
      "a LLM, including model parameters, model gradients, and\n",
      "optimizer parameters. Whereas, not all of the above data is\n",
      "necessary to be retained on each GPU, which would cause\n",
      "a memory redundancy problem. To resolve it, the ZeRO\n",
      "technique aims to retain only a fraction of data on each\n",
      "GPU, while the rest data can be retrieved from other GPUs\n",
      "when required. Specifically, ZeRO provides three solutions,\n",
      "depending on how the three parts of the data are stored,\n",
      "namely optimizer state partitioning, gradient partitioning,\n",
      "and parameter partitioning. Empirical results indicate that\n",
      "the first two solutions do not increase the communication\n",
      "start_index:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2e211024",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "af8f383f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7sxU20hRexzzluaW5upUdkLDIzTfc at 0x7fc8104f38b0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"start_index: 222\\nend_index: 267\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693332722,\n",
       "  \"id\": \"chatcmpl-7sxU20hRexzzluaW5upUdkLDIzTfc\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 11,\n",
       "    \"prompt_tokens\": 1872,\n",
       "    \"total_tokens\": 1883\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "680fa101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y to 10% Adam FP16 0.1 1.0 -\\nPanGu- α(200B) -'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[222:267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "02980eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amW FP16 0.1 - 0.1\\nPaLM (540B) 1M →4M 1×10−2no inverse square root Adafactor BF16 lr21.0 0.1\\nBLOOM (176B) 4M 6×10−5yes cosine decay to 10% Adam BF16 0.1 1.0 0.0\\nMT-NLG (530B) 64 K →3.75M 5×10−5yes cosine decay to 10% Adam BF16 0.1 1.0 -\\nGopher (280B) 3M →6M 4×10−5yes cosine decay to 10% Adam BF16 - 1.0 -\\nChinchilla (70B) 1.5M →3M 1×10−4yes cosine decay to 10% AdamW BF16 - - -\\nGalactica (120B) 2M 7×10−6yes linear decay to 10% AdamW - 0.1 1.0 0.1\\nLaMDA (137B) 256K - - - - BF16 - - -\\nJurassic-1 (178B) 32 K →3.2M 6×10−5yes - - - - - -\\nLLaMA (65B) 4M 1.5×10−4yes cosine decay to 10% AdamW - 0.1 1.0 -\\nGLM (130B) 0.4M →8.25M 8×10−5yes cosine decay to 10% AdamW FP16 0.1 1.0 0.1\\nT5 (11B) 64K 1×10−2no inverse square root AdaFactor - - - 0.1\\nERNIE 3.0 Titan (260B) - 1×10−4- - Adam FP16 0.1 1.0 -\\nPanGu- Σ(1.085T) 0.5M 2×10−5yes - Adam FP16 - - -\\n4.3.2 Scalable Training Techniques\\nAs the model and data sizes increase, it has become chal-\\nlenging to efficiently train LLMs under a limited compu-\\ntational resource. Especially, two primary technical issues\\nare required to be resolved, i.e.,increasing training through-\\nput and loading larger models into GPU memory. In this\\npart, we review several widely used approaches in existing\\nwork to address the above two challenges, namely 3D\\nparallelism [66, 229, 230], ZeRO [231], and mixed precision\\ntraining [232], and also give general suggestions about how\\nto utilize them for training.\\n3D Parallelism. 3D parallelism is actually a combination of\\nthree commonly used parallel training techniques, namely\\ndata parallelism, pipeline parallelism [229, 230], and tensor\\nparallelism [66]21. We next introduce the three parallel train-\\ning techniques.\\n•Data parallelism. Data parallelism is one of the most\\nfundamental approaches to improving the training through-\\nput. It replicates the model parameters and optimizer states\\nacross multiple GPUs and then distributes the whole train-\\ning corpus into these GPUs. In this way, each GPU only\\nneeds to process the assigned data for it, and performs\\nthe forward and backward propagation to obtain the gra-\\ndients. The computed gradients on different GPUs will be\\nfurther aggregated to obtain the gradients of the entire batch\\nfor updating the models in all GPUs. In this way, as the\\ncalculations of gradients are independently performed on\\ndifferent GPUs, the data parallelism mechanism is highly\\nscalable, enabling the way that increases the number of\\nGPUs to improve training throughput. Furthermore, this\\ntechnique is simple in implementation, and most of existing\\npopular deep learning libraries have already implemented\\ndata parallelism, such as TensorFlow and PyTorch.\\n•Pipeline parallelism. Pipeline parallelism aims to dis-\\ntribute the different layers of a LLM into multiple GPUs.\\nEspecially, in the case of a Transformer model, pipeline\\nparallelism loads consecutive layers onto the same GPU, to\\nreduce the cost of transmitting the computed hidden states\\nor gradients between GPUs. However, a naive implemen-\\n21. Model parallelism is a more broader term that includes tensor\\nparallelism and pipeline parallelism in some work [66].tation of pipeline parallelism may result in a lower GPU\\nutilization rate as each GPU has to wait for the previous\\none to complete the computation, leading to the unneces-\\nsary cost of bubbles overhead [229]. To reduce these bubbles\\nin pipeline parallelism, GPipe [229] and PipeDream [230]\\npropose the techniques of padding multiple batches of data\\nand asynchronous gradient update to improve the pipeline\\nefficiency.\\n•Tensor parallelism. Tensor parallelism is also a com-\\nmonly used technique that aims to decompose the LLM for\\nmulti-GPU loading. Unlike pipeline parallelism, tensor par-\\nallelism focuses on decomposing the tensors (the parameter\\nmatrices) of LLMs. For a matrix multiplication operation\\nY=XA in the LLM, the parameter matrix Acan be\\nsplit into two submatrices, A1andA2, by column, which\\ncan be expressed as Y= [XA 1, XA 2]. By placing matrices\\nA1andA2on different GPUs, the matrix multiplication\\noperation would be invoked at two GPUs in parallel, and\\nthe final result can be obtained by combining the outputs\\nfrom the two GPUs through across-GPU communication.\\nCurrently, tensor parallelism has been supported in several\\nopen-source libraries, e.g., Megatron-LM [66], and can be\\nextended to higher-dimensional tensors. Also, Colossal-AI\\nhas implemented tensor parallelism for higher-dimensional\\ntensors [233–235] and proposed sequence parallelism [236]\\nespecially for sequence data, which can further decompose\\nthe attention operation of the Transformer model.\\nZeRO. ZeRO [231] technique, proposed by the Deep-\\nSpeed [65] library, focuses on the issue of memory re-\\ndundancy in data parallelism. As mentioned before, data\\nparallelism requires each GPU to store the same copy of\\na LLM, including model parameters, model gradients, and\\noptimizer parameters. Whereas, not all of the above data is\\nnecessary to be retained on each GPU, which would cause\\na memory redundancy problem. To resolve it, the ZeRO\\ntechnique aims to retain only a fraction of data on each\\nGPU, while the rest data can be retrieved from other GPUs\\nwhen required. Specifically, ZeRO provides three solutions,\\ndepending on how the three parts of the data are stored,\\nnamely optimizer state partitioning, gradient partitioning,\\nand parameter partitioning. Empirical results indicate that\\nthe first two solutions do not increase the communication'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[334:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d4434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
