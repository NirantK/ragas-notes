{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81dc5",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "* Input set of documents\n",
    "    - number of samples to generate\n",
    "    - maximum token length (input + context + answer)\n",
    "    - difficulty distribution \n",
    "    - Seed questions? \n",
    "    - randomize answer output formats\n",
    "* Output : test set with questions,contexts,answer\n",
    "\n",
    "1. Select document and part of document to frame question from (random)\n",
    "2. Formulate a question,context pair contrained on output format. \n",
    "3. Identify difficulty type \n",
    "4. Evol question using evol-instruct like paradigm to improve difficulty \n",
    "    - ask for improved reasoning - (Evol instruct C Increased Reasoning Steps Prompt)\n",
    "    - Add more contexts and frame question for multi hop\n",
    "        - Adding more context\n",
    "           1. Identiy entity from current context and retrive paras with same entity : formulate new question\n",
    "           2. Identify similar paras using sentence similarity : formulate new question\n",
    "           \n",
    "           \n",
    "#### Ideas to increase complextity\n",
    "\n",
    "- Extend question by using info in the newly added contexts.\n",
    "\n",
    "\n",
    "- Concretizing\n",
    "\n",
    "    In RAG case,this should yeild an instruction that is related to real world use-case on concept decribed in the context. \n",
    "    For example, if context speaks about different types of instructions and their use-cases\n",
    "    question will be like \"I want to train a chatbot, how to form training dataset?\"\n",
    "\n",
    "\n",
    "- Improved reasoning\n",
    "           \n",
    "#### Open-issues\n",
    "1. How to ensure inter-document dependancy is null\n",
    "\n",
    "A question framed might have possible answers from different documents\n",
    "\n",
    "2. Questions framed by LLM are easily answerable. \n",
    "\n",
    "It feels like LLM first identifies a candidate sentence and frames question based on it. So by default the answer to questions can be located easily.\n",
    "\n",
    "3. GPT 3.5 tends to add or increase length of question when asked to create difficult question\n",
    "\n",
    "often this ends up as a unsually long question that contains two questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e9980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91ccf9e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "744262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open(\"/Users/shahules/openai-key.json\"))[\"ikka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0dcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "58d67a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "Embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8fcbd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(question, generated_questions):\n",
    "        question_vec = np.asarray(Embedding.embed_query(question)).reshape(1, -1)\n",
    "        gen_question_vec = np.asarray(\n",
    "            Embedding.embed_documents(generated_questions)\n",
    "        )\n",
    "        norm = np.linalg.norm(gen_question_vec, axis=1) * np.linalg.norm(\n",
    "            question_vec, axis=1\n",
    "        )\n",
    "        return (\n",
    "            np.dot(gen_question_vec, question_vec.T).reshape(\n",
    "                -1,\n",
    "            )\n",
    "            / norm\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e91820",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb673dd",
   "metadata": {},
   "source": [
    "- random select document \n",
    "- Identify sections from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69052348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fe1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_doc(\"../arxiv-llm/textdata/2303.18223v11.A_Survey_of_Large_Language_Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a236b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^(?:\\d+\\.\\d+\\s+)?[A-Z][A-Z-\\s]+$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d1744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "374f4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21537c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "f2403987",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_formulate = \"\"\"\n",
    "Your task is to formulate a question from given context satifying the rules given below:\n",
    "    1.The question should be fully answered from the given context. \n",
    "    2.The question should be framed from a part that contains non-trivial information. \n",
    "    3.The answer should to the question must be in {answer_format} format. \n",
    "    4.The answer should not contain any links. \n",
    "    5.The question should be of {difficulty} difficulty.\n",
    "    6.The question must be reasonable and must be understood and responded by humans.\n",
    "{context}:context\n",
    "\"\"\"\n",
    "\n",
    "context_formulate =  \"\"\"\\\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract chunks  from given context that is required to answer the question. \n",
    "Rules to follow while doing this task\n",
    "    1. The chunks could be anywhere in the provided context, pay attention to the whole context.\n",
    "    2. Extract the exact sentences from given context, you're not allowed to include or exclude even a single character from the candidate sentences.\n",
    "    3. If the context do not contain information required to answer the question return \"No candidate sentences found\".\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "The sentences that can be extracted from the given context to answer the question are:\"\"\" \n",
    "\n",
    "answer_formulate = \"\"\"\n",
    "Asnwer the question using the information from the qiven context. \n",
    "You are not allowed to include information that cannot be deducted from the given context.\n",
    "question:{question}\n",
    "context:{context}\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "context_from_answer = \"\"\"\n",
    "Given question, context and answer. Locate the relevant information in the context from context that was to used to form the given answer. \n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "answer:\\n{answer}\n",
    "extracted context:\"\"\"\n",
    "\n",
    "\n",
    "# answer_index_formulate = \"\"\"\n",
    "# Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
    "# You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
    "# \\n\\n\n",
    "# question:\\nWhen was Einstein born?\n",
    "# context:\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "# answer: the answer to the given question can be located between character index 0 and 155 of given context.  \n",
    "# question:\\n{question}\n",
    "# context:\\n{context}\n",
    "# answer:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "e3435f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = text.split('\\n\\n')[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "c07bc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = question_formulate.format(answer_format=\"text\",difficulty=\"medium\",context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "6280e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "41845c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "4d0d804b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs?'"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "880a66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"\"\"\n",
    "# Adolf Hitler was an Austrian-born German politician who was the dictator of Germany from 1933 until his suicide in 1945. He rose to power as the leader of the Nazi Party, becoming the chancellor in 1933 and then taking the title of Führer und Reichskanzler in 1934\n",
    "# \"\"\"\n",
    "# question = \"when was hitler born?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "921580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = context_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "2e211024",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "af8f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_context = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "942361bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20\\nfor position embeddings, RoPE or ALiBi is a better choice\\nsince it performs better on long sequences.\\n4.2.3 Pre-training Tasks\\nPre-training plays a key role that encodes general knowl-\\nedge from large-scale corpus into the massive model param-\\neters. For training LLMs, there are two commonly used pre-\\ntraining tasks, namely language modeling and denoising\\nautoencoding.\\nLanguage Modeling. The language modeling task (LM) is\\nthe most commonly used objective to pre-train decoder-only\\nLLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of\\ntokens x={x1, . . . , x n}, the LM task aims to autoregres-\\nsively predict the target tokens xibased on the preceding\\ntokens x<iin a sequence. A general training objective is to\\nmaximize the following likelihood:\\nLLM(x) =nX\\ni=1logP(xi|x<i). (4)\\nSince most language tasks can be cast as the prediction\\nproblem based on the input, these decoder-only LLMs might\\nbe potentially advantageous to implicitly learn how to ac-\\ncomplish these tasks in a unified LM way. Some studies\\nhave also revealed that decoder-only LLMs can be naturally\\ntransferred to certain tasks by autoregressively predicting\\nthe next tokens [26, 55], without fine-tuning. An important\\nvariant of LM is the prefix language modeling task, which is\\ndesigned for pre-training models with the prefix decoder\\narchitecture. The tokens within a randomly selected prefix\\nwould not be used in computing the loss of prefix language\\nmodeling. With the same amount of tokens seen during pre-\\ntraining, prefix language modeling performs slightly worse\\nthan language modeling, since fewer tokens in the sequence\\nare involved for model pre-training [29].\\nDenoising Autoencoding. In addition to conventional\\nLM, the denoising autoencoding task (DAE) has also been\\nwidely used to pre-train language models [24, 73]. The\\ninputs x\\\\˜xfor DAE task are corrupted text with randomly\\nreplaced spans. Then, the language models are trained to re-\\ncover the replaced tokens ˜x. Formally, the training objective\\nof DAE is denoted as follows:\\nLDAE(x) = log P(˜x|x\\\\˜x). (5)\\nHowever, the DAE task seems to be more complicated\\nin implementation than LM task. As a result, it has not\\nbeen widely used to pre-train large language models. Exist-\\ning LLMs that take DAE as pre-training objectives include\\nT5 [73] and GLM-130B [83]. These models are mainly trained\\nto recover the replaced spans in an autoregressive way.\\nMixture-of-Denoisers. Mixture-of-Denoisers (MoD) [80],\\nalso known as UL2 loss, was introduced as a unified ob-\\njective for pre-training language models. MoD regards both\\nLM and DAE objectives as different types of denoising tasks,\\nnamely S-denoiser (LM), R-denoiser (DAE, short span and\\nlow corruption), and X-denoiser (DAE, long span or high\\ncorruption). Among the three denoising tasks, S-denoiser\\nis similar to the conventional LM objective (Equation (4)),\\nwhile R-denoiser and X-denoiser are similar to DAE ob-\\njectives (Equation (5)) but differ from each other in thelengths of spans and ratio of corrupted text. For input sen-\\ntences started with different special tokens ( i.e.,{[R],[S],\\n[X]}), the model will be optimized using the corresponding\\ndenoisers. MoD has been applied in the latest PaLM 2\\nmodel [216].\\n4.2.4 Summary and Discussion\\nThe choice of architecture and pre-training tasks may incur\\ndifferent inductive biases for LLMs, which would lead to\\ndifferent model capacities. In this part, we discuss on several\\ntwo open issues about LLM architecture.\\nArchitecture Choice . In earlier literature of pre-trained lan-\\nguage models, there are lots of discussions on the effects\\nof different architectures [29, 80]. However, most LLMs are\\ndeveloped based on the causal decoder architecture, and\\nthere still lacks a theoretical analysis on its advantage over\\nthe other alternatives. Next, we briefly summarize existing\\ndiscussions on this issue.\\n•By pre-training with the LM objective, it seems that\\ncausal decoder architecture can achieve a superior zero-\\nshot and few-shot generalization capacity. Existing research\\nhas shown that without multi-task fine-tuning, the causal\\ndecoder has better zero-shot performance than other archi-\\ntectures [29]. The success of GPT-3 [55] has demonstrates\\nthat the large causal decoder model can be a good few-\\nshot learner. In addition, instruction tuning and alignment\\ntuning discussed in Section 5 have been proven to fur-\\nther enhance the capability of large causal decoder mod-\\nels [61, 62, 64].\\n•Scaling law has been widely observed in causal de-\\ncoders. By scaling the model size, the dataset size, and\\nthe total computation, the performance of causal decoders\\ncan be substantially improved [30, 55]. Thus, it has become\\nan important strategy to increase the model capacity of\\nthe causal decoder via scaling. However, more detailed\\ninvestigation on encoder-decoder models is still lacking, and\\nmore efforts are needed to investigate the performance of\\nencoder-decoder models at a large scale.\\nMore research efforts about the discussions on archi-\\ntectures and pre-training objectives are in need to analyze\\nhow the choices of the architecture and pre-training tasks\\naffect the capacity of LLMs, especially for encoder-decoder\\narchitectures. Besides the major architecture, the detailed\\nconfiguration of LLM is also worth attention, which has\\nbeen discussed in Section 4.2.2.\\nLong Context . One of the main drawbacks of Transformer-\\nbased language models is the context length is limited due\\nto the involved quadratic computational costs in both time\\nand memory. Meanwhile, there is an increasing demand\\nfor LLM applications with long context windows, such as\\nin PDF processing and story writing [217]. ChatGPT has\\nrecently released an updated variant with a context window\\nsize of up to 16K tokens, which is much longer than the\\ninitial one, i.e.,4K tokens. Additionally, GPT-4 was launched\\nwith variants with context window of 32K tokens [46]. Next,\\nwe discuss two important factors that support long context\\nmodeling for LLMs.\\n•Extrapolation. In real-world applications, it is possible\\nthat LLMs need to process long input text that exceeds the\\nmaximum length of the training corpus. The ability of LLMs'"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "0dfd6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- To construct the scientific corpus, existing efforts mainly collect arXiv papers, scientific textbooks, math webpages, and other related scientific resources.\\n- The first source is from programming question answering communities like Stack Exchange.\\n- The second source is from public software repositories such as GitHub, where code data (including comments and docstrings) are collected for utilization.'"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "7503b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = answer_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "889d4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cada2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "303fbf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some advantages of LoRA in parameter-efficient fine-tuning of Language Model Models (LLMs) are:\\n\\n1. Memory and storage savings: LoRA can significantly reduce the memory and storage usage of LLMs. It allows for the use of a single large model copy while maintaining task-specific low-rank decomposition matrices. This saves VRAM and storage space.\\n\\n2. Lightweight adaptation: LoRA provides a lightweight adaptation approach for downstream tasks. It allows for efficient tuning of LLMs, making them more suitable for resource-limited settings.\\n\\n3. Comparable performance with fewer parameters: LoRA performs relatively well compared to other efficient tuning methods, using significantly fewer trainable parameters. This means that it can achieve comparable performance to larger models while being more parameter-efficient.\\n\\nOverall, LoRA offers memory and storage savings, lightweight adaptation, and comparable performance with fewer parameters, making it advantageous for parameter-efficient fine-tuning of LLMs.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11097aa3",
   "metadata": {},
   "source": [
    "#### Extract more context for multi-hop questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "62b16b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=23\n",
    "all_contexts = text.split('\\n\\n')[index:index+20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ad921f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = calculate_similarity(extracted_context,all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "3d379103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83912225, 0.76749923, 0.87348663, 0.82593298, 0.8097002 ,\n",
       "       0.85576036, 0.84262018, 0.82826259, 0.8075985 , 0.82869592,\n",
       "       0.80941025, 0.82444939, 0.77911891, 0.84715337, 0.82129219,\n",
       "       0.81333971, 0.8430313 , 0.84772823, 0.8263808 , 0.84240174])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "fd407200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  5, 17, 13, 16,  6, 19,  0,  9,  7, 18,  3, 11, 14, 15,  4, 10,\n",
       "        8, 12,  1])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a9f3e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "mance of the model. Here, we discuss some essential factors\n",
      "for instance construction.\n",
      "•Scaling the instructions. It has been widely shown that\n",
      "scaling the number of tasks can largely enhance the general-\n",
      "ization ability of LLMs [28, 62, 79]. With the increasing of the\n",
      "task number, the model performance initially shows a con-\n",
      "tinuous growth pattern, while the gain becomes negligible\n",
      "when it reaches a certain level [64, 79]. A plausible specula-\n",
      "tion is that a certain number of representative tasks can pro-\n",
      "vide relatively sufficient knowledge and adding more tasks\n",
      "may not bring additional gains [64]. Also, it is beneficial to\n",
      "enhance the diversity of the task descriptions in several as-\n",
      "pects, such as length, structure, and creativity [28]. As for the\n",
      "number of instances per task, it has been found that a small\n",
      "number of instances can usually saturate the generalization\n",
      "performance of the model [62, 64]. Whereas, increasing the\n",
      "number of instances for some tasks to a large number ( e.g.,\n",
      "a few hundreds) could potentially result in the overfitting\n",
      "issue and impair the model performance [79, 252].\n",
      "•Formatting design. As an important factor, the design\n",
      "of natural language format also highly impacts the gener-\n",
      "alization performance of LLMs [79]. Typically, we can add\n",
      "task descriptions and optional demonstrations to the input-\n",
      "output pairs of existing datasets, where the task description\n",
      "is the most key part for LLMs to understand the task [79].\n",
      "Further, it can lead to substantial improvements by using an\n",
      "appropriate number of exemplars as demonstrations [64],\n",
      "which also alleviates the model sensitivity to instruction\n",
      "engineering [62, 64]. However, incorporating other compo-\n",
      "nents ( e.g., things to avoid, reasons, and suggestions) into\n",
      "instructions may have a negligible or even adverse effect\n",
      "on the performance of LLMs [79, 240]. Recently, to elicit\n",
      "the step-by-step reasoning ability of LLMs, some work [64]\n",
      "proposes to include chain-of-thought (CoT) examples for\n",
      "some reasoning datasets, such as arithmetic reasoning. It\n",
      "has been shown that fine-tuning LLMs with both CoT and\n",
      "non-CoT examples can lead to a good performance across\n",
      "various reasoning tasks, including those that require multi-\n",
      "hop reasoning ability ( e.g., commonsense question answer-\n",
      "ing and arithmetic reasoning) as well as those without the\n",
      "need for such a reasoning way ( e.g., sentiment analysis and\n",
      "extractive question answering) [64, 85].\n",
      "To summarize, it seems that the diversity and quality\n",
      "of instructions is more important than the number of in-\n",
      "stances [253] since the well-performing InstructGPT [61] and\n",
      "Alpaca [124] utilize fewer but more diverse instructions (or\n",
      "instances) than the Flan-series LLMs [62, 64]. Further, it is\n",
      "more useful to invite labelers to compose human-need tasks\n",
      "than using dataset-specific tasks. However, it still lacks gen-\n",
      "eral guidelines to annotate human-need instances, making\n",
      "the task composition somehow heuristic. To reduce human\n",
      "efforts, we can either reuse existing formatted datasets\n",
      "(Table 6) or automatically construct the instructions using\n",
      "existing LLMs [125]. We conduct a preliminary experiment\n",
      "to show the effectiveness of different construction methods\n",
      "in Section 5.1.4.\n",
      "5.1.2 Instruction Tuning Strategies\n",
      "Unlike pre-training, instruction tuning is often more effi-\n",
      "cient since only a moderate number of instances are used\n",
      "for training. Since instruction tuning can be considered asa supervised training process, its optimization is different\n",
      "from pre-training in several aspects [64], such as the training\n",
      "objective ( i.e.,sequence-to-sequence loss) and optimization\n",
      "configuration ( e.g., smaller batch size and learning rate),\n",
      "which require special attention in practice. In addition to\n",
      "these optimization configurations, there are also two impor-\n",
      "tant aspects to consider for instruction tuning:\n",
      "Balancing the Data Distribution. Since instruction tun-\n",
      "ing involves a mixture of different tasks, it is important\n",
      "to balance the proportion of different tasks during fine-\n",
      "tuning. A widely used method is the examples-proportional\n",
      "mixing strategy [73], i.e., combining all the datasets and\n",
      "sampling each instance equally from the mixed datasets.\n",
      "Furthermore, increasing the sampling ratio of high-quality\n",
      "collections ( e.g., FLAN [62] and P3 [241]) can generally\n",
      "lead to performance improvement according to recent find-\n",
      "ings [64, 85]. Further, it is common to set a maximum cap\n",
      "to control the maximum number of examples that a dataset\n",
      "can contain during instruction tuning [73], which is set to\n",
      "prevent larger datasets from overwhelming the entire dis-\n",
      "tribution [73, 85]. In practice, the maximum cap is typically\n",
      "set to several thousands or tens of thousands according to\n",
      "different datasets [62, 64].\n",
      "Combining Instruction T uning and Pre-Training. To make\n",
      "the tuning process more effective and stable, OPT-IML [85]\n",
      "incorporates pre-training data during instruction tuning,\n",
      "which can be regarded as regularization for model tuning.\n",
      "Further, instead of using a separate two-stage process ( pre-\n",
      "training then instruction tuning ), some studies attempt to\n",
      "train a model from scratch with a mixture of pre-training\n",
      "data ( i.e.,plain texts) and instruction tuning data ( i.e.,for-\n",
      "matted datasets) using multi-task learning [73]. Specifically,\n",
      "GLM-130B [83] and Galactica [35] integrate instruction-\n",
      "formatted datasets as a small proportion of the pre-training\n",
      "corpora to pre-train LLMs, which potentially achieves the\n",
      "advantages of pre-training and instruction tuning at the\n",
      "same time.\n",
      "5.1.3 The Effect of Instruction Tuning\n",
      "In this part, we discuss the effect of instruction tuning on\n",
      "LLMs in three major aspects.\n",
      "Performance Improvement. Despite being tuned on a mod-\n",
      "erate number of instances, instruction tuning has become\n",
      "an important way to improve or unlock the abilities of\n",
      "LLMs [64]. Recent studies have experimented with language\n",
      "models in multiple scales (ranging from 77M to 540B),\n",
      "showing that the models of different scales can all benefit\n",
      "from instruction tuning [64, 251], yielding improved perfor-\n",
      "mance as the parameter scale increases [84]. Further, smaller\n",
      "models with instruction tuning can even perform better\n",
      "than larger models without fine-tuning [28, 64]. Besides\n",
      "the model scale, instruction tuning demonstrates consistent\n",
      "improvements in various model architectures, pre-training\n",
      "objectives, and model adaptation methods [64]. In practice,\n",
      "instruction tuning offers a general approach to enhancing\n",
      "the abilities of existing language models [64] (including\n",
      "small-sized PLMs). Also, it is much less costly than pre-\n",
      "training, since the amount of instruction data required by\n",
      "LLMs is significantly smaller than pre-training data.\n"
     ]
    }
   ],
   "source": [
    "print(all_contexts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b752c",
   "metadata": {},
   "source": [
    "### Complicating instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "af24ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1\n",
    "## includes info in context2 and adds an 'and' part to the original question\n",
    "\n",
    "multi_context = \"\"\"\n",
    "You are a prompt rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "You will be provided with a question and two set of contexts namely context1 and context2. \n",
    "Your task is to complicate the given question in a way that answering it requires information derived from both context1 and context2. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be very long. \n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present in context1 and context2. \n",
    "    4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\n",
    "    \n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "65432c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = multi_context.format(question=question,context1=extracted_context,context2=all_contexts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "069cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "240a8161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tDZdGGXKjv8TTEdVgWeSk3k9IXUX at 0x7fe9e0670310> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are the two major approaches to adapting pre-trained LLMs, and how do factors such as scaling the instructions and formatting design impact their performance?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693394573,\n",
       "  \"id\": \"chatcmpl-7tDZdGGXKjv8TTEdVgWeSk3k9IXUX\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 30,\n",
       "    \"prompt_tokens\": 1731,\n",
       "    \"total_tokens\": 1761\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "245ab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concretizing \n",
    "Concretize_prompt = \"\"\"\n",
    "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "You will be provided with a question and a context. \n",
    "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
    "Relate the original question to a real-life scenario and reframe the question. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
    "    3. The rewritten question should not ask for any external links. \n",
    "    4. Rewritten question should only add maximum 15 words into given question\n",
    "\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "03691f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = Concretize_prompt.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "f938f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
      "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
      "You will be provided with a question and a context. \n",
      "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
      "Relate the original question to a real-life scenario and reframe the question. \n",
      "Follow the rules given below while rewriting the question.\n",
      "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
      "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
      "    3. The rewritten question should not ask for any external links. \n",
      "    4. Rewritten question should only add maximum 15 words into given question\n",
      "\n",
      "question:\n",
      "What are the two commonly used pre-training tasks for training LLMs?\n",
      "context:\n",
      "20\n",
      "for position embeddings, RoPE or ALiBi is a better choice\n",
      "since it performs better on long sequences.\n",
      "4.2.3 Pre-training Tasks\n",
      "Pre-training plays a key role that encodes general knowl-\n",
      "edge from large-scale corpus into the massive model param-\n",
      "eters. For training LLMs, there are two commonly used pre-\n",
      "training tasks, namely language modeling and denoising\n",
      "autoencoding.\n",
      "Language Modeling. The language modeling task (LM) is\n",
      "the most commonly used objective to pre-train decoder-only\n",
      "LLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of\n",
      "tokens x={x1, . . . , x n}, the LM task aims to autoregres-\n",
      "sively predict the target tokens xibased on the preceding\n",
      "tokens x<iin a sequence. A general training objective is to\n",
      "maximize the following likelihood:\n",
      "LLM(x) =nX\n",
      "i=1logP(xi|x<i). (4)\n",
      "Since most language tasks can be cast as the prediction\n",
      "problem based on the input, these decoder-only LLMs might\n",
      "be potentially advantageous to implicitly learn how to ac-\n",
      "complish these tasks in a unified LM way. Some studies\n",
      "have also revealed that decoder-only LLMs can be naturally\n",
      "transferred to certain tasks by autoregressively predicting\n",
      "the next tokens [26, 55], without fine-tuning. An important\n",
      "variant of LM is the prefix language modeling task, which is\n",
      "designed for pre-training models with the prefix decoder\n",
      "architecture. The tokens within a randomly selected prefix\n",
      "would not be used in computing the loss of prefix language\n",
      "modeling. With the same amount of tokens seen during pre-\n",
      "training, prefix language modeling performs slightly worse\n",
      "than language modeling, since fewer tokens in the sequence\n",
      "are involved for model pre-training [29].\n",
      "Denoising Autoencoding. In addition to conventional\n",
      "LM, the denoising autoencoding task (DAE) has also been\n",
      "widely used to pre-train language models [24, 73]. The\n",
      "inputs x\\˜xfor DAE task are corrupted text with randomly\n",
      "replaced spans. Then, the language models are trained to re-\n",
      "cover the replaced tokens ˜x. Formally, the training objective\n",
      "of DAE is denoted as follows:\n",
      "LDAE(x) = log P(˜x|x\\˜x). (5)\n",
      "However, the DAE task seems to be more complicated\n",
      "in implementation than LM task. As a result, it has not\n",
      "been widely used to pre-train large language models. Exist-\n",
      "ing LLMs that take DAE as pre-training objectives include\n",
      "T5 [73] and GLM-130B [83]. These models are mainly trained\n",
      "to recover the replaced spans in an autoregressive way.\n",
      "Mixture-of-Denoisers. Mixture-of-Denoisers (MoD) [80],\n",
      "also known as UL2 loss, was introduced as a unified ob-\n",
      "jective for pre-training language models. MoD regards both\n",
      "LM and DAE objectives as different types of denoising tasks,\n",
      "namely S-denoiser (LM), R-denoiser (DAE, short span and\n",
      "low corruption), and X-denoiser (DAE, long span or high\n",
      "corruption). Among the three denoising tasks, S-denoiser\n",
      "is similar to the conventional LM objective (Equation (4)),\n",
      "while R-denoiser and X-denoiser are similar to DAE ob-\n",
      "jectives (Equation (5)) but differ from each other in thelengths of spans and ratio of corrupted text. For input sen-\n",
      "tences started with different special tokens ( i.e.,{[R],[S],\n",
      "[X]}), the model will be optimized using the corresponding\n",
      "denoisers. MoD has been applied in the latest PaLM 2\n",
      "model [216].\n",
      "4.2.4 Summary and Discussion\n",
      "The choice of architecture and pre-training tasks may incur\n",
      "different inductive biases for LLMs, which would lead to\n",
      "different model capacities. In this part, we discuss on several\n",
      "two open issues about LLM architecture.\n",
      "Architecture Choice . In earlier literature of pre-trained lan-\n",
      "guage models, there are lots of discussions on the effects\n",
      "of different architectures [29, 80]. However, most LLMs are\n",
      "developed based on the causal decoder architecture, and\n",
      "there still lacks a theoretical analysis on its advantage over\n",
      "the other alternatives. Next, we briefly summarize existing\n",
      "discussions on this issue.\n",
      "•By pre-training with the LM objective, it seems that\n",
      "causal decoder architecture can achieve a superior zero-\n",
      "shot and few-shot generalization capacity. Existing research\n",
      "has shown that without multi-task fine-tuning, the causal\n",
      "decoder has better zero-shot performance than other archi-\n",
      "tectures [29]. The success of GPT-3 [55] has demonstrates\n",
      "that the large causal decoder model can be a good few-\n",
      "shot learner. In addition, instruction tuning and alignment\n",
      "tuning discussed in Section 5 have been proven to fur-\n",
      "ther enhance the capability of large causal decoder mod-\n",
      "els [61, 62, 64].\n",
      "•Scaling law has been widely observed in causal de-\n",
      "coders. By scaling the model size, the dataset size, and\n",
      "the total computation, the performance of causal decoders\n",
      "can be substantially improved [30, 55]. Thus, it has become\n",
      "an important strategy to increase the model capacity of\n",
      "the causal decoder via scaling. However, more detailed\n",
      "investigation on encoder-decoder models is still lacking, and\n",
      "more efforts are needed to investigate the performance of\n",
      "encoder-decoder models at a large scale.\n",
      "More research efforts about the discussions on archi-\n",
      "tectures and pre-training objectives are in need to analyze\n",
      "how the choices of the architecture and pre-training tasks\n",
      "affect the capacity of LLMs, especially for encoder-decoder\n",
      "architectures. Besides the major architecture, the detailed\n",
      "configuration of LLM is also worth attention, which has\n",
      "been discussed in Section 4.2.2.\n",
      "Long Context . One of the main drawbacks of Transformer-\n",
      "based language models is the context length is limited due\n",
      "to the involved quadratic computational costs in both time\n",
      "and memory. Meanwhile, there is an increasing demand\n",
      "for LLM applications with long context windows, such as\n",
      "in PDF processing and story writing [217]. ChatGPT has\n",
      "recently released an updated variant with a context window\n",
      "size of up to 16K tokens, which is much longer than the\n",
      "initial one, i.e.,4K tokens. Additionally, GPT-4 was launched\n",
      "with variants with context window of 32K tokens [46]. Next,\n",
      "we discuss two important factors that support long context\n",
      "modeling for LLMs.\n",
      "•Extrapolation. In real-world applications, it is possible\n",
      "that LLMs need to process long input text that exceeds the\n",
      "maximum length of the training corpus. The ability of LLMs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "bd05de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "83b0a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs?'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "5083c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_c = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "1dd38446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs in a real-life scenario?\\n'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "4c2ff172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C at 0x7fe9d180cf40> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The specific sources of data used for pre-training Language Models (LLMs) such as GPT-3 and GPT-NeoX include webpages, conversation data, books and news, scientific data, and code.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693402013,\n",
       "  \"id\": \"chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 44,\n",
       "    \"prompt_tokens\": 1263,\n",
       "    \"total_tokens\": 1307\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(answer_formulate.format(question=question_c,context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "971f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"\n",
    "I want you act as a Prompt Rewriter.\n",
    "Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "But the rewritten prompt must be reasonable and must be understood and responded by humans.\n",
    "Your rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please\n",
    "do not omit the input in #Given Prompt#.\n",
    "You SHOULD complicate the given prompt using the following method:\n",
    "Please replace general concepts with more specific concepts. or\n",
    "You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only\n",
    "add 10 to 20 words into #Given Prompt#.\n",
    "‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in\n",
    "#Rewritten Prompt#\n",
    "#Given Prompt#:\n",
    "Answer the question using given context:\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "#Rewritten Prompt#:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "75092683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs?'"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "37f7a8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning.'"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "ada30f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tFNLQhhT9kyUOaAIwd5Rd7TRaCDh at 0x7fe9f12a5270> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Please provide an answer to the following question based on the information given in the context:\\n\\nQuestion: What are the primary methods for fine-tuning pre-trained LLM models?\\n\\nContext: Following the pre-training process, LLM models can gain a broad range of capabilities to tackle different tasks. Nonetheless, numerous studies have demonstrated that their abilities can be further customized in accordance with specific objectives. In this particular section, we will elaborate on two main strategies for adapting pre-trained LLMs, namely instruction tuning and alignment tuning.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693401499,\n",
       "  \"id\": \"chatcmpl-7tFNLQhhT9kyUOaAIwd5Rd7TRaCDh\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 103,\n",
       "    \"prompt_tokens\": 302,\n",
       "    \"total_tokens\": 405\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(test_prompt.format(question=question,context=extracted_context),temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee453d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
