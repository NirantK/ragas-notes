{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81dc5",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "* Input set of documents\n",
    "    - number of samples to generate\n",
    "    - maximum token length (input + context + answer)\n",
    "    - difficulty distribution \n",
    "    - Seed questions? \n",
    "    - randomize answer output formats\n",
    "* Output : test set with questions,contexts,answer\n",
    "\n",
    "1. Select document and part of document to frame question from (random)\n",
    "2. Formulate a question,context pair contrained on output format. \n",
    "3. Identify difficulty type \n",
    "4. Evol question using evol-instruct like paradigm to improve difficulty \n",
    "    - ask for improved reasoning - (Evol instruct C Increased Reasoning Steps Prompt)\n",
    "    - Add more contexts and frame question for multi hop\n",
    "        - Adding more context\n",
    "           1. Identiy entity from current context and retrive paras with same entity : formulate new question\n",
    "           2. Identify similar paras using sentence similarity : formulate new question\n",
    "           \n",
    "           \n",
    "#### Ideas to increase complextity\n",
    "\n",
    "- Extend question by using info in the newly added contexts.\n",
    "\n",
    "\n",
    "- Concretizing\n",
    "\n",
    "    In RAG case,this should yeild an instruction that is related to real world use-case on concept decribed in the context. \n",
    "    For example, if context speaks about different types of instructions and their use-cases\n",
    "    question will be like \"I want to train a chatbot, how to form training dataset?\"\n",
    "\n",
    "\n",
    "- Improved reasoning\n",
    "           \n",
    "- Reasoning over multiple contexts\n",
    "    - add context1, context2, context3, etc untill max tokens - x. And then ask model to formulate a question that would require reasoning over multiple contexts. \n",
    "    \n",
    "#### Open-issues\n",
    "1. How to ensure inter-document dependancy is null\n",
    "\n",
    "A question framed might have possible answers from different documents\n",
    "\n",
    "2. Questions framed by LLM are easily answerable. \n",
    "\n",
    "It feels like LLM first identifies a candidate sentence and frames question based on it. So by default the answer to questions can be located easily.\n",
    "\n",
    "3. GPT 3.5 tends to add or increase length of question when asked to create difficult question\n",
    "\n",
    "often this ends up as a unsually long question that contains two questions\n",
    "\n",
    "4. Bad content : some pages / parts of document will contain bad contents like aknowledgements, examples, etc. Those should be deprioritized.\n",
    "    - This can be partly handled by a good chunking mechanism - each part should contains atlest 2k tokens. \n",
    "    - Some overlap - split intelligently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e9980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91ccf9e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d0b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open(\"/Users/shahules/openai-key.json\"))[\"ikka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0dcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d67a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "Embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcbd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(question, generated_questions):\n",
    "        question_vec = np.asarray(Embedding.embed_query(question)).reshape(1, -1)\n",
    "        gen_question_vec = np.asarray(\n",
    "            Embedding.embed_documents(generated_questions)\n",
    "        )\n",
    "        norm = np.linalg.norm(gen_question_vec, axis=1) * np.linalg.norm(\n",
    "            question_vec, axis=1\n",
    "        )\n",
    "        return (\n",
    "            np.dot(gen_question_vec, question_vec.T).reshape(\n",
    "                -1,\n",
    "            )\n",
    "            / norm\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e91820",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb673dd",
   "metadata": {},
   "source": [
    "- random select document \n",
    "- Identify sections from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKLIST = [\"Based on the provided context\",'this context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69052348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31fe1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e45b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_doc(\"../arxiv-llm/textdata/2304.12244v2.WizardLM__Empowering_Large_Language_Models_to_Follow_Complex_Instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42a236b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^(?:\\d+\\.\\d+\\s+)?[A-Z][A-Z-\\s]+$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81d1744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374f4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21537c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f2403987",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_formulate = \"\"\"\n",
    "Your task is to formulate a question from given context satifying the rules given below:\n",
    "    1.The question should be fully answered from the given context. \n",
    "    2.The question should be framed from a part that contains non-trivial information. \n",
    "    3.The answer should to the question must be in {answer_format} format. \n",
    "    4.The answer should not contain any links. \n",
    "    5.The question should be of {difficulty} difficulty.\n",
    "    6.The question must be reasonable and must be understood and responded by humans.\n",
    "    7.Do no use phrases that refers to provided context in the question.\n",
    "    \n",
    "\n",
    "{context}:context\n",
    "\"\"\"\n",
    "\n",
    "context_formulate =  \"\"\"\\\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract sentences from given context that can be used to answer the question. \n",
    "Rules to follow while doing this task:\n",
    "    1. Your task is not to answer the question using given context but to extract sentences from given context that can be used to answer given question.\n",
    "    2. The sentences could be anywhere in the provided context, pay attention to the whole context.\n",
    "    3. While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "    4. If the answer is not present in context, you should only return the empty string.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\" \n",
    "\n",
    "context_formulatev1 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information.\"\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "context_formulatev2 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "answer_formulate = \"\"\"\n",
    "Asnwer the question using the information from the qiven context. \n",
    "You are not allowed to include information that cannot be deducted from the given context.\n",
    "question:{question}\n",
    "context:{context}\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "context_from_answer = \"\"\"\n",
    "Given question, context and answer. Locate the relevant information in the context from context that was to used to form the given answer. \n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "answer:\\n{answer}\n",
    "extracted context:\"\"\"\n",
    "\n",
    "\n",
    "content_filtering = \"\"\"\n",
    "Identify if the following content is suitable for framing a high quality question. The content should be deemed unsuitable if the content is dominated by credits, examples, disclaimer, references,acknowledgments.etc.\n",
    "content:\\n{context}\n",
    "\"\"\"\n",
    "# answer_index_formulate = \"\"\"\n",
    "# Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
    "# You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
    "# \\n\\n\n",
    "# question:\\nWhen was Einstein born?\n",
    "# context:\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "# answer: the answer to the given question can be located between character index 0 and 155 of given context.  \n",
    "# question:\\n{question}\n",
    "# context:\\n{context}\n",
    "# answer:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e3435f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = text.split('\\n\\n')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4b3c16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = content_filtering.format(context=context)\n",
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2c2fbf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7vlH9qC6ycucWjbIfIrYlh5GgF19g at 0x7fb841a5dc70> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The content is suitable for framing a high quality question.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1694000779,\n",
       "  \"id\": \"chatcmpl-7vlH9qC6ycucWjbIfIrYlh5GgF19g\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 11,\n",
       "    \"prompt_tokens\": 972,\n",
       "    \"total_tokens\": 983\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c07bc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = question_formulate.format(answer_format=\"text\",difficulty=\"medium\",context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6280e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "41845c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4d0d804b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the recommended combination of cakes for a customer with a budget of $20 and a maximum calorie limit of 600?'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4a01c153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca As an AI language model, I do not have personal preferences or opinions. However, based on\n",
      "the given information, a customer can buy either the chocolate cake ($12, 400 calories) and\n",
      "the vanilla cake ($10, 300 calories) or the strawberry cake ($8, 200 calories) and the vanilla\n",
      "cake ($10, 300 calories). Both options will fulfill the customer’s requirements of buying two\n",
      "cakes with a total budget of $20 and a maximum of 600 calories. If the customer wants to\n",
      "have a balance of flavors, they can buy the chocolate cake and the strawberry cake. If they\n",
      "want to have a balance of calories, they can buy the vanilla cake and the strawberry cake.\n",
      "ChatGPT Based on the given information, the customer has a budget of $20 and wants to buy two cakes.\n",
      "Let’s assume that the customer buys chocolate and vanilla cake. In this case, the total cost\n",
      "would be $12 + $10 = $22, which exceeds the customer’s budget.\n",
      "So, we need to find two cakes that cost no more than $20 in total. The customer also wants to\n",
      "consume no more than 600 calories. If the customer buys the chocolate cake, which has the\n",
      "highest number of calories, they can only buy one more cake to meet the calorie requirement.\n",
      "If the customer buys the vanilla cake and the strawberry cake, the total cost would be $10 +\n",
      "$8 = $18, which fits the customer’s budget. Additionally, the total number of calories in these\n",
      "two cakes is 300 + 200 = 500, which is well below the maximum limit of 600 calories.\n",
      "Therefore, based on the customer’s budget and calorie requirements, it is recommended that\n",
      "they buy the vanilla and strawberry cakes.\n",
      "References\n",
      "[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\n",
      "Advances in neural information processing systems , 33:1877–1901, 2020.\n",
      "[2]Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n",
      "Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\n",
      "human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.\n",
      "[3] OpenAI. Gpt-4 technical report, 2023.\n",
      "[4]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\n",
      "language models. arXiv preprint arXiv:2302.13971 , 2023.\n",
      "[5]Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\n",
      "Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang\n",
      "Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large\n",
      "language models. ArXiv , abs/2303.18223, 2023.\n",
      "[6]Xingwei He, Zheng-Wen Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan\n",
      "Duan, and Weizhu Chen. Annollm: Making large language models to be better crowdsourced annotators.\n",
      "ArXiv , abs/2303.16854, 2023.\n",
      "[7]Zhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu. Dr. llama: Improving small language models in\n",
      "domain-specific qa via generative data augmentation. 2023.\n",
      "[8]Jia Li, Ge Li, Yongming Li, and Zhi Jin. Enabling programming thinking in large language models toward\n",
      "code generation. 2023.\n",
      "[9]Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei\n",
      "Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler.\n",
      "Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning\n",
      "Representations , 2022.\n",
      "[10] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\n",
      "Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 ,\n",
      "2021.\n",
      "[11] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Zero-\n",
      "prompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. arXiv preprint\n",
      "arXiv:2201.06910 , 2022.\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880a66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"\"\"\n",
    "# Adolf Hitler was an Austrian-born German politician who was the dictator of Germany from 1933 until his suicide in 1945. He rose to power as the leader of the Nazi Party, becoming the chancellor in 1933 and then taking the title of Führer und Reichskanzler in 1934\n",
    "# \"\"\"\n",
    "# question = \"when was hitler born?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "921580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = context_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e211024",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af8f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_context = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "524f23b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Instruction Evolver is an LLM that uses prompts to evolve instructions, with two types: in-depth evolving and in-breadth evolving.\\nIn-Depth Evolving enhances instructions by making them more complex and difficult through five types of prompts: add constraints, deepening, concretizing, increased reasoning steps, and complicating input.\\nThe core part of In-Depth Evolving’s prompt is \"Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., ChatGPT and GPT4 [ 3]) a bit harder to handle. But the rewritten prompt must be reasonable, understood, and responded to by humans\".\\nWe require the LLM to create challenging instructions that are reasonable and not arbitrarily imagined by AI.\\nA gradual difficulty increase is necessary to avoid filling the instruction set with extremely complex instructions, which would harm the generalization performance of trained models.\\nTo control difficulty increase, we make each evolution \"a bit harder\" and restrict adding a maximum of 10 to 20 words.\\nAmong the five mentioned evolving, all can be implemented without any in-context examples except for complicating input.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0e7d6e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evolution fails. Successful evolved instructions are added to the pool, while unsuccessful ones are\n",
      "placed back as they are, with the hope of upgrading them successfully in the next evolution epoch.\n",
      "Instruction Evolver. The Instruction Evolver is an LLM that uses prompts to evolve instructions,\n",
      "with two types: in-depth evolving and in-breadth evolving.\n",
      "Prompts of In-Depth Evolving. In-Depth Evolving enhances instructions by making them more\n",
      "complex and difficult through five types of prompts: add constraints, deepening, concretizing,\n",
      "increased reasoning steps, and complicating input. The core part of In-Depth Evolving’s prompt\n",
      "is \"Your objective is to rewrite a given prompt into a more complex version to make those famous\n",
      "AI systems (e.g., ChatGPT and GPT4 [ 3]) a bit harder to handle. But the rewritten prompt must be\n",
      "reasonable, understood, and responded to by humans\". We require the LLM to create challenging\n",
      "instructions that are reasonable and not arbitrarily imagined by AI. A gradual difficulty increase\n",
      "is necessary to avoid filling the instruction set with extremely complex instructions, which would\n",
      "harm the generalization performance of trained models. To control difficulty increase, we make each\n",
      "evolution \"a bit harder\" and restrict adding a maximum of 10 to 20 words. Among the five mentioned\n",
      "evolving, all can be implemented without any in-context examples except for complicating input. We\n",
      "show the prompt of add constraints as follows (the prompts of deepening, concretizing and increased\n",
      "reasoning steps will be detailed in the Appendix A-C)\n",
      "I want you act as a Prompt Rewriter.\n",
      "Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems\n",
      "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
      "But the rewritten prompt must be reasonable and must be understood and responded by humans.\n",
      "Your rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please\n",
      "do not omit the input in #Given Prompt#.\n",
      "You SHOULD complicate the given prompt using the following method:\n",
      "Please add one more constraints/requirements into #Given Prompt#\n",
      "You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only\n",
      "add 10 to 20 words into #Given Prompt#.\n",
      "‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in\n",
      "#Rewritten Prompt#\n",
      "#Given Prompt#:\n",
      "<Here is instruction.>\n",
      "#Rewritten Prompt#:\n",
      "For complicating input, we will use in-context demonstration. Due to the lengthy demonstrations, we\n",
      "will provide a brief template below, with the full prompt detailed in the Appendix D.\n",
      "I want you act as a Prompt Rewriter.\n",
      "Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems\n",
      "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
      "But the rewritten prompt must be reasonable and must be understood and responded by humans.\n",
      "You must add [XML data] format data as input data in [Rewritten Prompt]\n",
      "#Given Prompt#:\n",
      "<Here is Demonstration instruction 1.>\n",
      "#Rewritten Prompt#:\n",
      "<Here is Demonstration Example 1.>\n",
      "... N -1 Examples ...\n",
      "I want you act as a Prompt Rewriter.\n",
      "Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems\n",
      "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
      "But the rewritten prompt must be reasonable and must be understood and responded by humans.\n",
      "You must add [#Given Dataformat#] format data as input data, add [#Given Dataformat#] code as input code\n",
      "in [Rewritten Prompt]\n",
      "Rewrite prompt must be a question style instruction\n",
      "#Given Prompt#:\n",
      "<Here is instruction.>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dfd6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7503b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = answer_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "889d4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cada2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "303fbf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Python, variable names can be written by using alphanumeric characters and underscores. Variable names cannot start with a number and cannot contain spaces or special characters.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11097aa3",
   "metadata": {},
   "source": [
    "#### Extract more context for multi-hop questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b16b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=23\n",
    "all_contexts = text.split('\\n\\n')[index:index+20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad921f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = calculate_similarity(extracted_context,all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d379103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84381185, 0.77465926, 0.88152245, 0.83331187, 0.82022018,\n",
       "       0.86879151, 0.85556014, 0.84220169, 0.81447715, 0.83058065,\n",
       "       0.80893047, 0.82643535, 0.78249331, 0.85134229, 0.82774607,\n",
       "       0.81884942, 0.84903393, 0.84885221, 0.82688037, 0.84365871])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd407200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  5,  6, 13, 16, 17,  0, 19,  7,  3,  9, 14, 18, 11,  4, 15,  8,\n",
       "       10, 12,  1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9f3e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "mance of the model. Here, we discuss some essential factors\n",
      "for instance construction.\n",
      "•Scaling the instructions. It has been widely shown that\n",
      "scaling the number of tasks can largely enhance the general-\n",
      "ization ability of LLMs [28, 62, 79]. With the increasing of the\n",
      "task number, the model performance initially shows a con-\n",
      "tinuous growth pattern, while the gain becomes negligible\n",
      "when it reaches a certain level [64, 79]. A plausible specula-\n",
      "tion is that a certain number of representative tasks can pro-\n",
      "vide relatively sufficient knowledge and adding more tasks\n",
      "may not bring additional gains [64]. Also, it is beneficial to\n",
      "enhance the diversity of the task descriptions in several as-\n",
      "pects, such as length, structure, and creativity [28]. As for the\n",
      "number of instances per task, it has been found that a small\n",
      "number of instances can usually saturate the generalization\n",
      "performance of the model [62, 64]. Whereas, increasing the\n",
      "number of instances for some tasks to a large number ( e.g.,\n",
      "a few hundreds) could potentially result in the overfitting\n",
      "issue and impair the model performance [79, 252].\n",
      "•Formatting design. As an important factor, the design\n",
      "of natural language format also highly impacts the gener-\n",
      "alization performance of LLMs [79]. Typically, we can add\n",
      "task descriptions and optional demonstrations to the input-\n",
      "output pairs of existing datasets, where the task description\n",
      "is the most key part for LLMs to understand the task [79].\n",
      "Further, it can lead to substantial improvements by using an\n",
      "appropriate number of exemplars as demonstrations [64],\n",
      "which also alleviates the model sensitivity to instruction\n",
      "engineering [62, 64]. However, incorporating other compo-\n",
      "nents ( e.g., things to avoid, reasons, and suggestions) into\n",
      "instructions may have a negligible or even adverse effect\n",
      "on the performance of LLMs [79, 240]. Recently, to elicit\n",
      "the step-by-step reasoning ability of LLMs, some work [64]\n",
      "proposes to include chain-of-thought (CoT) examples for\n",
      "some reasoning datasets, such as arithmetic reasoning. It\n",
      "has been shown that fine-tuning LLMs with both CoT and\n",
      "non-CoT examples can lead to a good performance across\n",
      "various reasoning tasks, including those that require multi-\n",
      "hop reasoning ability ( e.g., commonsense question answer-\n",
      "ing and arithmetic reasoning) as well as those without the\n",
      "need for such a reasoning way ( e.g., sentiment analysis and\n",
      "extractive question answering) [64, 85].\n",
      "To summarize, it seems that the diversity and quality\n",
      "of instructions is more important than the number of in-\n",
      "stances [253] since the well-performing InstructGPT [61] and\n",
      "Alpaca [124] utilize fewer but more diverse instructions (or\n",
      "instances) than the Flan-series LLMs [62, 64]. Further, it is\n",
      "more useful to invite labelers to compose human-need tasks\n",
      "than using dataset-specific tasks. However, it still lacks gen-\n",
      "eral guidelines to annotate human-need instances, making\n",
      "the task composition somehow heuristic. To reduce human\n",
      "efforts, we can either reuse existing formatted datasets\n",
      "(Table 6) or automatically construct the instructions using\n",
      "existing LLMs [125]. We conduct a preliminary experiment\n",
      "to show the effectiveness of different construction methods\n",
      "in Section 5.1.4.\n",
      "5.1.2 Instruction Tuning Strategies\n",
      "Unlike pre-training, instruction tuning is often more effi-\n",
      "cient since only a moderate number of instances are used\n",
      "for training. Since instruction tuning can be considered asa supervised training process, its optimization is different\n",
      "from pre-training in several aspects [64], such as the training\n",
      "objective ( i.e.,sequence-to-sequence loss) and optimization\n",
      "configuration ( e.g., smaller batch size and learning rate),\n",
      "which require special attention in practice. In addition to\n",
      "these optimization configurations, there are also two impor-\n",
      "tant aspects to consider for instruction tuning:\n",
      "Balancing the Data Distribution. Since instruction tun-\n",
      "ing involves a mixture of different tasks, it is important\n",
      "to balance the proportion of different tasks during fine-\n",
      "tuning. A widely used method is the examples-proportional\n",
      "mixing strategy [73], i.e., combining all the datasets and\n",
      "sampling each instance equally from the mixed datasets.\n",
      "Furthermore, increasing the sampling ratio of high-quality\n",
      "collections ( e.g., FLAN [62] and P3 [241]) can generally\n",
      "lead to performance improvement according to recent find-\n",
      "ings [64, 85]. Further, it is common to set a maximum cap\n",
      "to control the maximum number of examples that a dataset\n",
      "can contain during instruction tuning [73], which is set to\n",
      "prevent larger datasets from overwhelming the entire dis-\n",
      "tribution [73, 85]. In practice, the maximum cap is typically\n",
      "set to several thousands or tens of thousands according to\n",
      "different datasets [62, 64].\n",
      "Combining Instruction T uning and Pre-Training. To make\n",
      "the tuning process more effective and stable, OPT-IML [85]\n",
      "incorporates pre-training data during instruction tuning,\n",
      "which can be regarded as regularization for model tuning.\n",
      "Further, instead of using a separate two-stage process ( pre-\n",
      "training then instruction tuning ), some studies attempt to\n",
      "train a model from scratch with a mixture of pre-training\n",
      "data ( i.e.,plain texts) and instruction tuning data ( i.e.,for-\n",
      "matted datasets) using multi-task learning [73]. Specifically,\n",
      "GLM-130B [83] and Galactica [35] integrate instruction-\n",
      "formatted datasets as a small proportion of the pre-training\n",
      "corpora to pre-train LLMs, which potentially achieves the\n",
      "advantages of pre-training and instruction tuning at the\n",
      "same time.\n",
      "5.1.3 The Effect of Instruction Tuning\n",
      "In this part, we discuss the effect of instruction tuning on\n",
      "LLMs in three major aspects.\n",
      "Performance Improvement. Despite being tuned on a mod-\n",
      "erate number of instances, instruction tuning has become\n",
      "an important way to improve or unlock the abilities of\n",
      "LLMs [64]. Recent studies have experimented with language\n",
      "models in multiple scales (ranging from 77M to 540B),\n",
      "showing that the models of different scales can all benefit\n",
      "from instruction tuning [64, 251], yielding improved perfor-\n",
      "mance as the parameter scale increases [84]. Further, smaller\n",
      "models with instruction tuning can even perform better\n",
      "than larger models without fine-tuning [28, 64]. Besides\n",
      "the model scale, instruction tuning demonstrates consistent\n",
      "improvements in various model architectures, pre-training\n",
      "objectives, and model adaptation methods [64]. In practice,\n",
      "instruction tuning offers a general approach to enhancing\n",
      "the abilities of existing language models [64] (including\n",
      "small-sized PLMs). Also, it is much less costly than pre-\n",
      "training, since the amount of instruction data required by\n",
      "LLMs is significantly smaller than pre-training data.\n"
     ]
    }
   ],
   "source": [
    "print(all_contexts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b752c",
   "metadata": {},
   "source": [
    "### Complicating instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af24ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1\n",
    "## includes info in context2 and adds an 'and' part to the original question\n",
    "\n",
    "multi_context = \"\"\"\n",
    "You are a prompt rewriter. You will be provided with a question and two set of contexts namely context1 and context2. \n",
    "Your task is to complicate the given question in a way that answering it requires information derived from both context1 and context2. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be very long. \n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present in context1 and context2. \n",
    "    4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\n",
    "    5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "\n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "971f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_part = \"\"\"\n",
    "Please rephrase the following multi-part question into a single, shortened and comprehensive question that encompasses all the key elements.\n",
    "question:\\n{question}\n",
    "rewritten question:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65432c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt_input \u001b[38;5;241m=\u001b[39m multi_context\u001b[38;5;241m.\u001b[39mformat(question\u001b[38;5;241m=\u001b[39m\u001b[43mquestion\u001b[49m,context1\u001b[38;5;241m=\u001b[39mextracted_context,context2\u001b[38;5;241m=\u001b[39mall_contexts[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_input = multi_context.format(question=question,context1=extracted_context,context2=all_contexts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "240a8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionv1 = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53689adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs, and how do factors such as scaling the instructions and formatting design impact their performance?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a95156",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(single_part.format(question=questionv1),temperature=0)\n",
    "questionv1_compact = output['choices'][0]['message']['content']\n",
    "questionv1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af504e",
   "metadata": {},
   "source": [
    "### Add more reasoning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13508eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = \"\"\"\n",
    "Please rewrite the following question to make it require multi-step reasoning to answer using the provided context. Ensure that the question remains relevant to the context.\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "Rewritten Question Requiring Multi-Step Reasoning:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8397919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f5a690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb70ce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7vOaLdbKUBYZ4ESBkBghw7nKXSCJo at 0x7faba9766ef0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are the two major approaches to adapting pre-trained LLMs and what are the steps involved in instruction tuning?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693913557,\n",
       "  \"id\": \"chatcmpl-7vOaLdbKUBYZ4ESBkBghw7nKXSCJo\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 23,\n",
       "    \"prompt_tokens\": 1535,\n",
       "    \"total_tokens\": 1558\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c226a42",
   "metadata": {},
   "source": [
    "## Concretizing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "245ab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concretizing \n",
    "Concretize_prompt = \"\"\"\n",
    "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "You will be provided with a question and a context. \n",
    "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
    "Relate the original question to a real-life scenario and reframe the question. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
    "    3. The rewritten question should not ask for any external links. \n",
    "    4. Rewritten question should only add maximum 15 words into given question\n",
    "\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "03691f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = Concretize_prompt.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "f938f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
      "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
      "You will be provided with a question and a context. \n",
      "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
      "Relate the original question to a real-life scenario and reframe the question. \n",
      "Follow the rules given below while rewriting the question.\n",
      "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
      "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
      "    3. The rewritten question should not ask for any external links. \n",
      "    4. Rewritten question should only add maximum 15 words into given question\n",
      "\n",
      "question:\n",
      "What are the two commonly used pre-training tasks for training LLMs?\n",
      "context:\n",
      "20\n",
      "for position embeddings, RoPE or ALiBi is a better choice\n",
      "since it performs better on long sequences.\n",
      "4.2.3 Pre-training Tasks\n",
      "Pre-training plays a key role that encodes general knowl-\n",
      "edge from large-scale corpus into the massive model param-\n",
      "eters. For training LLMs, there are two commonly used pre-\n",
      "training tasks, namely language modeling and denoising\n",
      "autoencoding.\n",
      "Language Modeling. The language modeling task (LM) is\n",
      "the most commonly used objective to pre-train decoder-only\n",
      "LLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of\n",
      "tokens x={x1, . . . , x n}, the LM task aims to autoregres-\n",
      "sively predict the target tokens xibased on the preceding\n",
      "tokens x<iin a sequence. A general training objective is to\n",
      "maximize the following likelihood:\n",
      "LLM(x) =nX\n",
      "i=1logP(xi|x<i). (4)\n",
      "Since most language tasks can be cast as the prediction\n",
      "problem based on the input, these decoder-only LLMs might\n",
      "be potentially advantageous to implicitly learn how to ac-\n",
      "complish these tasks in a unified LM way. Some studies\n",
      "have also revealed that decoder-only LLMs can be naturally\n",
      "transferred to certain tasks by autoregressively predicting\n",
      "the next tokens [26, 55], without fine-tuning. An important\n",
      "variant of LM is the prefix language modeling task, which is\n",
      "designed for pre-training models with the prefix decoder\n",
      "architecture. The tokens within a randomly selected prefix\n",
      "would not be used in computing the loss of prefix language\n",
      "modeling. With the same amount of tokens seen during pre-\n",
      "training, prefix language modeling performs slightly worse\n",
      "than language modeling, since fewer tokens in the sequence\n",
      "are involved for model pre-training [29].\n",
      "Denoising Autoencoding. In addition to conventional\n",
      "LM, the denoising autoencoding task (DAE) has also been\n",
      "widely used to pre-train language models [24, 73]. The\n",
      "inputs x\\˜xfor DAE task are corrupted text with randomly\n",
      "replaced spans. Then, the language models are trained to re-\n",
      "cover the replaced tokens ˜x. Formally, the training objective\n",
      "of DAE is denoted as follows:\n",
      "LDAE(x) = log P(˜x|x\\˜x). (5)\n",
      "However, the DAE task seems to be more complicated\n",
      "in implementation than LM task. As a result, it has not\n",
      "been widely used to pre-train large language models. Exist-\n",
      "ing LLMs that take DAE as pre-training objectives include\n",
      "T5 [73] and GLM-130B [83]. These models are mainly trained\n",
      "to recover the replaced spans in an autoregressive way.\n",
      "Mixture-of-Denoisers. Mixture-of-Denoisers (MoD) [80],\n",
      "also known as UL2 loss, was introduced as a unified ob-\n",
      "jective for pre-training language models. MoD regards both\n",
      "LM and DAE objectives as different types of denoising tasks,\n",
      "namely S-denoiser (LM), R-denoiser (DAE, short span and\n",
      "low corruption), and X-denoiser (DAE, long span or high\n",
      "corruption). Among the three denoising tasks, S-denoiser\n",
      "is similar to the conventional LM objective (Equation (4)),\n",
      "while R-denoiser and X-denoiser are similar to DAE ob-\n",
      "jectives (Equation (5)) but differ from each other in thelengths of spans and ratio of corrupted text. For input sen-\n",
      "tences started with different special tokens ( i.e.,{[R],[S],\n",
      "[X]}), the model will be optimized using the corresponding\n",
      "denoisers. MoD has been applied in the latest PaLM 2\n",
      "model [216].\n",
      "4.2.4 Summary and Discussion\n",
      "The choice of architecture and pre-training tasks may incur\n",
      "different inductive biases for LLMs, which would lead to\n",
      "different model capacities. In this part, we discuss on several\n",
      "two open issues about LLM architecture.\n",
      "Architecture Choice . In earlier literature of pre-trained lan-\n",
      "guage models, there are lots of discussions on the effects\n",
      "of different architectures [29, 80]. However, most LLMs are\n",
      "developed based on the causal decoder architecture, and\n",
      "there still lacks a theoretical analysis on its advantage over\n",
      "the other alternatives. Next, we briefly summarize existing\n",
      "discussions on this issue.\n",
      "•By pre-training with the LM objective, it seems that\n",
      "causal decoder architecture can achieve a superior zero-\n",
      "shot and few-shot generalization capacity. Existing research\n",
      "has shown that without multi-task fine-tuning, the causal\n",
      "decoder has better zero-shot performance than other archi-\n",
      "tectures [29]. The success of GPT-3 [55] has demonstrates\n",
      "that the large causal decoder model can be a good few-\n",
      "shot learner. In addition, instruction tuning and alignment\n",
      "tuning discussed in Section 5 have been proven to fur-\n",
      "ther enhance the capability of large causal decoder mod-\n",
      "els [61, 62, 64].\n",
      "•Scaling law has been widely observed in causal de-\n",
      "coders. By scaling the model size, the dataset size, and\n",
      "the total computation, the performance of causal decoders\n",
      "can be substantially improved [30, 55]. Thus, it has become\n",
      "an important strategy to increase the model capacity of\n",
      "the causal decoder via scaling. However, more detailed\n",
      "investigation on encoder-decoder models is still lacking, and\n",
      "more efforts are needed to investigate the performance of\n",
      "encoder-decoder models at a large scale.\n",
      "More research efforts about the discussions on archi-\n",
      "tectures and pre-training objectives are in need to analyze\n",
      "how the choices of the architecture and pre-training tasks\n",
      "affect the capacity of LLMs, especially for encoder-decoder\n",
      "architectures. Besides the major architecture, the detailed\n",
      "configuration of LLM is also worth attention, which has\n",
      "been discussed in Section 4.2.2.\n",
      "Long Context . One of the main drawbacks of Transformer-\n",
      "based language models is the context length is limited due\n",
      "to the involved quadratic computational costs in both time\n",
      "and memory. Meanwhile, there is an increasing demand\n",
      "for LLM applications with long context windows, such as\n",
      "in PDF processing and story writing [217]. ChatGPT has\n",
      "recently released an updated variant with a context window\n",
      "size of up to 16K tokens, which is much longer than the\n",
      "initial one, i.e.,4K tokens. Additionally, GPT-4 was launched\n",
      "with variants with context window of 32K tokens [46]. Next,\n",
      "we discuss two important factors that support long context\n",
      "modeling for LLMs.\n",
      "•Extrapolation. In real-world applications, it is possible\n",
      "that LLMs need to process long input text that exceeds the\n",
      "maximum length of the training corpus. The ability of LLMs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "bd05de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "83b0a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs?'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "5083c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_c = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "1dd38446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs in a real-life scenario?\\n'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "4c2ff172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C at 0x7fe9d180cf40> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The specific sources of data used for pre-training Language Models (LLMs) such as GPT-3 and GPT-NeoX include webpages, conversation data, books and news, scientific data, and code.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693402013,\n",
       "  \"id\": \"chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 44,\n",
       "    \"prompt_tokens\": 1263,\n",
       "    \"total_tokens\": 1307\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(answer_formulate.format(question=question_c,context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ee2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context. Ensure that the question can be answered entirely from the information present in the context. Do not frame questions that contains more than 15 words.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Detailed Description and Rules:\n",
    "1. Understand the Context: Read the provided context carefully and understand the information it contains. Identify key pieces of information that are scattered or indirectly related to each othe\n",
    "2. Identify Multiple Steps: Think about how you can construct a question that requires the reader to connect pieces of information from different parts of the context. This may involve drawing connections between various facts or concepts.\n",
    "3. Formulate a Multi-hop Question: Craft a question that necessitates the reader to make multiple logical connections or inferences based on the information provided in the context. Here are some strategies to create multi-hop questions:\n",
    "\n",
    "   - Bridge related entities: Identify information that relates specific entities and frame question that can be answered only by analysing information of both entities.\n",
    "   \n",
    "   - Use Pronouns: identify (he, she, it, they) that refer to same entity or concepts in the context, and ask questions that would require the reader to figure out what pronouns refer to.\n",
    "\n",
    "   - Refer to Specific Details: Mention specific details or facts from different parts of the context and ask how they are related.\n",
    "\n",
    "   - Pose Hypothetical Scenarios: Present a hypothetical situation or scenario that requires combining different elements from the context to arrive at an answer.\n",
    "\n",
    "   - Ask About Cause and Effect: Inquire about the causes or effects of specific events or actions mentioned in the context, which may involve reasoning through multiple steps.\n",
    "\n",
    "4. Ensure Clarity: Make sure the question is clear and unambiguous. It should be evident to the reader that they need to connect multiple pieces of information to answer the question.\n",
    "5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "Multi-hop Reasoning Question:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3200248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(test_prompt.format(context=context))\n",
    "question_r = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc8b2241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_r.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab87ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some challenges that need to be addressed when applying LLMs to real-world scenarios?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9b58f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "some recent work has studied the human-like characteristics\n",
      "of LLMs, such as self-awareness, theory of mind (ToM), and\n",
      "affective computing [603, 604]. In particular, an empirical\n",
      "evaluation of ToM conducted on two classic false-belief\n",
      "tasks speculates that LLMs may have ToM-like abilities\n",
      "since the model in the GPT-3.5 series achieves comparable\n",
      "performance with nine-year-old children in ToM task [603].\n",
      "In addition, another line of work has investigated applying\n",
      "LLMs into the software development domain, e.g., code\n",
      "suggestion [605], code summarization [606], and automated\n",
      "program repair [607]. To summarize, to assist humans by\n",
      "LLMs in real-world tasks has become a significant area of\n",
      "research. However, it also presents challenges. Ensuring the\n",
      "accuracy of LLM-generated content, addressing biases, and\n",
      "maintaining user privacy and data security are crucial con-\n",
      "siderations when applying LLMs to real-world scenarios.\n",
      "10 C ONCLUSION AND FUTURE DIRECTIONS\n",
      "In this survey, we have reviewed the recent progress of\n",
      "large language models (LLMs), and introduced the key\n",
      "concepts, findings, and techniques for understanding and\n",
      "utilizing LLMs. We focus on the large-sized models ( i.e.,\n",
      "having a size larger than 10B) while excluding the contents\n",
      "of early pre-trained language models ( e.g., BERT and GPT-\n",
      "2) that have been well covered in the existing literature. In\n",
      "particular, our survey has discussed four important aspects\n",
      "of LLMs, i.e., pre-training, adaptation tuning, utilization,\n",
      "and evaluation. For each aspect, we highlight the techniques\n",
      "or findings that are key to the success of LLMs. Furthermore,\n",
      "we also summarize the available resources for developing\n",
      "LLMs and discuss important implementation guidelines for\n",
      "reproducing LLMs. This survey tries to cover the most\n",
      "recent literature about LLMs and provides a good reference\n",
      "resource on this topic for both researchers and engineers.\n",
      "Next, we summarize the discussions of this survey, and\n",
      "introduce the challenges and future directions for LLMs, in\n",
      "the following aspects.\n",
      "Theory and Principle. To understand the underlying work-\n",
      "ing mechanism of LLMs, one of the greatest mysteries\n",
      "is how information is distributed, organized, and utilized\n",
      "through the very large, deep neural network. It is important\n",
      "to reveal the basic principles or elements that establish the\n",
      "foundation of the abilities of LLMs. In particular, scaling\n",
      "seems to play an important role in increasing the capacity\n",
      "of LLMs [31, 55, 59]. It has been shown that some emergent\n",
      "abilities would occur in an unexpected way (a sudden per-\n",
      "formance leap) when the parameter scale of language mod-\n",
      "els increases to a critical size ( e.g., 10B) [31, 33], typically in-\n",
      "cluding in-context learning, instruction following, and step-\n",
      "by-step reasoning. These emergent abilities are fascinating\n",
      "yet perplexing: when and how they are obtained by LLMs\n",
      "are not yet clear. Recent studies either conduct extensive\n",
      "experiments for investigating the effect of emergent abilities\n",
      "and the contributing factors to such abilities [307, 608, 609],\n",
      "or explain some specific abilities with existing theoretical\n",
      "frameworks [60, 317]. An insightful technical post also spe-\n",
      "cially discusses this topic [47], taking the GPT-series models\n",
      "as the target. However, more formal theories and principles\n",
      "to understand, characterize, and explain the abilities orbehaviors of LLMs are still missing. Since emergent abilities\n",
      "bear a close analogy to phase transitions in nature [31, 58],\n",
      "cross-discipline theories or principles ( e.g., whether LLMs\n",
      "can be considered as some kind of complex systems) might\n",
      "be useful to explain and understand the behaviors of LLMs.\n",
      "These fundamental questions are worth exploring for the\n",
      "research community, which are important for developing\n",
      "the next-generation LLMs.\n",
      "Model Architecture. Due to the scalability and effective-\n",
      "ness, Transformer, consisting of stacked multi-head self-\n",
      "attention layers, has become the de facto architecture for\n",
      "building LLMs. Various strategies have been proposed to\n",
      "improve the performance of this architecture, such as neural\n",
      "network configuration and scalable parallel training (see\n",
      "discussions in Section 4.2.2). To enhance the model capacity\n",
      "(e.g., the multi-turn conversation ability), existing LLMs\n",
      "typically maintain a long context window, e.g., GPT-4-32k\n",
      "has an extremely large context length of 32,768 tokens. Thus,\n",
      "a practical consideration is to reduce the time complexity\n",
      "(originally to be quadratic costs) incurred by the standard\n",
      "self-attention mechanism. It is important to investigate the\n",
      "effect of more efficient Transformer variants in building\n",
      "LLMs [610], e.g., sparse attention has been used in GPT-\n",
      "3 [55]. Besides, catastrophic forgetting has been a long-\n",
      "standing challenge for neural networks, which also has a\n",
      "negative impact on LLMs. When tuning LLMs with new\n",
      "data, the originally learned knowledge is likely to be dam-\n",
      "aged, e.g., fine-tuning a LLM according to some specific\n",
      "tasks will affect the general ability of LLMs. A similar case\n",
      "occurs when LLMs are aligned with human values (called\n",
      "alignment tax [61, 268]). Thus, it is necessary to consider\n",
      "extending existing architectures with more flexible mech-\n",
      "anisms or modules that can effectively support data update\n",
      "and task specialization.\n",
      "Model Training. In practice, it is very difficult to pre-\n",
      "train capable LLMs, due to the huge computation con-\n",
      "sumption and the sensitivity to data quality and training\n",
      "tricks [69, 83]. Thus, it becomes particularly important to\n",
      "develop more systemic, economical pre-training approaches\n",
      "for optimizing LLMs, considering the factors of model ef-\n",
      "fectiveness, efficiency optimization, and training stability.\n",
      "More model checking or performance diagnosis methods\n",
      "(e.g., predictable scaling in GPT-4 [46]) should be developed\n",
      "in order to detect early abnormal issues during training.\n",
      "Furthermore, it also calls for more flexible mechanisms of\n",
      "hardware support or resource schedule, so as to better\n",
      "organize and utilize the resources in a computing cluster.\n",
      "Since it is very costly to pre-train a LLM from scratch, it\n",
      "is important to design a suitable mechanisms for continu-\n",
      "ally pre-training or fine-tuning the LLM based on publicly\n",
      "available model checkpoints ( e.g., LLaMA [57] and Flan-\n",
      "T5 [64]). For this purpose, a number of technical issues\n",
      "have to be resolved, e.g., catastrophic forgetting and task\n",
      "specialization. However, to date, there still lack open-source\n",
      "model checkpoints for LLMs with complete pre-processing\n",
      "and training logs ( e.g., the scripts to prepare the pre-training\n",
      "data) for reproduction. We believe that it will be of great\n",
      "value to report more technical details in open-source models\n",
      "for the research of LLMs. Furthermore, it is also important\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27507841",
   "metadata": {},
   "source": [
    "## COntent filteting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3d970ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d205c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2dc473d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=3)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "290af208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.247114151922858"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b38d169b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0, 18, 27, 19, 37, 38, 17, 21, 40, 26, 30, 16, 35,  8, 31, 11,\n",
       "         32, 41, 10, 12, 20,  4, 29,  9, 39, 22, 33, 34, 24, 15, 36, 23,\n",
       "         14, 13, 25, 28,  5,  2,  6,  7,  1,  3]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(axis=1).reshape(1,-1).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5f283671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table 1: A comparison case on Code Generation skill\\nSkill: Code Generation Difficulty: 10\\nInstruction: you must write a simple version control system, using c++\\n[Model: WizardLM ]As an AI language model, I can provide you with a basic version control\\nsystem in C++. Here is an example code that implements a simple version control system:\\n# include <iostream >\\n# include <string >\\n# include <vector >\\n# include <fstream >\\nusing namespace std;\\n// Version control system\\nclass VersionControl {\\npublic :\\n// Constructor\\nVersionControl () {\\nthis -> currentBranch = \" master \";\\nthis -> versionFile = \" version . txt \";\\n}\\n// Method to check out a branch\\nvoid checkOut ( string branch ) {\\n// Check if the branch already exists\\nif ( existBranch ( branch )) {\\n// If the branch exists , return success\\ncout << \" Branch \" << branch << \" already exists .\" << endl ;\\n} else {\\n// If the branch does not exist , create it\\ncreateBranch ( branch );\\n// Checkout the current branch\\ncheckoutBranch ();\\n}\\n}\\n// Method to check out the current branch\\nvoid checkoutBranch () {\\n// Open the version file\\nifstream versionFile ;\\nversionFile . open ( versionFilePath ());\\n// Read the version number from the version file\\nint version ;\\nversionFile >> version ;\\n// Checkout the current branch\\ncout << \" Checking out \" << currentBranch << \"...\" << endl ;\\nifstream checkoutFile ;\\ncheckoutFile . open ( currentBranchPath () + \"/\" + currentBranch + \". cpp \");\\nif ( checkoutFile ) {\\ncout << \" Success !\" << endl ;\\n} else {\\ncout << \" Error : Unable to checkout file .\" << endl ;\\n}\\n}\\n// Method to create a new branch\\nvoid createBranch ( string branch ) {\\n// Create a new branch directory\\ndirectory ( branchPath () + \"/\" + branch );\\n// Create a new version file\\nofstream versionFile ;\\nversionFile . open ( versionFilePath () + \"/\" + branch + \". txt \");\\nversionFile << \"0\" << endl ;\\nversionFile . close ();\\n}\\n17'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "309c9a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.625448980177723"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[40].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "492c2931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Deepening Prompt\\nI want you act as a Prompt Rewriter.\\nYour objective is to rewrite a given prompt into a more complex version to make those famous AI systems\\n(e.g., ChatGPT and GPT4) a bit harder to handle.\\nBut the rewritten prompt must be reasonable and must be understood and responded by humans.\\nYour rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please\\ndo not omit the input in #Given Prompt#.\\nYou SHOULD complicate the given prompt using the following method:\\nIf #Given Prompt# contains inquiries about certain issues, the depth and breadth of the inquiry can be\\nincreased. or\\nYou should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only\\nadd 10 to 20 words into #Given Prompt#.\\n‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in\\n#Rewritten Prompt#\\n#Given Prompt#:\\n<Here is instruction.>\\n#Rewritten Prompt#:\\nB Concretizing Prompt\\nI want you act as a Prompt Rewriter.\\nYour objective is to rewrite a given prompt into a more complex version to make those famous AI systems\\n(e.g., ChatGPT and GPT4) a bit harder to handle.\\nBut the rewritten prompt must be reasonable and must be understood and responded by humans.\\nYour rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please\\ndo not omit the input in #Given Prompt#.\\nYou SHOULD complicate the given prompt using the following method:\\nPlease replace general concepts with more specific concepts. or\\nYou should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only\\nadd 10 to 20 words into #Given Prompt#.\\n‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in\\n#Rewritten Prompt#\\n#Given Prompt#:\\n<Here is instruction.>\\n#Rewritten Prompt#:\\nC Increased Reasoning Steps Prompt\\nI want you act as a Prompt Rewriter.\\nYour objective is to rewrite a given prompt into a more complex version to make those famous AI systems\\n(e.g., ChatGPT and GPT4) a bit harder to handle.\\nBut the rewritten prompt must be reasonable and must be understood and responded by humans.\\nYour rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please\\ndo not omit the input in #Given Prompt#.\\nYou SHOULD complicate the given prompt using the following method:\\nIf #Given Prompt# can be solved with just a few simple thinking processes, you can rewrite it to\\nexplicitly request multiple-step reasoning.\\nYou should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only\\nadd 10 to 20 words into #Given Prompt#.\\n‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in\\n#Rewritten Prompt#\\n#Given Prompt#:\\n<Here is instruction.>\\n#Rewritten Prompt#:\\n11'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "71869544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "75011069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c446ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
