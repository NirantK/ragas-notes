{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81dc5",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "* Input set of documents\n",
    "    - number of samples to generate\n",
    "    - maximum token length (input + context + answer)\n",
    "    - difficulty distribution \n",
    "    - Seed questions? \n",
    "    - randomize answer output formats\n",
    "* Output : test set with questions,contexts,answer\n",
    "\n",
    "1. Select document and part of document to frame question from (random)\n",
    "2. Formulate a question,context pair contrained on output format. \n",
    "3. Identify difficulty type \n",
    "4. Evol question using evol-instruct like paradigm to improve difficulty \n",
    "    - ask for improved reasoning - (Evol instruct C Increased Reasoning Steps Prompt)\n",
    "    - Add more contexts and frame question for multi hop\n",
    "        - Adding more context\n",
    "           1. Identiy entity from current context and retrive paras with same entity : formulate new question\n",
    "           2. Identify similar paras using sentence similarity : formulate new question\n",
    "           \n",
    "           \n",
    "#### Ideas to increase complextity\n",
    "\n",
    "- Extend question by using info in the newly added contexts.\n",
    "\n",
    "\n",
    "- Concretizing\n",
    "\n",
    "    In RAG case,this should yeild an instruction that is related to real world use-case on concept decribed in the context. \n",
    "    For example, if context speaks about different types of instructions and their use-cases\n",
    "    question will be like \"I want to train a chatbot, how to form training dataset?\"\n",
    "\n",
    "\n",
    "- Improved reasoning\n",
    "           \n",
    "- Reasoning over multiple contexts\n",
    "    - add context1, context2, context3, etc untill max tokens - x. And then ask model to formulate a question that would require reasoning over multiple contexts. \n",
    "    \n",
    "- Condition the question - introduce new conditions\n",
    "    \n",
    "### Question filtering\n",
    "- Follow question elimination trick from evol-instruct\n",
    "- Remove phrases like 'according to given question,etc from'\n",
    "- Simplify question and decrease length \n",
    "- Length upper cut of questions (~20)\n",
    "    \n",
    "#### Open-issues\n",
    "1. How to ensure inter-document dependancy is null\n",
    "\n",
    "A question framed might have possible answers from different documents\n",
    "\n",
    "2. Questions framed by LLM are easily answerable. \n",
    "\n",
    "It feels like LLM first identifies a candidate sentence and frames question based on it. So by default the answer to questions can be located easily.\n",
    "\n",
    "3. GPT 3.5 tends to add or increase length of question when asked to create difficult question\n",
    "\n",
    "often this ends up as a unsually long question that contains two questions\n",
    "\n",
    "4. Bad content : some pages / parts of document will contain bad contents like aknowledgements, examples, etc. Those should be deprioritized.\n",
    "    - This can be partly handled by a good chunking mechanism - each part should contains atlest 2k tokens. \n",
    "    - Some overlap - split intelligently. \n",
    "    \n",
    "    \n",
    "### TODO\n",
    "- garbage filtering\n",
    "- take inspiration from algorithms used to clean pre-training data? \n",
    "- content filtering can be done with LLM but an algorithm that could prioritize chunks might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e9980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91ccf9e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4420177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbc55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open(\"/Users/shahules/openai-key.json\"))[\"ikka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa0dcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f56fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", MODEL),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d67a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "Embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fcbd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbour(text, text_list, min_similarity=0.8, top_k=3):\n",
    "        text_vec = np.asarray(Embedding.embed_query(text)).reshape(1, -1)\n",
    "        text_list_vec = np.asarray(\n",
    "            Embedding.embed_documents(text_list)\n",
    "        )\n",
    "        norm = np.linalg.norm(text_list_vec, axis=1) * np.linalg.norm(\n",
    "            text_vec, axis=1\n",
    "        )\n",
    "        similarity =  (\n",
    "            np.dot(text_list_vec, text_vec.T).reshape(\n",
    "                -1,\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        similarity  = similarity[similarity>min_similarity]\n",
    "        return similarity.argsort().tolist()[-top_k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd578ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "split long documents to max length chunks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def doc_filter(corpus,top_k=20):\n",
    "    vectorizer = TfidfVectorizer(min_df=0.05,max_df=0.5)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X = X.mean(axis=1).reshape(1,-1).argsort().tolist()[0]\n",
    "    return X[-top_k:][::-1]\n",
    "\n",
    "def split_text(text, max_length=1000):\n",
    "        chunks = []\n",
    "        corpus = text.split('\\n')\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        for chunk in corpus:\n",
    "            \n",
    "            if len(current_chunk + chunk)//4 < max_length:\n",
    "                current_chunk = \"\\n\".join([current_chunk,chunk])\n",
    "            else:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = chunk\n",
    "            \n",
    "        return chunks\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff761f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e91820",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb673dd",
   "metadata": {},
   "source": [
    "- random select document \n",
    "- Identify sections from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dadf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKLIST = [\"Based on the provided context\",'this context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e45b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../arxiv-llm/textdata\"\n",
    "text = read_doc(\"../arxiv-llm/textdata/2303.18223v11.A_Survey_of_Large_Language_Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7034b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = split_text(text)\n",
    "top_docs = doc_filter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d594b8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 13, 83, 8, 84, 3, 0, 6, 82, 33, 69, 77, 80, 19, 63, 52, 42, 23, 18]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2403987",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_formulate = \"\"\"\n",
    "Your task is to formulate a question from given context satifying the rules given below:\n",
    "    1.The question should be fully answered from the given context. \n",
    "    2.The question should be framed from a part of context that contains important information. It can also be from tables,code,etc.\n",
    "    3.The answer to the question should not contain any links. \n",
    "    4.The question should be of moderate difficulty.\n",
    "    5.The question must be reasonable and must be understood and responded by humans.\n",
    "    6.Do no use phrases like 'provided context',etc in the question\n",
    "    7.Avoid framing question using word \"and\" that can be decomposed into more than one question.\n",
    "    8. The question should not contain more than 10 words, make of use of abbreviation wherever possible.\n",
    "    \n",
    "context:{context}\n",
    "\"\"\"\n",
    "\n",
    "context_formulate =  \"\"\"\\\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract sentences from given context that can be used to answer the question. \n",
    "Rules to follow while doing this task:\n",
    "    1. Your task is not to answer the question using given context but to extract sentences from given context that can be used to answer given question.\n",
    "    2. The sentences could be anywhere in the provided context, pay attention to the whole context.\n",
    "    3. While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "    4. If the answer is not present in context, you should only return the empty string.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\" \n",
    "\n",
    "context_formulatev1 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information.\"\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "context_formulatev2 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "answer_formulate = \"\"\"\n",
    "Asnwer the question using the information from the qiven context. \n",
    "question:{question}\n",
    "context:{context}\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "context_from_answer = \"\"\"\n",
    "Given question, context and answer. Locate the relevant information in the context from context that was to used to form the given answer. \n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "answer:\\n{answer}\n",
    "extracted context:\"\"\"\n",
    "\n",
    "\n",
    "content_filtering = \"\"\"\n",
    "Identify if the following content is suitable for framing a high quality question. The content should be deemed unsuitable if the content is dominated by credits, examples, disclaimer, references,acknowledgments.etc.\n",
    "content:\\n{context}\n",
    "\"\"\"\n",
    "# answer_index_formulate = \"\"\"\n",
    "# Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
    "# You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
    "# \\n\\n\n",
    "# question:\\nWhen was Einstein born?\n",
    "# context:\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "# answer: the answer to the given question can be located between character index 0 and 155 of given context.  \n",
    "# question:\\n{question}\n",
    "# context:\\n{context}\n",
    "# answer:\n",
    "# \"\"\"\n",
    "\n",
    "resoning_single_context = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context. Ensure that the question can be answered entirely from the information present in the context. Do not frame questions that contains more than 15 words.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "resoning_multi_context = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context1 and context2. Ensure that the question can be answered entirely from the information present in the contexts. Do not frame questions that contains more than 15 words.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_multi_context = \"\"\"\n",
    " You are a prompt rewriter. You will be provided with a question and two set of contexts namely context1 and context2.\n",
    " Your task to is to complicate the given question to improve the difficulty of answering. You should do it by rewriting it into a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context1 and context2. Ensure that the question can be answered entirely from the information present in the contexts. Do not frame questions that contains more than 15 words.\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "reasoning = \"\"\"\n",
    "You are a prompt rewriter. You will be provided with a question and a long context.Your task to is to complicate the given question to improve the difficulty of answering. \n",
    "You should do complicate the question by rewriting question into a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in given context. \n",
    "Here are some strategies to create multi-hop questions:\n",
    "\n",
    "   - Bridge related entities: Identify information that relates specific entities and frame question that can be answered only by analysing information of both entities.\n",
    "   \n",
    "   - Use Pronouns: identify (he, she, it, they) that refer to same entity or concepts in the context, and ask questions that would require the reader to figure out what pronouns refer to.\n",
    "\n",
    "   - Refer to Specific Details: Mention specific details or facts from different parts of the context including tables, code, etc and ask how they are related.\n",
    "\n",
    "   - Pose Hypothetical Scenarios: Present a hypothetical situation or scenario that requires combining different elements from the context to arrive at an answer.\n",
    "\n",
    "Rules to follow when rewriting question:\n",
    "1. Ensure that the rewritten question can be answered entirely from the information present in the contexts.\n",
    "2. Do not frame questions that contains more than 15 words. Use abbreviation wherever possible.\n",
    "3. Make sure the question is clear and unambiguous. \n",
    "4. phrases like 'based on the provided context','according to the context',etc are not allowed to appear in the question.\n",
    "\n",
    "question: {question}\n",
    "CONTEXTS:\n",
    "{context}\n",
    "\n",
    "Multi-hop Reasoning Question:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reasoning_question_reformulte = \"\"\"\n",
    "Rewrite the following question to make it more indirect and shorter while retaining the essence of the original question. The goal is to create a question that conveys the same meaning but in a less direct manner.\n",
    "The rewritten question should shorter so use abbreviation wherever possible.\n",
    "Original Question:\n",
    "{question}\n",
    "\n",
    "Indirectly Rewritten Question:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fc606e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = corpus[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6280e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = question_formulate.format(context=context)\n",
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41845c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdb2f964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two settings considered to evaluate the instruction following ability of fine-tuned models?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a23527f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_neigh \u001b[38;5;241m=\u001b[39m find_neighbour(context,corpus[top_docs[\u001b[43mi\u001b[49m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:top_docs[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m20\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "top_neigh = find_neighbour(context,corpus[top_docs[i]+1:top_docs[i]+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context2 = corpus[top_docs[i]+top_neigh[0]]\n",
    "# full_context = \"\".join([f\"\\n\\ncontext{i+1}\\n\\n{ctx}\" for i,ctx in enumerate([context,context2])])\n",
    "full_context = \"\\n\\n\".join([context,context2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "921580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e211024",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af8f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionv1 = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2a98106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the process of evaluating the instruction following ability of fine-tuned models, which two distinct settings are employed and what are the primary types of data each setting utilizes?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a37707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'What are the two major approaches to adapting pre-trained LLMs, and how do factors such as scaling the instructions and formatting design impact their performance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea383f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning_question_reformulte.format(question=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52c7cb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do instruction scaling and format design affect the performance of the two main pre-trained LLM adaptation methods?'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm2(prompt_input,temperature=0)\n",
    "output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dfd6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7503b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = answer_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "889d4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cada2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "303fbf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Python, variable names can be written by using alphanumeric characters and underscores. Variable names cannot start with a number and cannot contain spaces or special characters.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b752c",
   "metadata": {},
   "source": [
    "### Complicating instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af24ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1\n",
    "## includes info in context2 and adds an 'and' part to the original question\n",
    "\n",
    "multi_context = \"\"\"\n",
    "You are a prompt rewriter. You will be provided with a question and two set of contexts namely context1 and context2. \n",
    "Your task is to complicate the given question in a way that answering it requires information derived from both context1 and context2. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be very long. Use abbreviation wherever possible.\n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present in context1 and context2. \n",
    "    4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\n",
    "    5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "\n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "971f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_part = \"\"\"\n",
    "Please rephrase the following multi-part question into a single, shortened and comprehensive question that encompasses all the key elements.\n",
    "question:\\n{question}\n",
    "rewritten question:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "65432c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [137]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt_input \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_context\u001b[49m\u001b[38;5;241m.\u001b[39mformat(question\u001b[38;5;241m=\u001b[39mquestion,context1\u001b[38;5;241m=\u001b[39mextracted_context,context2\u001b[38;5;241m=\u001b[39mcorpus[all_contexts[\u001b[38;5;241m2\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_context' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_input = multi_context.format(question=question,context1=extracted_context,context2=all_contexts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "240a8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionv1 = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53689adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs, and how do factors such as scaling the instructions and formatting design impact their performance?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05a95156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two main methods for adapting pre-trained Language Learning Models (LLMs) and how do elements like instruction scaling and formatting design affect their performance?'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm2(single_part.format(question=q),temperature=0)\n",
    "questionv1_compact = output['choices'][0]['message']['content']\n",
    "questionv1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af504e",
   "metadata": {},
   "source": [
    "### reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3238e227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 13, 83, 8, 84, 3, 0, 6, 82, 33, 69, 77, 80, 19, 63, 52, 42, 23, 18]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b2884069",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8397919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning.format(reasoning_type=resoning_single_context,context=corpus[top_docs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8f5a690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bb70ce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7w8iLF1QO2bXaFdwQtLb8iuMefLmL at 0x7fc9e9e3c5e0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are some deficiencies of existing prompting approaches and what strategies have been proposed to address them?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1694090877,\n",
       "  \"id\": \"chatcmpl-7w8iLF1QO2bXaFdwQtLb8iuMefLmL\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 18,\n",
       "    \"prompt_tokens\": 1299,\n",
       "    \"total_tokens\": 1317\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2caeae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in order to detect early abnormal issues during training.\n",
      "Furthermore, it also calls for more flexible mechanisms of\n",
      "hardware support or resource schedule, so as to better\n",
      "organize and utilize the resources in a computing cluster.\n",
      "Since it is very costly to pre-train a LLM from scratch, it\n",
      "is important to design a suitable mechanisms for continu-\n",
      "ally pre-training or fine-tuning the LLM based on publicly\n",
      "available model checkpoints ( e.g., LLaMA [57] and Flan-\n",
      "T5 [64]). For this purpose, a number of technical issues\n",
      "have to be resolved, e.g., catastrophic forgetting and task\n",
      "specialization. However, to date, there still lack open-source\n",
      "model checkpoints for LLMs with complete pre-processing\n",
      "and training logs ( e.g., the scripts to prepare the pre-training\n",
      "data) for reproduction. We believe that it will be of great\n",
      "value to report more technical details in open-source models\n",
      "for the research of LLMs. Furthermore, it is also important\n",
      "\n",
      "57\n",
      "to develop more improvement tuning strategies that effec-\n",
      "tively elicits the model abilities.\n",
      "Model Utilization. Since fine-tuning is very costly in real\n",
      "applications, prompting has become the prominent approach\n",
      "to using LLMs. By combining task descriptions and demon-\n",
      "stration examples into prompts, in-context learning (a spe-\n",
      "cial form of prompting) endows LLMs with the ability to\n",
      "perform well on new tasks, even outperforming full-data\n",
      "fine-tuned models in some cases. Furthermore, to enhance\n",
      "the ability of complex reasoning, advanced prompting tech-\n",
      "niques have been proposed, exemplified by the chain-of-\n",
      "thought (CoT) strategy, which includes the intermediate\n",
      "reasoning steps into prompts. However, existing prompt-\n",
      "ing approaches still have several deficiencies described as\n",
      "follows. Firstly, it involves considerable human efforts in\n",
      "the design of prompts. It would be quite useful to au-\n",
      "tomatically generate effective prompts for solving various\n",
      "tasks. Secondly, some complex tasks ( e.g., formal proof and\n",
      "numerical computation) require specific knowledge or logic\n",
      "rules, which may not be well expressed in natural language\n",
      "or demonstrated by examples. Thus, it is important to\n",
      "develop more informative, flexible task formatting methods\n",
      "for prompts46. Thirdly, existing prompting strategies mainly\n",
      "focus on single-turn performance. It is useful to develop\n",
      "interactive prompting mechanisms ( e.g., through natural\n",
      "language conversations) for solving complex tasks, which\n",
      "have been demonstrated to be very useful by ChatGPT.\n",
      "Safety and Alignment. Despite their capacities, LLMs pose\n",
      "similar safety challenges as small language models. For\n",
      "example, LLMs exhibit a tendency to generate hallucina-\n",
      "tions [448], which are texts that seem plausible but may be\n",
      "factually incorrect. What is worse, LLMs might be elicited by\n",
      "intentional instructions to produce harmful, biased, or toxic\n",
      "texts for malicious systems, leading to the potential risks of\n",
      "misuse [55, 61]. To have a detailed discussion of the safety\n",
      "issues of LLMs ( e.g., privacy, overreliance, disinformation,\n",
      "and influence operations), the readers can refer to the GPT-\n",
      "3/4 technical reports [46, 55]. As the major approach to\n",
      "averting these issues, reinforcement learning from human\n",
      "feedback (RLHF) [61, 100] has been widely used by in-\n",
      "corporating humans in the training loop for developing\n",
      "well-aligned LLMs. To improve the model safety, it is also\n",
      "important to include safety-relevant prompts during RLHF,\n",
      "as shown by GPT-4 [46]. However, RLHF heavily relies\n",
      "on high-quality human feedback data from professional\n",
      "labelers, making it difficult to be properly implemented in\n",
      "practice. Therefore, it is necessary to improve the RLHF\n",
      "framework for reducing the efforts of human labelers and\n",
      "seek a more efficient annotation approach with guaranteed\n",
      "data quality, e.g., LLMs can be employed to assist the\n",
      "labeling work. More recently, red teaming [115, 269] has\n",
      "been adopted for improving the model safety of LLMs,\n",
      "which utilizes the collected adversarial prompts to refine\n"
     ]
    }
   ],
   "source": [
    "print(corpus[top_docs[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c226a42",
   "metadata": {},
   "source": [
    "## Concretizing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "245ab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concretizing \n",
    "Concretize_prompt = \"\"\"\n",
    "Concretize the following question by providing a specific, real-world scenario or example that illustrates the concept or situation described in the question. The goal is to make the question more tangible and relatable.\n",
    "\n",
    "question:\\n{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1f318f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "for i in np.random.randint(0,len(top_docs),2):\n",
    "    print(i)\n",
    "    context = corpus[top_docs[i]]\n",
    "    prompt_input = question_formulate.format(context=context)\n",
    "    output = llm2(prompt_input)\n",
    "    question = output['choices'][0]['message']['content']\n",
    "    prompt_input = Concretize_prompt.format(question=question,context=context)\n",
    "    output = llm2(prompt_input)\n",
    "    question_c = output['choices'][0]['message']['content']\n",
    "    data_list.append({\n",
    "        \"question\":question,\n",
    "        \"complex\":question_c\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ec7eb6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35,  0])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,len(corpus),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fbb40602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are the commonly used model architectures for pre-training LLMs?',\n",
       "  'complex': \"In the field of natural language processing, one commonly used model architecture for pre-training Language Model (LLMs) is the Transformer model. For example, let's say a team of researchers is working on developing a chatbot that can understand and generate human-like responses. They decide to pre-train their LLM using a Transformer architecture. They feed the model with a large corpus of text data, such as books, articles, and internet forums, to learn the patterns and structures of language. This pre-training phase helps the model to capture the nuances of language and develop a strong language understanding capability. Once the pre-training is complete, the researchers fine-tune the model on a specific task, such as chatbot response generation, to make it more specialized and accurate in generating appropriate responses in a conversational context.\"},\n",
       " {'question': 'What is the dominant method for generating human feedback data in existing work?',\n",
       "  'complex': \"In the field of product development, companies often rely on user feedback to improve their products. One dominant method for generating human feedback data is through online surveys. For example, a popular social media platform may release a new feature and want to gather user opinions on its usability and satisfaction. They could create a survey with questions about the feature's functionality, ease of use, and overall experience. By distributing the survey to a sample of their user base, they can collect valuable feedback data to inform future iterations and improvements to the feature.\"}]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "03691f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = Concretize_prompt.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f938f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concretize the following question by providing a specific, real-world scenario or example that illustrates the concept or situation described in the question. The goal is to make the question more tangible and relatable.\n",
      "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
      "Relate the original question to a real-life scenario and reframe the question. \n",
      "Follow the rules given below while rewriting the question.\n",
      "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
      "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
      "    3. The rewritten question should not ask for any external links. \n",
      "    4. The rewritten question must not be more than 15 words. Keep the overall length of the question short by using abbreviations wherever possible.\n",
      "\n",
      "question:\n",
      "What are the three typical emergent abilities of LLMs mentioned in the context?\n",
      "context:\n",
      "sizes should be increased in equal scales, i.e.,having similar\n",
      "values for aandbin Equation (3).\n",
      "Though with some restricted assumptions, these scaling\n",
      "laws provide an intuitive understanding of the scaling ef-\n",
      "fect, making it feasible to predict the performance of LLMs\n",
      "during training [46]. However, some abilities ( e.g.,in-context\n",
      "learning [55]) are unpredictable according to the scaling law,\n",
      "which can be observed only when the model size exceeds a\n",
      "certain level (as discussed below).\n",
      "Emergent Abilities of LLMs . In the literature [31], emergent\n",
      "abilities of LLMs are formally defined as “the abilities that\n",
      "are not present in small models but arise in large models”,\n",
      "which is one of the most prominent features that distin-\n",
      "guish LLMs from previous PLMs. It further introduces a\n",
      "6. Here, Nc,Dcand Ccare measured in the number of non-\n",
      "embedding parameters, the number of training tokens and the number\n",
      "of FP-days, respectively. According to the original paper [30], CcandC\n",
      "should be denoted by Cmin\n",
      "c andCmin, corresponding to the optimal\n",
      "use of compute. We use the simplified notations for ease of discussions.notable characteristic when emergent abilities occur [31]:\n",
      "performance rises significantly above random when the\n",
      "scale reaches a certain level. By analogy, such an emergent\n",
      "pattern has close connections with the phenomenon of phase\n",
      "transition in physics [31, 58]. In principle, emergent abilities\n",
      "can be defined in relation to some complex tasks [31, 59],\n",
      "while we are more concerned with general abilities that\n",
      "can be applied to solve a variety of tasks. Here, we briefly\n",
      "introduce three typical emergent abilities for LLMs and\n",
      "representative models that possess such an ability7.\n",
      "•In-context learning. The in-context learning (ICL) ability\n",
      "is formally introduced by GPT-3 [55]: assuming that the\n",
      "language model has been provided with a natural language\n",
      "instruction and/or several task demonstrations, it can gen-\n",
      "erate the expected output for the test instances by com-\n",
      "pleting the word sequence of input text, without requiring\n",
      "additional training or gradient update8. Among the GPT-\n",
      "series models, the 175B GPT-3 model exhibited a strong ICL\n",
      "ability in general, but not the GPT-1 and GPT-2 models. Such\n",
      "an ability also depends on the specific downstream task. For\n",
      "example, the ICL ability can emerge on the arithmetic tasks\n",
      "(e.g., the 3-digit addition and subtraction) for the 13B GPT-3,\n",
      "but 175B GPT-3 even cannot work well on the Persian QA\n",
      "task [31].\n",
      "•Instruction following. By fine-tuning with a mixture of\n",
      "multi-task datasets formatted via natural language descrip-\n",
      "tions (called instruction tuning ), LLMs are shown to perform\n",
      "well on unseen tasks that are also described in the form\n",
      "of instructions [28, 61, 62]. With instruction tuning, LLMs\n",
      "are enabled to follow the task instructions for new tasks\n",
      "without using explicit examples, thus having an improved\n",
      "generalization ability. According to the experiments in [62],\n",
      "instruction-tuned LaMDA-PT [63] started to significantly\n",
      "outperform the untuned one on unseen tasks when the\n",
      "model size reached 68B, but not for 8B or smaller model\n",
      "sizes. A recent study [64] found that a model size of 62B is\n",
      "at least required for PaLM to perform well on various tasks\n",
      "in four evaluation benchmarks ( i.e.,MMLU, BBH, TyDiQA\n",
      "and MGSM), though a much smaller size might suffice for\n",
      "some specific tasks ( e.g., MMLU).\n",
      "•Step-by-step reasoning. For small language models, it\n",
      "is usually difficult to solve complex tasks that involve\n",
      "multiple reasoning steps, e.g., mathematical word problems.\n",
      "In contrast, with the chain-of-thought (CoT) prompting\n",
      "strategy [33], LLMs can solve such tasks by utilizing the\n",
      "prompting mechanism that involves intermediate reasoning\n",
      "steps for deriving the final answer. This ability is speculated\n",
      "to be potentially obtained by training on code [33, 47]. An\n",
      "empirical study [33] has shown that CoT prompting can\n",
      "bring performance gains (on arithmetic reasoning bench-\n",
      "rewritten question:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bd05de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "83b0a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the three typical emergent abilities of LLMs mentioned in the context?'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5083c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_c = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1dd38446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are three emergent abilities of LLMs mentioned in the context, such as in-context learning?'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "4c2ff172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C at 0x7fe9d180cf40> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The specific sources of data used for pre-training Language Models (LLMs) such as GPT-3 and GPT-NeoX include webpages, conversation data, books and news, scientific data, and code.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693402013,\n",
       "  \"id\": \"chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 44,\n",
       "    \"prompt_tokens\": 1263,\n",
       "    \"total_tokens\": 1307\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(answer_formulate.format(question=question_c,context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ee2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context. Ensure that the question can be answered entirely from the information present in the context. Do not frame questions that contains more than 15 words.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Detailed Description and Rules:\n",
    "1. Understand the Context: Read the provided context carefully and understand the information it contains. Identify key pieces of information that are scattered or indirectly related to each othe\n",
    "2. Identify Multiple Steps: Think about how you can construct a question that requires the reader to connect pieces of information from different parts of the context. This may involve drawing connections between various facts or concepts.\n",
    "3. Formulate a Multi-hop Question: Craft a question that necessitates the reader to make multiple logical connections or inferences based on the information provided in the context. Here are some strategies to create multi-hop questions:\n",
    "\n",
    "   - Bridge related entities: Identify information that relates specific entities and frame question that can be answered only by analysing information of both entities.\n",
    "   \n",
    "   - Use Pronouns: identify (he, she, it, they) that refer to same entity or concepts in the context, and ask questions that would require the reader to figure out what pronouns refer to.\n",
    "\n",
    "   - Refer to Specific Details: Mention specific details or facts from different parts of the context and ask how they are related.\n",
    "\n",
    "   - Pose Hypothetical Scenarios: Present a hypothetical situation or scenario that requires combining different elements from the context to arrive at an answer.\n",
    "\n",
    "   - Ask About Cause and Effect: Inquire about the causes or effects of specific events or actions mentioned in the context, which may involve reasoning through multiple steps.\n",
    "\n",
    "4. Ensure Clarity: Make sure the question is clear and unambiguous. It should be evident to the reader that they need to connect multiple pieces of information to answer the question.\n",
    "5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "Multi-hop Reasoning Question:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3200248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(test_prompt.format(context=context))\n",
    "question_r = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc8b2241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_r.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab87ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some challenges that need to be addressed when applying LLMs to real-world scenarios?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9b58f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "some recent work has studied the human-like characteristics\n",
      "of LLMs, such as self-awareness, theory of mind (ToM), and\n",
      "affective computing [603, 604]. In particular, an empirical\n",
      "evaluation of ToM conducted on two classic false-belief\n",
      "tasks speculates that LLMs may have ToM-like abilities\n",
      "since the model in the GPT-3.5 series achieves comparable\n",
      "performance with nine-year-old children in ToM task [603].\n",
      "In addition, another line of work has investigated applying\n",
      "LLMs into the software development domain, e.g., code\n",
      "suggestion [605], code summarization [606], and automated\n",
      "program repair [607]. To summarize, to assist humans by\n",
      "LLMs in real-world tasks has become a significant area of\n",
      "research. However, it also presents challenges. Ensuring the\n",
      "accuracy of LLM-generated content, addressing biases, and\n",
      "maintaining user privacy and data security are crucial con-\n",
      "siderations when applying LLMs to real-world scenarios.\n",
      "10 C ONCLUSION AND FUTURE DIRECTIONS\n",
      "In this survey, we have reviewed the recent progress of\n",
      "large language models (LLMs), and introduced the key\n",
      "concepts, findings, and techniques for understanding and\n",
      "utilizing LLMs. We focus on the large-sized models ( i.e.,\n",
      "having a size larger than 10B) while excluding the contents\n",
      "of early pre-trained language models ( e.g., BERT and GPT-\n",
      "2) that have been well covered in the existing literature. In\n",
      "particular, our survey has discussed four important aspects\n",
      "of LLMs, i.e., pre-training, adaptation tuning, utilization,\n",
      "and evaluation. For each aspect, we highlight the techniques\n",
      "or findings that are key to the success of LLMs. Furthermore,\n",
      "we also summarize the available resources for developing\n",
      "LLMs and discuss important implementation guidelines for\n",
      "reproducing LLMs. This survey tries to cover the most\n",
      "recent literature about LLMs and provides a good reference\n",
      "resource on this topic for both researchers and engineers.\n",
      "Next, we summarize the discussions of this survey, and\n",
      "introduce the challenges and future directions for LLMs, in\n",
      "the following aspects.\n",
      "Theory and Principle. To understand the underlying work-\n",
      "ing mechanism of LLMs, one of the greatest mysteries\n",
      "is how information is distributed, organized, and utilized\n",
      "through the very large, deep neural network. It is important\n",
      "to reveal the basic principles or elements that establish the\n",
      "foundation of the abilities of LLMs. In particular, scaling\n",
      "seems to play an important role in increasing the capacity\n",
      "of LLMs [31, 55, 59]. It has been shown that some emergent\n",
      "abilities would occur in an unexpected way (a sudden per-\n",
      "formance leap) when the parameter scale of language mod-\n",
      "els increases to a critical size ( e.g., 10B) [31, 33], typically in-\n",
      "cluding in-context learning, instruction following, and step-\n",
      "by-step reasoning. These emergent abilities are fascinating\n",
      "yet perplexing: when and how they are obtained by LLMs\n",
      "are not yet clear. Recent studies either conduct extensive\n",
      "experiments for investigating the effect of emergent abilities\n",
      "and the contributing factors to such abilities [307, 608, 609],\n",
      "or explain some specific abilities with existing theoretical\n",
      "frameworks [60, 317]. An insightful technical post also spe-\n",
      "cially discusses this topic [47], taking the GPT-series models\n",
      "as the target. However, more formal theories and principles\n",
      "to understand, characterize, and explain the abilities orbehaviors of LLMs are still missing. Since emergent abilities\n",
      "bear a close analogy to phase transitions in nature [31, 58],\n",
      "cross-discipline theories or principles ( e.g., whether LLMs\n",
      "can be considered as some kind of complex systems) might\n",
      "be useful to explain and understand the behaviors of LLMs.\n",
      "These fundamental questions are worth exploring for the\n",
      "research community, which are important for developing\n",
      "the next-generation LLMs.\n",
      "Model Architecture. Due to the scalability and effective-\n",
      "ness, Transformer, consisting of stacked multi-head self-\n",
      "attention layers, has become the de facto architecture for\n",
      "building LLMs. Various strategies have been proposed to\n",
      "improve the performance of this architecture, such as neural\n",
      "network configuration and scalable parallel training (see\n",
      "discussions in Section 4.2.2). To enhance the model capacity\n",
      "(e.g., the multi-turn conversation ability), existing LLMs\n",
      "typically maintain a long context window, e.g., GPT-4-32k\n",
      "has an extremely large context length of 32,768 tokens. Thus,\n",
      "a practical consideration is to reduce the time complexity\n",
      "(originally to be quadratic costs) incurred by the standard\n",
      "self-attention mechanism. It is important to investigate the\n",
      "effect of more efficient Transformer variants in building\n",
      "LLMs [610], e.g., sparse attention has been used in GPT-\n",
      "3 [55]. Besides, catastrophic forgetting has been a long-\n",
      "standing challenge for neural networks, which also has a\n",
      "negative impact on LLMs. When tuning LLMs with new\n",
      "data, the originally learned knowledge is likely to be dam-\n",
      "aged, e.g., fine-tuning a LLM according to some specific\n",
      "tasks will affect the general ability of LLMs. A similar case\n",
      "occurs when LLMs are aligned with human values (called\n",
      "alignment tax [61, 268]). Thus, it is necessary to consider\n",
      "extending existing architectures with more flexible mech-\n",
      "anisms or modules that can effectively support data update\n",
      "and task specialization.\n",
      "Model Training. In practice, it is very difficult to pre-\n",
      "train capable LLMs, due to the huge computation con-\n",
      "sumption and the sensitivity to data quality and training\n",
      "tricks [69, 83]. Thus, it becomes particularly important to\n",
      "develop more systemic, economical pre-training approaches\n",
      "for optimizing LLMs, considering the factors of model ef-\n",
      "fectiveness, efficiency optimization, and training stability.\n",
      "More model checking or performance diagnosis methods\n",
      "(e.g., predictable scaling in GPT-4 [46]) should be developed\n",
      "in order to detect early abnormal issues during training.\n",
      "Furthermore, it also calls for more flexible mechanisms of\n",
      "hardware support or resource schedule, so as to better\n",
      "organize and utilize the resources in a computing cluster.\n",
      "Since it is very costly to pre-train a LLM from scratch, it\n",
      "is important to design a suitable mechanisms for continu-\n",
      "ally pre-training or fine-tuning the LLM based on publicly\n",
      "available model checkpoints ( e.g., LLaMA [57] and Flan-\n",
      "T5 [64]). For this purpose, a number of technical issues\n",
      "have to be resolved, e.g., catastrophic forgetting and task\n",
      "specialization. However, to date, there still lack open-source\n",
      "model checkpoints for LLMs with complete pre-processing\n",
      "and training logs ( e.g., the scripts to prepare the pre-training\n",
      "data) for reproduction. We believe that it will be of great\n",
      "value to report more technical details in open-source models\n",
      "for the research of LLMs. Furthermore, it is also important\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6864e1",
   "metadata": {},
   "source": [
    "## Condition on origial question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16153e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_prompt = \"\"\"\n",
    "Rewrite the provided question to increase its complexity by introducing a conditional element.\n",
    "The goal is to make the question more intricate by incorporating a scenario or condition that affects the context of the question.\n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be longer than 25 words. Use abbreviation wherever possible.\n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present context.\n",
    "    4. phrases like 'provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "for example,\n",
    "question: What are the general principles for designing prompts in LLMs?\n",
    "Rewritten Question:how to apply prompt designing principles to improve LLMs performance in reasoning tasks\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "Rewritten Question\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de6f9d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two settings considered to evaluate the instruction following ability of fine-tuned models?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b013705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two settings considered to evaluate the instruction following ability of fine-tuned models?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2c7fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = condition_prompt.format(question=question,context=context)\n",
    "output = llm2(prompt_input,temperature=0)\n",
    "question_cond = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f72cc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Considering the evaluation of fine-tuned models' instruction following ability, what are the two settings employed and how do they differ in their approach?\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "how to apply prompt designing principles to improve it's performance in reasoning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af31a1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7wTXeogoap5L9TqI2PvCsXJbSlHf1 at 0x7f82ec37d3b0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are the implications of using LLMs in educational settings and what potential issues can arise from their application?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1694170938,\n",
       "  \"id\": \"chatcmpl-7wTXeogoap5L9TqI2PvCsXJbSlHf1\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 22,\n",
       "    \"prompt_tokens\": 71,\n",
       "    \"total_tokens\": 93\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(single_part.format(question=question_cond))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d09c4",
   "metadata": {},
   "source": [
    "## Reformat question into conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0feb3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = \"\"\"\n",
    "Reformat the provided question into two or three separate questions that could be part of a conversation. Each question should focus on a specific aspect or subtopic related to the original question.\n",
    "question:{question}\n",
    "\n",
    "Reformatted Questions for Conversation:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5cf04df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Considering the evaluation of fine-tuned models' instruction following ability, what are the two settings employed and how do they differ in their approach?\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3968b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = conversation.format(question=question_cond)\n",
    "output = llm2(prompt_input,temperature=0)\n",
    "questions_conv = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84db1e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. What are the two settings employed in the evaluation of fine-tuned models' instruction following ability?\\n2. How do these two settings differ in their approach?\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d9260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
