{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81dc5",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "* Input set of documents\n",
    "    - number of samples to generate\n",
    "    - maximum token length (input + context + answer)\n",
    "    - difficulty distribution \n",
    "    - Seed questions? \n",
    "    - randomize answer output formats\n",
    "* Output : test set with questions,contexts,answer\n",
    "\n",
    "1. Select document and part of document to frame question from (random)\n",
    "2. Formulate a question,context pair contrained on output format. \n",
    "3. Identify difficulty type \n",
    "4. Evol question using evol-instruct like paradigm to improve difficulty \n",
    "    - ask for improved reasoning - (Evol instruct C Increased Reasoning Steps Prompt)\n",
    "    - Add more contexts and frame question for multi hop\n",
    "        - Adding more context\n",
    "           1. Identiy entity from current context and retrive paras with same entity : formulate new question\n",
    "           2. Identify similar paras using sentence similarity : formulate new question\n",
    "           \n",
    "           \n",
    "#### Ideas to increase complextity\n",
    "\n",
    "- Extend question by using info in the newly added contexts.\n",
    "\n",
    "\n",
    "- Concretizing\n",
    "\n",
    "    In RAG case,this should yeild an instruction that is related to real world use-case on concept decribed in the context. \n",
    "    For example, if context speaks about different types of instructions and their use-cases\n",
    "    question will be like \"I want to train a chatbot, how to form training dataset?\"\n",
    "\n",
    "\n",
    "- Improved reasoning\n",
    "           \n",
    "#### Open-issues\n",
    "1. How to ensure inter-document dependancy is null\n",
    "\n",
    "A question framed might have possible answers from different documents\n",
    "\n",
    "2. Questions framed by LLM are easily answerable. \n",
    "\n",
    "It feels like LLM first identifies a candidate sentence and frames question based on it. So by default the answer to questions can be located easily.\n",
    "\n",
    "3. GPT 3.5 tends to add or increase length of question when asked to create difficult question\n",
    "\n",
    "often this ends up as a unsually long question that contains two questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e9980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91ccf9e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open(\"/Users/shahules/openai-key.json\"))[\"ikka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0dcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d67a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "Embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcbd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(question, generated_questions):\n",
    "        question_vec = np.asarray(Embedding.embed_query(question)).reshape(1, -1)\n",
    "        gen_question_vec = np.asarray(\n",
    "            Embedding.embed_documents(generated_questions)\n",
    "        )\n",
    "        norm = np.linalg.norm(gen_question_vec, axis=1) * np.linalg.norm(\n",
    "            question_vec, axis=1\n",
    "        )\n",
    "        return (\n",
    "            np.dot(gen_question_vec, question_vec.T).reshape(\n",
    "                -1,\n",
    "            )\n",
    "            / norm\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e91820",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb673dd",
   "metadata": {},
   "source": [
    "- random select document \n",
    "- Identify sections from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69052348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31fe1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_doc(\"../arxiv-llm/textdata/2303.18223v11.A_Survey_of_Large_Language_Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a236b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^(?:\\d+\\.\\d+\\s+)?[A-Z][A-Z-\\s]+$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d1744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374f4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21537c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2403987",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_formulate = \"\"\"\n",
    "Your task is to formulate a question from given context satifying the rules given below:\n",
    "    1.The question should be fully answered from the given context. \n",
    "    2.The question should be framed from a part that contains non-trivial information. \n",
    "    3.The answer should to the question must be in {answer_format} format. \n",
    "    4.The answer should not contain any links. \n",
    "    5.The question should be of {difficulty} difficulty.\n",
    "    6.The question must be reasonable and must be understood and responded by humans.\n",
    "{context}:context\n",
    "\"\"\"\n",
    "\n",
    "context_formulate =  \"\"\"\\\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract sentences from given context that can be used to answer the question. \n",
    "Rules to follow while doing this task:\n",
    "    1. Your task is not to answer the question using given context but to extract sentences from given context that can be used to answer given question.\n",
    "    2. The sentences could be anywhere in the provided context, pay attention to the whole context.\n",
    "    3. While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "    4. If the answer is not present in context, you should only return the empty string.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\" \n",
    "\n",
    "context_formulatev1 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information.\"\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "context_formulatev2 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "answer_formulate = \"\"\"\n",
    "Asnwer the question using the information from the qiven context. \n",
    "You are not allowed to include information that cannot be deducted from the given context.\n",
    "question:{question}\n",
    "context:{context}\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "context_from_answer = \"\"\"\n",
    "Given question, context and answer. Locate the relevant information in the context from context that was to used to form the given answer. \n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "answer:\\n{answer}\n",
    "extracted context:\"\"\"\n",
    "\n",
    "\n",
    "# answer_index_formulate = \"\"\"\n",
    "# Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
    "# You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
    "# \\n\\n\n",
    "# question:\\nWhen was Einstein born?\n",
    "# context:\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "# answer: the answer to the given question can be located between character index 0 and 155 of given context.  \n",
    "# question:\\n{question}\n",
    "# context:\\n{context}\n",
    "# answer:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3435f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = text.split('\\n\\n')[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c07bc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = question_formulate.format(answer_format=\"text\",difficulty=\"medium\",context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6280e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41845c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d0d804b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880a66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"\"\"\n",
    "# Adolf Hitler was an Austrian-born German politician who was the dictator of Germany from 1933 until his suicide in 1945. He rose to power as the leader of the Nazi Party, becoming the chancellor in 1933 and then taking the title of Führer und Reichskanzler in 1934\n",
    "# \"\"\"\n",
    "# question = \"when was hitler born?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "921580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = context_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e211024",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af8f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_context = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e7d6e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23\\noverhead, and the third solution increases about 50% com-\\nmunication overhead but saves memory proportional to\\nthe number of GPUs. PyTorch has implemented a similar\\ntechnique as ZeRO, called FSDP [237].\\nMixed Precision Training. In previous PLMs ( e.g.,\\nBERT [23]), 32-bit floating-point numbers, also known as\\nFP32, have been predominantly used for pre-training. In\\nrecent years, to pre-train extremely large language models,\\nsome studies [232] have started to utilize 16-bit floating-\\npoint numbers (FP16), which reduces memory usage and\\ncommunication overhead. Additionally, as popular NVIDIA\\nGPUs ( e.g., A100) have twice the amount of FP16 computa-\\ntion units as FP32, the computational efficiency of FP16 can\\nbe further improved. However, existing work has found that\\nFP16 may lead to the loss of computational accuracy [59, 69],\\nwhich affects the final model performance. To alleviate it, an\\nalternative called Brain Floating Point (BF16) has been used\\nfor training, which allocates more exponent bits and fewer\\nsignificant bits than FP16. For pre-training, BF16 generally\\nperforms better than FP16 on representation accuracy [69].\\nOverall Training Suggestion. In practice, the above train-\\ning techniques, especially 3D parallelism, are often jointly\\nused to improve the training throughput and large model\\nloading. For instance, researchers have incorporated 8-way\\ndata parallelism, 4-way tensor parallelism, and 12-way\\npipeline parallelism, enabling the training of BLOOM [69]\\non 384 A100 GPUs. Currently, open-source libraries like\\nDeepSpeed [65], Colossal-AI [149], and Alpa [238] can well\\nsupport the three parallel training methods. To reduce the\\nmemory redundancy, ZeRO, FSDP , and activation recom-\\nputation techniques [68, 239] can be also employed for\\ntraining LLMs, which have already been integrated into\\nDeepSpeed, PyTorch, and Megatron-LM. In addition, the\\nmixed precision training technique such as BF16 can be\\nalso leveraged to improve the training efficiency and reduce\\nGPU memory usage, while it requires necessary support on\\nhardware ( e.g., A100 GPU). Because training large models is\\na time-intensive process, it would be useful to forecast the\\nmodel performance and detect abnormal issues at an early\\nstage. For this purpose, GPT-4 [46] has recently introduced\\na new mechanism called predictable scaling built on a deep\\nlearning stack, enabling the performance prediction of large\\nmodels with a much smaller model, which might be quite\\nuseful for developing LLMs. In practice, one can further\\nleverage the supporting training techniques of mainstream\\ndeep learning frameworks. For instance, PyTorch supports\\nthe data parallel training algorithm FSDP [237] ( i.e.,fully\\nsharded data parallel), which allows for partial offloading\\nof training computations to CPUs if desired.\\n5 A DAPTATION OF LLM S\\nAfter pre-training, LLMs can acquire the general abilities\\nfor solving various tasks. However, an increasing number\\nof studies have shown that LLM’s abilities can be further\\nadapted according to specific goals. In this section, we\\nintroduce two major approaches to adapting pre-trained\\nLLMs, namely instruction tuning and alignment tuning. The\\nformer approach mainly aims to enhance (or unlock) the\\nabilities of LLMs, while the latter approach aims to align thebehaviors of LLMs with human values or preferences. Fur-\\nther, we will also discuss efficient tuning and quantization\\nfor model adaptation in resource-limited settings. In what\\nfollows, we will introduce the four parts in detail.\\nTABLE 6: A detailed list of available collections for instruc-\\ntion tuning.\\nCategories Collections Time #Examples\\nTaskNat. Inst. [240] Apr-2021 193K\\nFLAN [62] Sep-2021 4.4M\\nP3 [241] Oct-2021 12.1M\\nSuper Nat. Inst. [79] Apr-2022 5M\\nMVPCorpus [242] Jun-2022 41M\\nxP3 [84] Nov-2022 81M\\nOIG22Mar-2023 43M\\nChatHH-RLHF [243] Apr-2022 160K\\nHC3 [244] Jan-2023 87K\\nShareGPT23Mar-2023 90K\\nDolly24Apr-2023 15K\\nOpenAssistant [245] Apr-2023 161K\\nSyntheticSelf-Instruct [125] Dec-2022 82K\\nAlpaca [119] Mar-2023 52K\\nGuanaco25Mar-2023 535K\\nBaize [246] Apr-2023 158K\\nBELLE [247] Apr-2023 1.5M\\n5.1 Instruction Tuning\\nIn essence, instruction tuning is the approach to fine-tuning\\npre-trained LLMs on a collection of formatted instances in\\nthe form of natural language [62], which is highly related\\nto supervised fine-tuning [61] and multi-task prompted\\ntraining [28]. In order to perform instruction tuning, we first\\nneed to collect or construct instruction-formatted instances.\\nThen, we employ these formatted instances to fine-tune\\nLLMs in a supervised learning way ( e.g., training with the\\nsequence-to-sequence loss). After instruction tuning, LLMs\\ncan demonstrate superior abilities to generalize to unseen\\ntasks [28, 62, 64], even in a multilingual setting [84].\\nA recent survey [248] presents a systematic overview\\nof the research on instruction tuning. In comparison to\\nthat, we mainly focus on the effect of instruction tuning\\non LLMs and provide detailed guidelines or strategies for\\ninstance collection and tuning. In addition, we also discuss\\nthe use of instruction tuning for satisfying the real needs of\\nusers, which has been widely applied in existing LLMs, e.g.,\\nInstructGPT [61] and GPT-4 [46].\\n5.1.1 Formatted Instance Construction\\nGenerally, an instruction-formatted instance consists of a\\ntask description (called an instruction ), an optional input,\\nthe corresponding output, and a small number of demon-\\nstrations (optional). As important public resources, exist-\\ning studies have released a large number of labeled data\\nformatted in natural language (see the list of available re-\\nsources in Table 6). Next, we introduce three major methods\\nfor constructing formatted instances (see an illustration in\\n22. https://laion.ai/blog/oig-dataset/\\n23. https://sharegpt.com/\\n24. https://github.com/databrickslabs/dolly\\n25. https://huggingface.co/datasets/JosephusCheung/GuanacoDataset'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dfd6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7503b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = answer_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "889d4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cada2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "303fbf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The two major approaches to adapting pre-trained LLMs are instruction tuning and alignment tuning.'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11097aa3",
   "metadata": {},
   "source": [
    "#### Extract more context for multi-hop questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b16b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=23\n",
    "all_contexts = text.split('\\n\\n')[index:index+20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad921f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = calculate_similarity(extracted_context,all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d379103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84381185, 0.77465926, 0.88152245, 0.83331187, 0.82022018,\n",
       "       0.86879151, 0.85556014, 0.84220169, 0.81447715, 0.83058065,\n",
       "       0.80893047, 0.82643535, 0.78249331, 0.85134229, 0.82774607,\n",
       "       0.81884942, 0.84903393, 0.84885221, 0.82688037, 0.84365871])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd407200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  5,  6, 13, 16, 17,  0, 19,  7,  3,  9, 14, 18, 11,  4, 15,  8,\n",
       "       10, 12,  1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9f3e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "mance of the model. Here, we discuss some essential factors\n",
      "for instance construction.\n",
      "•Scaling the instructions. It has been widely shown that\n",
      "scaling the number of tasks can largely enhance the general-\n",
      "ization ability of LLMs [28, 62, 79]. With the increasing of the\n",
      "task number, the model performance initially shows a con-\n",
      "tinuous growth pattern, while the gain becomes negligible\n",
      "when it reaches a certain level [64, 79]. A plausible specula-\n",
      "tion is that a certain number of representative tasks can pro-\n",
      "vide relatively sufficient knowledge and adding more tasks\n",
      "may not bring additional gains [64]. Also, it is beneficial to\n",
      "enhance the diversity of the task descriptions in several as-\n",
      "pects, such as length, structure, and creativity [28]. As for the\n",
      "number of instances per task, it has been found that a small\n",
      "number of instances can usually saturate the generalization\n",
      "performance of the model [62, 64]. Whereas, increasing the\n",
      "number of instances for some tasks to a large number ( e.g.,\n",
      "a few hundreds) could potentially result in the overfitting\n",
      "issue and impair the model performance [79, 252].\n",
      "•Formatting design. As an important factor, the design\n",
      "of natural language format also highly impacts the gener-\n",
      "alization performance of LLMs [79]. Typically, we can add\n",
      "task descriptions and optional demonstrations to the input-\n",
      "output pairs of existing datasets, where the task description\n",
      "is the most key part for LLMs to understand the task [79].\n",
      "Further, it can lead to substantial improvements by using an\n",
      "appropriate number of exemplars as demonstrations [64],\n",
      "which also alleviates the model sensitivity to instruction\n",
      "engineering [62, 64]. However, incorporating other compo-\n",
      "nents ( e.g., things to avoid, reasons, and suggestions) into\n",
      "instructions may have a negligible or even adverse effect\n",
      "on the performance of LLMs [79, 240]. Recently, to elicit\n",
      "the step-by-step reasoning ability of LLMs, some work [64]\n",
      "proposes to include chain-of-thought (CoT) examples for\n",
      "some reasoning datasets, such as arithmetic reasoning. It\n",
      "has been shown that fine-tuning LLMs with both CoT and\n",
      "non-CoT examples can lead to a good performance across\n",
      "various reasoning tasks, including those that require multi-\n",
      "hop reasoning ability ( e.g., commonsense question answer-\n",
      "ing and arithmetic reasoning) as well as those without the\n",
      "need for such a reasoning way ( e.g., sentiment analysis and\n",
      "extractive question answering) [64, 85].\n",
      "To summarize, it seems that the diversity and quality\n",
      "of instructions is more important than the number of in-\n",
      "stances [253] since the well-performing InstructGPT [61] and\n",
      "Alpaca [124] utilize fewer but more diverse instructions (or\n",
      "instances) than the Flan-series LLMs [62, 64]. Further, it is\n",
      "more useful to invite labelers to compose human-need tasks\n",
      "than using dataset-specific tasks. However, it still lacks gen-\n",
      "eral guidelines to annotate human-need instances, making\n",
      "the task composition somehow heuristic. To reduce human\n",
      "efforts, we can either reuse existing formatted datasets\n",
      "(Table 6) or automatically construct the instructions using\n",
      "existing LLMs [125]. We conduct a preliminary experiment\n",
      "to show the effectiveness of different construction methods\n",
      "in Section 5.1.4.\n",
      "5.1.2 Instruction Tuning Strategies\n",
      "Unlike pre-training, instruction tuning is often more effi-\n",
      "cient since only a moderate number of instances are used\n",
      "for training. Since instruction tuning can be considered asa supervised training process, its optimization is different\n",
      "from pre-training in several aspects [64], such as the training\n",
      "objective ( i.e.,sequence-to-sequence loss) and optimization\n",
      "configuration ( e.g., smaller batch size and learning rate),\n",
      "which require special attention in practice. In addition to\n",
      "these optimization configurations, there are also two impor-\n",
      "tant aspects to consider for instruction tuning:\n",
      "Balancing the Data Distribution. Since instruction tun-\n",
      "ing involves a mixture of different tasks, it is important\n",
      "to balance the proportion of different tasks during fine-\n",
      "tuning. A widely used method is the examples-proportional\n",
      "mixing strategy [73], i.e., combining all the datasets and\n",
      "sampling each instance equally from the mixed datasets.\n",
      "Furthermore, increasing the sampling ratio of high-quality\n",
      "collections ( e.g., FLAN [62] and P3 [241]) can generally\n",
      "lead to performance improvement according to recent find-\n",
      "ings [64, 85]. Further, it is common to set a maximum cap\n",
      "to control the maximum number of examples that a dataset\n",
      "can contain during instruction tuning [73], which is set to\n",
      "prevent larger datasets from overwhelming the entire dis-\n",
      "tribution [73, 85]. In practice, the maximum cap is typically\n",
      "set to several thousands or tens of thousands according to\n",
      "different datasets [62, 64].\n",
      "Combining Instruction T uning and Pre-Training. To make\n",
      "the tuning process more effective and stable, OPT-IML [85]\n",
      "incorporates pre-training data during instruction tuning,\n",
      "which can be regarded as regularization for model tuning.\n",
      "Further, instead of using a separate two-stage process ( pre-\n",
      "training then instruction tuning ), some studies attempt to\n",
      "train a model from scratch with a mixture of pre-training\n",
      "data ( i.e.,plain texts) and instruction tuning data ( i.e.,for-\n",
      "matted datasets) using multi-task learning [73]. Specifically,\n",
      "GLM-130B [83] and Galactica [35] integrate instruction-\n",
      "formatted datasets as a small proportion of the pre-training\n",
      "corpora to pre-train LLMs, which potentially achieves the\n",
      "advantages of pre-training and instruction tuning at the\n",
      "same time.\n",
      "5.1.3 The Effect of Instruction Tuning\n",
      "In this part, we discuss the effect of instruction tuning on\n",
      "LLMs in three major aspects.\n",
      "Performance Improvement. Despite being tuned on a mod-\n",
      "erate number of instances, instruction tuning has become\n",
      "an important way to improve or unlock the abilities of\n",
      "LLMs [64]. Recent studies have experimented with language\n",
      "models in multiple scales (ranging from 77M to 540B),\n",
      "showing that the models of different scales can all benefit\n",
      "from instruction tuning [64, 251], yielding improved perfor-\n",
      "mance as the parameter scale increases [84]. Further, smaller\n",
      "models with instruction tuning can even perform better\n",
      "than larger models without fine-tuning [28, 64]. Besides\n",
      "the model scale, instruction tuning demonstrates consistent\n",
      "improvements in various model architectures, pre-training\n",
      "objectives, and model adaptation methods [64]. In practice,\n",
      "instruction tuning offers a general approach to enhancing\n",
      "the abilities of existing language models [64] (including\n",
      "small-sized PLMs). Also, it is much less costly than pre-\n",
      "training, since the amount of instruction data required by\n",
      "LLMs is significantly smaller than pre-training data.\n"
     ]
    }
   ],
   "source": [
    "print(all_contexts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b752c",
   "metadata": {},
   "source": [
    "### Complicating instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af24ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1\n",
    "## includes info in context2 and adds an 'and' part to the original question\n",
    "\n",
    "multi_context = \"\"\"\n",
    "You are a prompt rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "You will be provided with a question and two set of contexts namely context1 and context2. \n",
    "Your task is to complicate the given question in a way that answering it requires information derived from both context1 and context2. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be very long. \n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present in context1 and context2. \n",
    "    4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\n",
    "    \n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "971f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"\n",
    "Please rephrase the following multi-part question into a single, shortened and comprehensive question that encompasses all the key elements.\n",
    "question:\\n{question}\n",
    "rewritten question:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65432c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = multi_context.format(question=question,context1=extracted_context,context2=all_contexts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "240a8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionv1 = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53689adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs, and how do factors such as scaling the instructions and formatting design impact their performance?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a95156",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(test_prompt.format(question=questionv1),temperature=0)\n",
    "questionv1_compact = output['choices'][0]['message']['content']\n",
    "questionv1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af504e",
   "metadata": {},
   "source": [
    "### Add more reasoning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13508eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = \"\"\"\n",
    "Please rewrite the following question to make it require multi-step reasoning to answer using the provided context. Ensure that the question remains relevant to the context.\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "Rewritten Question Requiring Multi-Step Reasoning:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8397919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f5a690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb70ce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7vOaLdbKUBYZ4ESBkBghw7nKXSCJo at 0x7faba9766ef0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are the two major approaches to adapting pre-trained LLMs and what are the steps involved in instruction tuning?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693913557,\n",
       "  \"id\": \"chatcmpl-7vOaLdbKUBYZ4ESBkBghw7nKXSCJo\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 23,\n",
       "    \"prompt_tokens\": 1535,\n",
       "    \"total_tokens\": 1558\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c226a42",
   "metadata": {},
   "source": [
    "## Concretizing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "245ab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concretizing \n",
    "Concretize_prompt = \"\"\"\n",
    "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "You will be provided with a question and a context. \n",
    "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
    "Relate the original question to a real-life scenario and reframe the question. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
    "    3. The rewritten question should not ask for any external links. \n",
    "    4. Rewritten question should only add maximum 15 words into given question\n",
    "\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "03691f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = Concretize_prompt.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "f938f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
      "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
      "You will be provided with a question and a context. \n",
      "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
      "Relate the original question to a real-life scenario and reframe the question. \n",
      "Follow the rules given below while rewriting the question.\n",
      "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
      "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
      "    3. The rewritten question should not ask for any external links. \n",
      "    4. Rewritten question should only add maximum 15 words into given question\n",
      "\n",
      "question:\n",
      "What are the two commonly used pre-training tasks for training LLMs?\n",
      "context:\n",
      "20\n",
      "for position embeddings, RoPE or ALiBi is a better choice\n",
      "since it performs better on long sequences.\n",
      "4.2.3 Pre-training Tasks\n",
      "Pre-training plays a key role that encodes general knowl-\n",
      "edge from large-scale corpus into the massive model param-\n",
      "eters. For training LLMs, there are two commonly used pre-\n",
      "training tasks, namely language modeling and denoising\n",
      "autoencoding.\n",
      "Language Modeling. The language modeling task (LM) is\n",
      "the most commonly used objective to pre-train decoder-only\n",
      "LLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of\n",
      "tokens x={x1, . . . , x n}, the LM task aims to autoregres-\n",
      "sively predict the target tokens xibased on the preceding\n",
      "tokens x<iin a sequence. A general training objective is to\n",
      "maximize the following likelihood:\n",
      "LLM(x) =nX\n",
      "i=1logP(xi|x<i). (4)\n",
      "Since most language tasks can be cast as the prediction\n",
      "problem based on the input, these decoder-only LLMs might\n",
      "be potentially advantageous to implicitly learn how to ac-\n",
      "complish these tasks in a unified LM way. Some studies\n",
      "have also revealed that decoder-only LLMs can be naturally\n",
      "transferred to certain tasks by autoregressively predicting\n",
      "the next tokens [26, 55], without fine-tuning. An important\n",
      "variant of LM is the prefix language modeling task, which is\n",
      "designed for pre-training models with the prefix decoder\n",
      "architecture. The tokens within a randomly selected prefix\n",
      "would not be used in computing the loss of prefix language\n",
      "modeling. With the same amount of tokens seen during pre-\n",
      "training, prefix language modeling performs slightly worse\n",
      "than language modeling, since fewer tokens in the sequence\n",
      "are involved for model pre-training [29].\n",
      "Denoising Autoencoding. In addition to conventional\n",
      "LM, the denoising autoencoding task (DAE) has also been\n",
      "widely used to pre-train language models [24, 73]. The\n",
      "inputs x\\˜xfor DAE task are corrupted text with randomly\n",
      "replaced spans. Then, the language models are trained to re-\n",
      "cover the replaced tokens ˜x. Formally, the training objective\n",
      "of DAE is denoted as follows:\n",
      "LDAE(x) = log P(˜x|x\\˜x). (5)\n",
      "However, the DAE task seems to be more complicated\n",
      "in implementation than LM task. As a result, it has not\n",
      "been widely used to pre-train large language models. Exist-\n",
      "ing LLMs that take DAE as pre-training objectives include\n",
      "T5 [73] and GLM-130B [83]. These models are mainly trained\n",
      "to recover the replaced spans in an autoregressive way.\n",
      "Mixture-of-Denoisers. Mixture-of-Denoisers (MoD) [80],\n",
      "also known as UL2 loss, was introduced as a unified ob-\n",
      "jective for pre-training language models. MoD regards both\n",
      "LM and DAE objectives as different types of denoising tasks,\n",
      "namely S-denoiser (LM), R-denoiser (DAE, short span and\n",
      "low corruption), and X-denoiser (DAE, long span or high\n",
      "corruption). Among the three denoising tasks, S-denoiser\n",
      "is similar to the conventional LM objective (Equation (4)),\n",
      "while R-denoiser and X-denoiser are similar to DAE ob-\n",
      "jectives (Equation (5)) but differ from each other in thelengths of spans and ratio of corrupted text. For input sen-\n",
      "tences started with different special tokens ( i.e.,{[R],[S],\n",
      "[X]}), the model will be optimized using the corresponding\n",
      "denoisers. MoD has been applied in the latest PaLM 2\n",
      "model [216].\n",
      "4.2.4 Summary and Discussion\n",
      "The choice of architecture and pre-training tasks may incur\n",
      "different inductive biases for LLMs, which would lead to\n",
      "different model capacities. In this part, we discuss on several\n",
      "two open issues about LLM architecture.\n",
      "Architecture Choice . In earlier literature of pre-trained lan-\n",
      "guage models, there are lots of discussions on the effects\n",
      "of different architectures [29, 80]. However, most LLMs are\n",
      "developed based on the causal decoder architecture, and\n",
      "there still lacks a theoretical analysis on its advantage over\n",
      "the other alternatives. Next, we briefly summarize existing\n",
      "discussions on this issue.\n",
      "•By pre-training with the LM objective, it seems that\n",
      "causal decoder architecture can achieve a superior zero-\n",
      "shot and few-shot generalization capacity. Existing research\n",
      "has shown that without multi-task fine-tuning, the causal\n",
      "decoder has better zero-shot performance than other archi-\n",
      "tectures [29]. The success of GPT-3 [55] has demonstrates\n",
      "that the large causal decoder model can be a good few-\n",
      "shot learner. In addition, instruction tuning and alignment\n",
      "tuning discussed in Section 5 have been proven to fur-\n",
      "ther enhance the capability of large causal decoder mod-\n",
      "els [61, 62, 64].\n",
      "•Scaling law has been widely observed in causal de-\n",
      "coders. By scaling the model size, the dataset size, and\n",
      "the total computation, the performance of causal decoders\n",
      "can be substantially improved [30, 55]. Thus, it has become\n",
      "an important strategy to increase the model capacity of\n",
      "the causal decoder via scaling. However, more detailed\n",
      "investigation on encoder-decoder models is still lacking, and\n",
      "more efforts are needed to investigate the performance of\n",
      "encoder-decoder models at a large scale.\n",
      "More research efforts about the discussions on archi-\n",
      "tectures and pre-training objectives are in need to analyze\n",
      "how the choices of the architecture and pre-training tasks\n",
      "affect the capacity of LLMs, especially for encoder-decoder\n",
      "architectures. Besides the major architecture, the detailed\n",
      "configuration of LLM is also worth attention, which has\n",
      "been discussed in Section 4.2.2.\n",
      "Long Context . One of the main drawbacks of Transformer-\n",
      "based language models is the context length is limited due\n",
      "to the involved quadratic computational costs in both time\n",
      "and memory. Meanwhile, there is an increasing demand\n",
      "for LLM applications with long context windows, such as\n",
      "in PDF processing and story writing [217]. ChatGPT has\n",
      "recently released an updated variant with a context window\n",
      "size of up to 16K tokens, which is much longer than the\n",
      "initial one, i.e.,4K tokens. Additionally, GPT-4 was launched\n",
      "with variants with context window of 32K tokens [46]. Next,\n",
      "we discuss two important factors that support long context\n",
      "modeling for LLMs.\n",
      "•Extrapolation. In real-world applications, it is possible\n",
      "that LLMs need to process long input text that exceeds the\n",
      "maximum length of the training corpus. The ability of LLMs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "bd05de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "83b0a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs?'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "5083c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_c = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "1dd38446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs in a real-life scenario?\\n'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "4c2ff172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C at 0x7fe9d180cf40> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The specific sources of data used for pre-training Language Models (LLMs) such as GPT-3 and GPT-NeoX include webpages, conversation data, books and news, scientific data, and code.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693402013,\n",
       "  \"id\": \"chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 44,\n",
       "    \"prompt_tokens\": 1263,\n",
       "    \"total_tokens\": 1307\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(answer_formulate.format(question=question_c,context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0ee2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context. Ensure that the question can be answered entirely from the information present in the context. Do not frame questions that contains more than 15 words.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Detailed Description and Rules:\n",
    "1. Understand the Context: Read the provided context carefully and understand the information it contains. Identify key pieces of information that are scattered or indirectly related to each othe\n",
    "2. Identify Multiple Steps: Think about how you can construct a question that requires the reader to connect pieces of information from different parts of the context. This may involve drawing connections between various facts or concepts.\n",
    "3. Formulate a Multi-hop Question: Craft a question that necessitates the reader to make multiple logical connections or inferences based on the information provided in the context. Here are some strategies to create multi-hop questions:\n",
    "\n",
    "   - Bridge related entities: Identify information that relates specific entities and frame question that can be answered only by analysing information of both entities.\n",
    "   \n",
    "   - Use Pronouns: identify (he, she, it, they) that refer to same entity or concepts in the context, and ask questions that would require the reader to figure out what pronouns refer to.\n",
    "\n",
    "   - Refer to Specific Details: Mention specific details or facts from different parts of the context and ask how they are related.\n",
    "\n",
    "   - Pose Hypothetical Scenarios: Present a hypothetical situation or scenario that requires combining different elements from the context to arrive at an answer.\n",
    "\n",
    "   - Ask About Cause and Effect: Inquire about the causes or effects of specific events or actions mentioned in the context, which may involve reasoning through multiple steps.\n",
    "\n",
    "4. Ensure Clarity: Make sure the question is clear and unambiguous. It should be evident to the reader that they need to connect multiple pieces of information to answer the question.\n",
    "\n",
    "Multi-hop Reasoning Question:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3200248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(test_prompt.format(context=context))\n",
    "question_r = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc8b2241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_r.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ab87ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which closed-source model performs the best on complex tasks such as GSM8k and HumanEval, and how does its performance compare to other closed-source models?'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e9b58f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "datasets, namely LAMBADA [183] (language modeling),\n",
      "WMT’22 [373] (machine translation), XSum [377] (text sum-\n",
      "marization), and HumanEval [89] (code synthesis) for eval-\n",
      "uation. In WMT’22, we construct a new evaluation set\n",
      "by selecting 1000 examples for each language pair from\n",
      "the original large-scale test set to examine the average\n",
      "performance of LLMs in machine translation. We evaluate\n",
      "the zero-shot performance of LLMs on these datasets, and\n",
      "compute the accuracy of predicting words for LAMBADA,\n",
      "BLEU-4 for WMT’22, ROUGE-L for XSum, and pass@ 10for\n",
      "HumanEval.\n",
      "•Knowledge utilization. To evaluate the ability of knowl-\n",
      "edge utilization, we select four question answering datasets\n",
      "(i.e., TriviaQA [387], Natural Questions [383], Web Ques-\n",
      "tions [386], and ARC [384]), and a fact extraction dataset,\n",
      "WikiFact [400]. We also report the zero-shot performance of\n",
      "LLMs on these datasets, and compute accuracy for ARC and\n",
      "exact match for other datasets.\n",
      "•Complex reasoning. For complex reasoning, we eval-\n",
      "uate the comparison models on OpenbookQA [395], Hel-\n",
      "laSwag [412], and SocialIQA [411] for knowledge reason-\n",
      "ing; Colored Objects [265] and Penguins in the Table [265]\n",
      "for symbolic reasoning; GSM8k [422] and MATH [264] for\n",
      "mathematical reasoning. We compute the accuracy for Open-\n",
      "bookQA, HellaSwag, and SocialIQA; solve rate for Colored\n",
      "Objects and Penguins in the Table; and accuracy for GSM8k\n",
      "and MATH. For knowledge reasoning tasks, we evaluate\n",
      "the zero-shot performance, since they are all QA tasks that\n",
      "can be solved in a zero-shot setting. For complex symbolic\n",
      "reasoning and mathematical reasoning tasks, we leverage\n",
      "3-shot in-context exemplars to better elicit LLMs to accom-\n",
      "plish them. Following existing work [33, 346], we also utilize\n",
      "the chain-of-thought prompting strategy for better solving\n",
      "the mathematical reasoning tasks.\n",
      "•Human alignment. For human alignment, we select\n",
      "TruthfulQA [385] to measure whether a LLM is truth-\n",
      "ful in generating answers to questions, CrowS-Pairs [504]\n",
      "and WinoGender [505] to assess the stereotypes in LLMs,\n",
      "RealToxityPrompts [506] to evaluate the extent to which\n",
      "LLMs generate toxic language, and HaluEval [471] to test\n",
      "the ability of LLMs to recognize hallucination. As the test\n",
      "set of Real-Toxicity-Prompts is too large, we randomly\n",
      "sample 10000 examples from it for evaluation. We fol-\n",
      "low LLaMA [57] to report the zero-shot performance, and\n",
      "compute the accuracy of identifying a claim as true for\n",
      "TruthfulQA, accuracy of recognizing biased sentences (high\n",
      "perplexity) for CrowS-Pairs, coreference resolution accuracy\n",
      "(he/she/they) for WinoGender, toxicity score for RealToxi-\n",
      "tyPrompts, and average accuracy of recognizing hallucina-\n",
      "tions for HaluEval. For TruthfulQA, we follow existing\n",
      "work [57] that utilizes text-davinci-003 to replace humans\n",
      "for scoring. For Crows-Pairs and WinoGender, we follow\n",
      "the experimental settings of LLaMA [57] to compute the\n",
      "perplexity and coreference resolution score. For RealTox-\n",
      "ityPrompts, we utilize the Perspective-API45for toxicity\n",
      "evaluation.\n",
      "•Interaction with environment. To test this ability, we\n",
      "select ALFWorld [510] and WebShop [511] for evaluation,\n",
      "which simulate real-world scenarios such as household\n",
      "45. https://perspectiveapi.com/and e-commerce environments. We follow the setting of\n",
      "ReAct [361] that evaluate the 1-shot and 2-shot performance\n",
      "of LLMs on WebShop and ALFWorld respectively, and com-\n",
      "pute success rate for ALFWorld and average score/success rate\n",
      "for WebShop. Further, we also follow ReAct [361] to reduce\n",
      "the length of the input prompt and utilize line break as the\n",
      "EOS token.\n",
      "•Tool manipulation. For tool manipulation, we consider\n",
      "two kinds of tools including search engine and model in-\n",
      "terfaces. Therefore, we adopt two tool manipulation bench-\n",
      "marks, i.e., HotpotQA [409] and Gorilla [518]. HotpotQA\n",
      "requires LLMs to use search engine to retrieve documents\n",
      "from the web, and Gorilla to invoke model APIs from\n",
      "three hubs of TorchHub, TensorHub and HuggingFace. We\n",
      "compute exact match for HotpotQA and accuracy for Gorilla.\n",
      "For HotpotQA, we follow ReAct [361] to report the 3-shot\n",
      "performance. For Gorilla, we follow the code released by its\n",
      "paper [518], and evaluate the zero-shot performance.\n",
      "Implementation Details. For each task and dataset, we\n",
      "evaluate the compared LLMs using the same prompts and\n",
      "results parsing method provided by existing work ( i.e.,\n",
      "TruthfulQA, HotPotQA, Gorilla, HaluEval) or designed ac-\n",
      "cording to our empirical experience ( i.e., TriviaQA, Nat-\n",
      "ural Questions, Web Questions, ARC, WikiFact, GSM8k,\n",
      "MATH, C-Objects, Penguins, LAMBADA, WMT’22, XSum,\n",
      "HumanEval, CrowS-Pairs, WinoGender, RealToxityPrompt).\n",
      "Specifically, all the experiments about closed-source models\n",
      "are based on invoking their official APIs, while for open-\n",
      "source models, we utilize their publicly available code and\n",
      "model parameters, and perform the inference on 8 A800-\n",
      "80G GPUs. For TriviaQA, OpenbookQA, HellaSwag, and\n",
      "SocialIQA, we experiment on the development set since the\n",
      "test set is not publicly released. While for other datasets,\n",
      "we experiment on the test set. To reproduce our experi-\n",
      "ments, we also publicly release our experimental code and\n",
      "data in https://github.com/RUCAIBox/LLMSurvey/tree/\n",
      "main/Experiments.\n",
      "7.3.3 Results Analysis and Findings\n",
      "We report the experimental results in Table 11, and analyze\n",
      "the results in the following.\n",
      "Analysis of Closed-Source Models. We summarize our\n",
      "analysis and findings of the four closed-source models ( i.e.,\n",
      "ChatGPT, Claude, Davinci003 and Davinci002) as follows:\n",
      "•These four closed-source models achieve promising results\n",
      "as general-purpose task solvers, in which ChatGPT mostly per-\n",
      "forms the best. ChatGPT, Claude, Davinci003 and Davinci002\n",
      "perform well in most of tasks, including complex tasks ( e.g.,\n",
      "GSM8k), which have shown great potential to be general-\n",
      "purpose task solvers. Among them, ChatGPT exhibits a\n",
      "more superior model capacity on the evaluation tasks,\n",
      "winning the best on ten tasks. In some evaluation tasks,\n",
      "the performance gap between ChatGPT and other closed-\n",
      "source models is very large, especially for complex tasks\n",
      "e.g., 76.50 (ChatGPT) v.s.49.73 (Davinci002) on GSM8k, and\n",
      "79.88 (ChatGPT) v.s.51.22 (Claude) on HumanEval.\n",
      "•ChatGPT and Davinci003 perform better on interac-\n",
      "tion with environment and tool manipulation tasks. On the\n",
      "two evaluation tasks, two OpenAI models, ChatGPT and\n",
      "Davinci003, perform better than other models by a large\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2094fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
