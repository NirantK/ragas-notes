{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81dc5",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "* Input set of documents\n",
    "    - number of samples to generate\n",
    "    - maximum token length (input + context + answer)\n",
    "    - difficulty distribution \n",
    "    - Seed questions? \n",
    "    - randomize answer output formats\n",
    "* Output : test set with questions,contexts,answer\n",
    "\n",
    "1. Select document and part of document to frame question from (random)\n",
    "2. Formulate a question,context pair contrained on output format. \n",
    "3. Identify difficulty type \n",
    "4. Evol question using evol-instruct like paradigm to improve difficulty \n",
    "    - ask for improved reasoning - (Evol instruct C Increased Reasoning Steps Prompt)\n",
    "    - Add more contexts and frame question for multi hop\n",
    "        - Adding more context\n",
    "           1. Identiy entity from current context and retrive paras with same entity : formulate new question\n",
    "           2. Identify similar paras using sentence similarity : formulate new question\n",
    "           \n",
    "           \n",
    "#### Ideas to increase complextity\n",
    "\n",
    "- Extend question by using info in the newly added contexts.\n",
    "\n",
    "\n",
    "- Concretizing\n",
    "\n",
    "    In RAG case,this should yeild an instruction that is related to real world use-case on concept decribed in the context. \n",
    "    For example, if context speaks about different types of instructions and their use-cases\n",
    "    question will be like \"I want to train a chatbot, how to form training dataset?\"\n",
    "\n",
    "\n",
    "- Improved reasoning\n",
    "           \n",
    "- Reasoning over multiple contexts\n",
    "    - add context1, context2, context3, etc untill max tokens - x. And then ask model to formulate a question that would require reasoning over multiple contexts. \n",
    "    \n",
    "#### Open-issues\n",
    "1. How to ensure inter-document dependancy is null\n",
    "\n",
    "A question framed might have possible answers from different documents\n",
    "\n",
    "2. Questions framed by LLM are easily answerable. \n",
    "\n",
    "It feels like LLM first identifies a candidate sentence and frames question based on it. So by default the answer to questions can be located easily.\n",
    "\n",
    "3. GPT 3.5 tends to add or increase length of question when asked to create difficult question\n",
    "\n",
    "often this ends up as a unsually long question that contains two questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e9980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f91ccf9e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b4a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = json.load(open(\"/Users/shahules/openai-key.json\"))[\"ikka\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0dcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d67a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "Embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcbd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(question, generated_questions):\n",
    "        question_vec = np.asarray(Embedding.embed_query(question)).reshape(1, -1)\n",
    "        gen_question_vec = np.asarray(\n",
    "            Embedding.embed_documents(generated_questions)\n",
    "        )\n",
    "        norm = np.linalg.norm(gen_question_vec, axis=1) * np.linalg.norm(\n",
    "            question_vec, axis=1\n",
    "        )\n",
    "        return (\n",
    "            np.dot(gen_question_vec, question_vec.T).reshape(\n",
    "                -1,\n",
    "            )\n",
    "            / norm\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e91820",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb673dd",
   "metadata": {},
   "source": [
    "- random select document \n",
    "- Identify sections from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90602f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKLIST = [\"Based on the provided context\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69052348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31fe1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45b8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_doc(\"../arxiv-llm/textdata/2303.18223v11.A_Survey_of_Large_Language_Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a236b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^(?:\\d+\\.\\d+\\s+)?[A-Z][A-Z-\\s]+$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d1744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374f4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21537c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0:155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2403987",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_formulate = \"\"\"\n",
    "Your task is to formulate a question from given context satifying the rules given below:\n",
    "    1.The question should be fully answered from the given context. \n",
    "    2.The question should be framed from a part that contains non-trivial information. \n",
    "    3.The answer should to the question must be in {answer_format} format. \n",
    "    4.The answer should not contain any links. \n",
    "    5.The question should be of {difficulty} difficulty.\n",
    "    6.The question must be reasonable and must be understood and responded by humans.\n",
    "{context}:context\n",
    "\"\"\"\n",
    "\n",
    "context_formulate =  \"\"\"\\\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract sentences from given context that can be used to answer the question. \n",
    "Rules to follow while doing this task:\n",
    "    1. Your task is not to answer the question using given context but to extract sentences from given context that can be used to answer given question.\n",
    "    2. The sentences could be anywhere in the provided context, pay attention to the whole context.\n",
    "    3. While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "    4. If the answer is not present in context, you should only return the empty string.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\" \n",
    "\n",
    "context_formulatev1 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information.\"\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "context_formulatev2 = \"\"\"\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "\n",
    "\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "candidate sentences:\\n\n",
    "\"\"\"\n",
    "\n",
    "answer_formulate = \"\"\"\n",
    "Asnwer the question using the information from the qiven context. \n",
    "You are not allowed to include information that cannot be deducted from the given context.\n",
    "question:{question}\n",
    "context:{context}\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "context_from_answer = \"\"\"\n",
    "Given question, context and answer. Locate the relevant information in the context from context that was to used to form the given answer. \n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "answer:\\n{answer}\n",
    "extracted context:\"\"\"\n",
    "\n",
    "\n",
    "# answer_index_formulate = \"\"\"\n",
    "# Locate the relevant information in the context and provide the start and end indices of the text that can be used to answer the question. Keep in mind that the relevant information might be surrounded by other unrelated text. \n",
    "# You can identify the relevant portion using any relevant keywords, phrases, or patterns present in the context.\n",
    "# \\n\\n\n",
    "# question:\\nWhen was Einstein born?\n",
    "# context:\\nAlbert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛʁt ˈʔaɪnʃtaɪn] (listen); 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "# answer: the answer to the given question can be located between character index 0 and 155 of given context.  \n",
    "# question:\\n{question}\n",
    "# context:\\n{context}\n",
    "# answer:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3435f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = text.split('\\n\\n')[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c07bc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = question_formulate.format(answer_format=\"text\",difficulty=\"medium\",context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6280e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41845c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d0d804b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880a66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"\"\"\n",
    "# Adolf Hitler was an Austrian-born German politician who was the dictator of Germany from 1933 until his suicide in 1945. He rose to power as the leader of the Nazi Party, becoming the chancellor in 1933 and then taking the title of Führer und Reichskanzler in 1934\n",
    "# \"\"\"\n",
    "# question = \"when was hitler born?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "921580d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = context_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e211024",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af8f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_context = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e7d6e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23\\noverhead, and the third solution increases about 50% com-\\nmunication overhead but saves memory proportional to\\nthe number of GPUs. PyTorch has implemented a similar\\ntechnique as ZeRO, called FSDP [237].\\nMixed Precision Training. In previous PLMs ( e.g.,\\nBERT [23]), 32-bit floating-point numbers, also known as\\nFP32, have been predominantly used for pre-training. In\\nrecent years, to pre-train extremely large language models,\\nsome studies [232] have started to utilize 16-bit floating-\\npoint numbers (FP16), which reduces memory usage and\\ncommunication overhead. Additionally, as popular NVIDIA\\nGPUs ( e.g., A100) have twice the amount of FP16 computa-\\ntion units as FP32, the computational efficiency of FP16 can\\nbe further improved. However, existing work has found that\\nFP16 may lead to the loss of computational accuracy [59, 69],\\nwhich affects the final model performance. To alleviate it, an\\nalternative called Brain Floating Point (BF16) has been used\\nfor training, which allocates more exponent bits and fewer\\nsignificant bits than FP16. For pre-training, BF16 generally\\nperforms better than FP16 on representation accuracy [69].\\nOverall Training Suggestion. In practice, the above train-\\ning techniques, especially 3D parallelism, are often jointly\\nused to improve the training throughput and large model\\nloading. For instance, researchers have incorporated 8-way\\ndata parallelism, 4-way tensor parallelism, and 12-way\\npipeline parallelism, enabling the training of BLOOM [69]\\non 384 A100 GPUs. Currently, open-source libraries like\\nDeepSpeed [65], Colossal-AI [149], and Alpa [238] can well\\nsupport the three parallel training methods. To reduce the\\nmemory redundancy, ZeRO, FSDP , and activation recom-\\nputation techniques [68, 239] can be also employed for\\ntraining LLMs, which have already been integrated into\\nDeepSpeed, PyTorch, and Megatron-LM. In addition, the\\nmixed precision training technique such as BF16 can be\\nalso leveraged to improve the training efficiency and reduce\\nGPU memory usage, while it requires necessary support on\\nhardware ( e.g., A100 GPU). Because training large models is\\na time-intensive process, it would be useful to forecast the\\nmodel performance and detect abnormal issues at an early\\nstage. For this purpose, GPT-4 [46] has recently introduced\\na new mechanism called predictable scaling built on a deep\\nlearning stack, enabling the performance prediction of large\\nmodels with a much smaller model, which might be quite\\nuseful for developing LLMs. In practice, one can further\\nleverage the supporting training techniques of mainstream\\ndeep learning frameworks. For instance, PyTorch supports\\nthe data parallel training algorithm FSDP [237] ( i.e.,fully\\nsharded data parallel), which allows for partial offloading\\nof training computations to CPUs if desired.\\n5 A DAPTATION OF LLM S\\nAfter pre-training, LLMs can acquire the general abilities\\nfor solving various tasks. However, an increasing number\\nof studies have shown that LLM’s abilities can be further\\nadapted according to specific goals. In this section, we\\nintroduce two major approaches to adapting pre-trained\\nLLMs, namely instruction tuning and alignment tuning. The\\nformer approach mainly aims to enhance (or unlock) the\\nabilities of LLMs, while the latter approach aims to align thebehaviors of LLMs with human values or preferences. Fur-\\nther, we will also discuss efficient tuning and quantization\\nfor model adaptation in resource-limited settings. In what\\nfollows, we will introduce the four parts in detail.\\nTABLE 6: A detailed list of available collections for instruc-\\ntion tuning.\\nCategories Collections Time #Examples\\nTaskNat. Inst. [240] Apr-2021 193K\\nFLAN [62] Sep-2021 4.4M\\nP3 [241] Oct-2021 12.1M\\nSuper Nat. Inst. [79] Apr-2022 5M\\nMVPCorpus [242] Jun-2022 41M\\nxP3 [84] Nov-2022 81M\\nOIG22Mar-2023 43M\\nChatHH-RLHF [243] Apr-2022 160K\\nHC3 [244] Jan-2023 87K\\nShareGPT23Mar-2023 90K\\nDolly24Apr-2023 15K\\nOpenAssistant [245] Apr-2023 161K\\nSyntheticSelf-Instruct [125] Dec-2022 82K\\nAlpaca [119] Mar-2023 52K\\nGuanaco25Mar-2023 535K\\nBaize [246] Apr-2023 158K\\nBELLE [247] Apr-2023 1.5M\\n5.1 Instruction Tuning\\nIn essence, instruction tuning is the approach to fine-tuning\\npre-trained LLMs on a collection of formatted instances in\\nthe form of natural language [62], which is highly related\\nto supervised fine-tuning [61] and multi-task prompted\\ntraining [28]. In order to perform instruction tuning, we first\\nneed to collect or construct instruction-formatted instances.\\nThen, we employ these formatted instances to fine-tune\\nLLMs in a supervised learning way ( e.g., training with the\\nsequence-to-sequence loss). After instruction tuning, LLMs\\ncan demonstrate superior abilities to generalize to unseen\\ntasks [28, 62, 64], even in a multilingual setting [84].\\nA recent survey [248] presents a systematic overview\\nof the research on instruction tuning. In comparison to\\nthat, we mainly focus on the effect of instruction tuning\\non LLMs and provide detailed guidelines or strategies for\\ninstance collection and tuning. In addition, we also discuss\\nthe use of instruction tuning for satisfying the real needs of\\nusers, which has been widely applied in existing LLMs, e.g.,\\nInstructGPT [61] and GPT-4 [46].\\n5.1.1 Formatted Instance Construction\\nGenerally, an instruction-formatted instance consists of a\\ntask description (called an instruction ), an optional input,\\nthe corresponding output, and a small number of demon-\\nstrations (optional). As important public resources, exist-\\ning studies have released a large number of labeled data\\nformatted in natural language (see the list of available re-\\nsources in Table 6). Next, we introduce three major methods\\nfor constructing formatted instances (see an illustration in\\n22. https://laion.ai/blog/oig-dataset/\\n23. https://sharegpt.com/\\n24. https://github.com/databrickslabs/dolly\\n25. https://huggingface.co/datasets/JosephusCheung/GuanacoDataset'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dfd6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7503b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = answer_formulate.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "889d4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cada2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = output['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "303fbf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The two major approaches to adapting pre-trained LLMs are instruction tuning and alignment tuning.'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11097aa3",
   "metadata": {},
   "source": [
    "#### Extract more context for multi-hop questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b16b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=23\n",
    "all_contexts = text.split('\\n\\n')[index:index+20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad921f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = calculate_similarity(extracted_context,all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d379103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84381185, 0.77465926, 0.88152245, 0.83331187, 0.82022018,\n",
       "       0.86879151, 0.85556014, 0.84220169, 0.81447715, 0.83058065,\n",
       "       0.80893047, 0.82643535, 0.78249331, 0.85134229, 0.82774607,\n",
       "       0.81884942, 0.84903393, 0.84885221, 0.82688037, 0.84365871])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd407200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  5,  6, 13, 16, 17,  0, 19,  7,  3,  9, 14, 18, 11,  4, 15,  8,\n",
       "       10, 12,  1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9f3e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "mance of the model. Here, we discuss some essential factors\n",
      "for instance construction.\n",
      "•Scaling the instructions. It has been widely shown that\n",
      "scaling the number of tasks can largely enhance the general-\n",
      "ization ability of LLMs [28, 62, 79]. With the increasing of the\n",
      "task number, the model performance initially shows a con-\n",
      "tinuous growth pattern, while the gain becomes negligible\n",
      "when it reaches a certain level [64, 79]. A plausible specula-\n",
      "tion is that a certain number of representative tasks can pro-\n",
      "vide relatively sufficient knowledge and adding more tasks\n",
      "may not bring additional gains [64]. Also, it is beneficial to\n",
      "enhance the diversity of the task descriptions in several as-\n",
      "pects, such as length, structure, and creativity [28]. As for the\n",
      "number of instances per task, it has been found that a small\n",
      "number of instances can usually saturate the generalization\n",
      "performance of the model [62, 64]. Whereas, increasing the\n",
      "number of instances for some tasks to a large number ( e.g.,\n",
      "a few hundreds) could potentially result in the overfitting\n",
      "issue and impair the model performance [79, 252].\n",
      "•Formatting design. As an important factor, the design\n",
      "of natural language format also highly impacts the gener-\n",
      "alization performance of LLMs [79]. Typically, we can add\n",
      "task descriptions and optional demonstrations to the input-\n",
      "output pairs of existing datasets, where the task description\n",
      "is the most key part for LLMs to understand the task [79].\n",
      "Further, it can lead to substantial improvements by using an\n",
      "appropriate number of exemplars as demonstrations [64],\n",
      "which also alleviates the model sensitivity to instruction\n",
      "engineering [62, 64]. However, incorporating other compo-\n",
      "nents ( e.g., things to avoid, reasons, and suggestions) into\n",
      "instructions may have a negligible or even adverse effect\n",
      "on the performance of LLMs [79, 240]. Recently, to elicit\n",
      "the step-by-step reasoning ability of LLMs, some work [64]\n",
      "proposes to include chain-of-thought (CoT) examples for\n",
      "some reasoning datasets, such as arithmetic reasoning. It\n",
      "has been shown that fine-tuning LLMs with both CoT and\n",
      "non-CoT examples can lead to a good performance across\n",
      "various reasoning tasks, including those that require multi-\n",
      "hop reasoning ability ( e.g., commonsense question answer-\n",
      "ing and arithmetic reasoning) as well as those without the\n",
      "need for such a reasoning way ( e.g., sentiment analysis and\n",
      "extractive question answering) [64, 85].\n",
      "To summarize, it seems that the diversity and quality\n",
      "of instructions is more important than the number of in-\n",
      "stances [253] since the well-performing InstructGPT [61] and\n",
      "Alpaca [124] utilize fewer but more diverse instructions (or\n",
      "instances) than the Flan-series LLMs [62, 64]. Further, it is\n",
      "more useful to invite labelers to compose human-need tasks\n",
      "than using dataset-specific tasks. However, it still lacks gen-\n",
      "eral guidelines to annotate human-need instances, making\n",
      "the task composition somehow heuristic. To reduce human\n",
      "efforts, we can either reuse existing formatted datasets\n",
      "(Table 6) or automatically construct the instructions using\n",
      "existing LLMs [125]. We conduct a preliminary experiment\n",
      "to show the effectiveness of different construction methods\n",
      "in Section 5.1.4.\n",
      "5.1.2 Instruction Tuning Strategies\n",
      "Unlike pre-training, instruction tuning is often more effi-\n",
      "cient since only a moderate number of instances are used\n",
      "for training. Since instruction tuning can be considered asa supervised training process, its optimization is different\n",
      "from pre-training in several aspects [64], such as the training\n",
      "objective ( i.e.,sequence-to-sequence loss) and optimization\n",
      "configuration ( e.g., smaller batch size and learning rate),\n",
      "which require special attention in practice. In addition to\n",
      "these optimization configurations, there are also two impor-\n",
      "tant aspects to consider for instruction tuning:\n",
      "Balancing the Data Distribution. Since instruction tun-\n",
      "ing involves a mixture of different tasks, it is important\n",
      "to balance the proportion of different tasks during fine-\n",
      "tuning. A widely used method is the examples-proportional\n",
      "mixing strategy [73], i.e., combining all the datasets and\n",
      "sampling each instance equally from the mixed datasets.\n",
      "Furthermore, increasing the sampling ratio of high-quality\n",
      "collections ( e.g., FLAN [62] and P3 [241]) can generally\n",
      "lead to performance improvement according to recent find-\n",
      "ings [64, 85]. Further, it is common to set a maximum cap\n",
      "to control the maximum number of examples that a dataset\n",
      "can contain during instruction tuning [73], which is set to\n",
      "prevent larger datasets from overwhelming the entire dis-\n",
      "tribution [73, 85]. In practice, the maximum cap is typically\n",
      "set to several thousands or tens of thousands according to\n",
      "different datasets [62, 64].\n",
      "Combining Instruction T uning and Pre-Training. To make\n",
      "the tuning process more effective and stable, OPT-IML [85]\n",
      "incorporates pre-training data during instruction tuning,\n",
      "which can be regarded as regularization for model tuning.\n",
      "Further, instead of using a separate two-stage process ( pre-\n",
      "training then instruction tuning ), some studies attempt to\n",
      "train a model from scratch with a mixture of pre-training\n",
      "data ( i.e.,plain texts) and instruction tuning data ( i.e.,for-\n",
      "matted datasets) using multi-task learning [73]. Specifically,\n",
      "GLM-130B [83] and Galactica [35] integrate instruction-\n",
      "formatted datasets as a small proportion of the pre-training\n",
      "corpora to pre-train LLMs, which potentially achieves the\n",
      "advantages of pre-training and instruction tuning at the\n",
      "same time.\n",
      "5.1.3 The Effect of Instruction Tuning\n",
      "In this part, we discuss the effect of instruction tuning on\n",
      "LLMs in three major aspects.\n",
      "Performance Improvement. Despite being tuned on a mod-\n",
      "erate number of instances, instruction tuning has become\n",
      "an important way to improve or unlock the abilities of\n",
      "LLMs [64]. Recent studies have experimented with language\n",
      "models in multiple scales (ranging from 77M to 540B),\n",
      "showing that the models of different scales can all benefit\n",
      "from instruction tuning [64, 251], yielding improved perfor-\n",
      "mance as the parameter scale increases [84]. Further, smaller\n",
      "models with instruction tuning can even perform better\n",
      "than larger models without fine-tuning [28, 64]. Besides\n",
      "the model scale, instruction tuning demonstrates consistent\n",
      "improvements in various model architectures, pre-training\n",
      "objectives, and model adaptation methods [64]. In practice,\n",
      "instruction tuning offers a general approach to enhancing\n",
      "the abilities of existing language models [64] (including\n",
      "small-sized PLMs). Also, it is much less costly than pre-\n",
      "training, since the amount of instruction data required by\n",
      "LLMs is significantly smaller than pre-training data.\n"
     ]
    }
   ],
   "source": [
    "print(all_contexts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b752c",
   "metadata": {},
   "source": [
    "### Complicating instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af24ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 1\n",
    "## includes info in context2 and adds an 'and' part to the original question\n",
    "\n",
    "multi_context = \"\"\"\n",
    "You are a prompt rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "You will be provided with a question and two set of contexts namely context1 and context2. \n",
    "Your task is to complicate the given question in a way that answering it requires information derived from both context1 and context2. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question should not be very long. \n",
    "    2. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    3. The rewritten question must be fully answerable from information present in context1 and context2. \n",
    "    4. Read and understand both contexts and rewrite the question so that answering requires insight from both context1 and context2.\n",
    "    \n",
    "question:\\n{question}\n",
    "context1:\\n{context1}\n",
    "context2:\\n{context2}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "971f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_part = \"\"\"\n",
    "Please rephrase the following multi-part question into a single, shortened and comprehensive question that encompasses all the key elements.\n",
    "question:\\n{question}\n",
    "rewritten question:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65432c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = multi_context.format(question=question,context1=extracted_context,context2=all_contexts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "240a8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionv1 = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53689adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two major approaches to adapting pre-trained LLMs, and how do factors such as scaling the instructions and formatting design impact their performance?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a95156",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(single_part.format(question=questionv1),temperature=0)\n",
    "questionv1_compact = output['choices'][0]['message']['content']\n",
    "questionv1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af504e",
   "metadata": {},
   "source": [
    "### Add more reasoning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13508eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = \"\"\"\n",
    "Please rewrite the following question to make it require multi-step reasoning to answer using the provided context. Ensure that the question remains relevant to the context.\n",
    "question:{question}\n",
    "context:\\n{context}\n",
    "Rewritten Question Requiring Multi-Step Reasoning:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8397919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = reasoning.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f5a690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb70ce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7vOaLdbKUBYZ4ESBkBghw7nKXSCJo at 0x7faba9766ef0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"What are the two major approaches to adapting pre-trained LLMs and what are the steps involved in instruction tuning?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693913557,\n",
       "  \"id\": \"chatcmpl-7vOaLdbKUBYZ4ESBkBghw7nKXSCJo\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 23,\n",
       "    \"prompt_tokens\": 1535,\n",
       "    \"total_tokens\": 1558\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c226a42",
   "metadata": {},
   "source": [
    "## Concretizing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "245ab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concretizing \n",
    "Concretize_prompt = \"\"\"\n",
    "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
    "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "You will be provided with a question and a context. \n",
    "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
    "Relate the original question to a real-life scenario and reframe the question. \n",
    "Follow the rules given below while rewriting the question.\n",
    "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
    "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
    "    3. The rewritten question should not ask for any external links. \n",
    "    4. Rewritten question should only add maximum 15 words into given question\n",
    "\n",
    "question:\\n{question}\n",
    "context:\\n{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "03691f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = Concretize_prompt.format(question=question,context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "f938f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a question rewriter. Your objective is to rewrite a given question into a more complex version to make those famous AI systems\n",
      "(e.g., ChatGPT and GPT4) a bit harder to handle.\n",
      "You will be provided with a question and a context. \n",
      "Your task is to rewrite the question. You SHOULD complicate the given question using the following method:\n",
      "Relate the original question to a real-life scenario and reframe the question. \n",
      "Follow the rules given below while rewriting the question.\n",
      "    1. The rewritten question must be reasonable and must be understood and responded by humans.\n",
      "    2. The rewritten question should be fully answerable from insights derived from the provided context. \n",
      "    3. The rewritten question should not ask for any external links. \n",
      "    4. Rewritten question should only add maximum 15 words into given question\n",
      "\n",
      "question:\n",
      "What are the two commonly used pre-training tasks for training LLMs?\n",
      "context:\n",
      "20\n",
      "for position embeddings, RoPE or ALiBi is a better choice\n",
      "since it performs better on long sequences.\n",
      "4.2.3 Pre-training Tasks\n",
      "Pre-training plays a key role that encodes general knowl-\n",
      "edge from large-scale corpus into the massive model param-\n",
      "eters. For training LLMs, there are two commonly used pre-\n",
      "training tasks, namely language modeling and denoising\n",
      "autoencoding.\n",
      "Language Modeling. The language modeling task (LM) is\n",
      "the most commonly used objective to pre-train decoder-only\n",
      "LLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of\n",
      "tokens x={x1, . . . , x n}, the LM task aims to autoregres-\n",
      "sively predict the target tokens xibased on the preceding\n",
      "tokens x<iin a sequence. A general training objective is to\n",
      "maximize the following likelihood:\n",
      "LLM(x) =nX\n",
      "i=1logP(xi|x<i). (4)\n",
      "Since most language tasks can be cast as the prediction\n",
      "problem based on the input, these decoder-only LLMs might\n",
      "be potentially advantageous to implicitly learn how to ac-\n",
      "complish these tasks in a unified LM way. Some studies\n",
      "have also revealed that decoder-only LLMs can be naturally\n",
      "transferred to certain tasks by autoregressively predicting\n",
      "the next tokens [26, 55], without fine-tuning. An important\n",
      "variant of LM is the prefix language modeling task, which is\n",
      "designed for pre-training models with the prefix decoder\n",
      "architecture. The tokens within a randomly selected prefix\n",
      "would not be used in computing the loss of prefix language\n",
      "modeling. With the same amount of tokens seen during pre-\n",
      "training, prefix language modeling performs slightly worse\n",
      "than language modeling, since fewer tokens in the sequence\n",
      "are involved for model pre-training [29].\n",
      "Denoising Autoencoding. In addition to conventional\n",
      "LM, the denoising autoencoding task (DAE) has also been\n",
      "widely used to pre-train language models [24, 73]. The\n",
      "inputs x\\˜xfor DAE task are corrupted text with randomly\n",
      "replaced spans. Then, the language models are trained to re-\n",
      "cover the replaced tokens ˜x. Formally, the training objective\n",
      "of DAE is denoted as follows:\n",
      "LDAE(x) = log P(˜x|x\\˜x). (5)\n",
      "However, the DAE task seems to be more complicated\n",
      "in implementation than LM task. As a result, it has not\n",
      "been widely used to pre-train large language models. Exist-\n",
      "ing LLMs that take DAE as pre-training objectives include\n",
      "T5 [73] and GLM-130B [83]. These models are mainly trained\n",
      "to recover the replaced spans in an autoregressive way.\n",
      "Mixture-of-Denoisers. Mixture-of-Denoisers (MoD) [80],\n",
      "also known as UL2 loss, was introduced as a unified ob-\n",
      "jective for pre-training language models. MoD regards both\n",
      "LM and DAE objectives as different types of denoising tasks,\n",
      "namely S-denoiser (LM), R-denoiser (DAE, short span and\n",
      "low corruption), and X-denoiser (DAE, long span or high\n",
      "corruption). Among the three denoising tasks, S-denoiser\n",
      "is similar to the conventional LM objective (Equation (4)),\n",
      "while R-denoiser and X-denoiser are similar to DAE ob-\n",
      "jectives (Equation (5)) but differ from each other in thelengths of spans and ratio of corrupted text. For input sen-\n",
      "tences started with different special tokens ( i.e.,{[R],[S],\n",
      "[X]}), the model will be optimized using the corresponding\n",
      "denoisers. MoD has been applied in the latest PaLM 2\n",
      "model [216].\n",
      "4.2.4 Summary and Discussion\n",
      "The choice of architecture and pre-training tasks may incur\n",
      "different inductive biases for LLMs, which would lead to\n",
      "different model capacities. In this part, we discuss on several\n",
      "two open issues about LLM architecture.\n",
      "Architecture Choice . In earlier literature of pre-trained lan-\n",
      "guage models, there are lots of discussions on the effects\n",
      "of different architectures [29, 80]. However, most LLMs are\n",
      "developed based on the causal decoder architecture, and\n",
      "there still lacks a theoretical analysis on its advantage over\n",
      "the other alternatives. Next, we briefly summarize existing\n",
      "discussions on this issue.\n",
      "•By pre-training with the LM objective, it seems that\n",
      "causal decoder architecture can achieve a superior zero-\n",
      "shot and few-shot generalization capacity. Existing research\n",
      "has shown that without multi-task fine-tuning, the causal\n",
      "decoder has better zero-shot performance than other archi-\n",
      "tectures [29]. The success of GPT-3 [55] has demonstrates\n",
      "that the large causal decoder model can be a good few-\n",
      "shot learner. In addition, instruction tuning and alignment\n",
      "tuning discussed in Section 5 have been proven to fur-\n",
      "ther enhance the capability of large causal decoder mod-\n",
      "els [61, 62, 64].\n",
      "•Scaling law has been widely observed in causal de-\n",
      "coders. By scaling the model size, the dataset size, and\n",
      "the total computation, the performance of causal decoders\n",
      "can be substantially improved [30, 55]. Thus, it has become\n",
      "an important strategy to increase the model capacity of\n",
      "the causal decoder via scaling. However, more detailed\n",
      "investigation on encoder-decoder models is still lacking, and\n",
      "more efforts are needed to investigate the performance of\n",
      "encoder-decoder models at a large scale.\n",
      "More research efforts about the discussions on archi-\n",
      "tectures and pre-training objectives are in need to analyze\n",
      "how the choices of the architecture and pre-training tasks\n",
      "affect the capacity of LLMs, especially for encoder-decoder\n",
      "architectures. Besides the major architecture, the detailed\n",
      "configuration of LLM is also worth attention, which has\n",
      "been discussed in Section 4.2.2.\n",
      "Long Context . One of the main drawbacks of Transformer-\n",
      "based language models is the context length is limited due\n",
      "to the involved quadratic computational costs in both time\n",
      "and memory. Meanwhile, there is an increasing demand\n",
      "for LLM applications with long context windows, such as\n",
      "in PDF processing and story writing [217]. ChatGPT has\n",
      "recently released an updated variant with a context window\n",
      "size of up to 16K tokens, which is much longer than the\n",
      "initial one, i.e.,4K tokens. Additionally, GPT-4 was launched\n",
      "with variants with context window of 32K tokens [46]. Next,\n",
      "we discuss two important factors that support long context\n",
      "modeling for LLMs.\n",
      "•Extrapolation. In real-world applications, it is possible\n",
      "that LLMs need to process long input text that exceeds the\n",
      "maximum length of the training corpus. The ability of LLMs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "bd05de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "83b0a6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs?'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "5083c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_c = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "1dd38446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the two commonly used pre-training tasks for training LLMs in a real-life scenario?\\n'"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "4c2ff172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C at 0x7fe9d180cf40> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"The specific sources of data used for pre-training Language Models (LLMs) such as GPT-3 and GPT-NeoX include webpages, conversation data, books and news, scientific data, and code.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1693402013,\n",
       "  \"id\": \"chatcmpl-7tFVdnu0QTYeSDqD8JTRqowNtox7C\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 44,\n",
       "    \"prompt_tokens\": 1263,\n",
       "    \"total_tokens\": 1307\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2(answer_formulate.format(question=question_c,context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ee2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"\n",
    "Create a multi-hop reasoning question based on the provided context. The question should require the reader to make multiple logical connections or inferences using the information available in the context. Ensure that the question can be answered entirely from the information present in the context. Do not frame questions that contains more than 15 words.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Detailed Description and Rules:\n",
    "1. Understand the Context: Read the provided context carefully and understand the information it contains. Identify key pieces of information that are scattered or indirectly related to each othe\n",
    "2. Identify Multiple Steps: Think about how you can construct a question that requires the reader to connect pieces of information from different parts of the context. This may involve drawing connections between various facts or concepts.\n",
    "3. Formulate a Multi-hop Question: Craft a question that necessitates the reader to make multiple logical connections or inferences based on the information provided in the context. Here are some strategies to create multi-hop questions:\n",
    "\n",
    "   - Bridge related entities: Identify information that relates specific entities and frame question that can be answered only by analysing information of both entities.\n",
    "   \n",
    "   - Use Pronouns: identify (he, she, it, they) that refer to same entity or concepts in the context, and ask questions that would require the reader to figure out what pronouns refer to.\n",
    "\n",
    "   - Refer to Specific Details: Mention specific details or facts from different parts of the context and ask how they are related.\n",
    "\n",
    "   - Pose Hypothetical Scenarios: Present a hypothetical situation or scenario that requires combining different elements from the context to arrive at an answer.\n",
    "\n",
    "   - Ask About Cause and Effect: Inquire about the causes or effects of specific events or actions mentioned in the context, which may involve reasoning through multiple steps.\n",
    "\n",
    "4. Ensure Clarity: Make sure the question is clear and unambiguous. It should be evident to the reader that they need to connect multiple pieces of information to answer the question.\n",
    "5. phrases like 'based on the provided context','according to the context?',etc are not allowed to appear in the question.\n",
    "Multi-hop Reasoning Question:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3200248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(test_prompt.format(context=context))\n",
    "question_r = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc8b2241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_r.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab87ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some challenges that need to be addressed when applying LLMs to real-world scenarios?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9b58f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "some recent work has studied the human-like characteristics\n",
      "of LLMs, such as self-awareness, theory of mind (ToM), and\n",
      "affective computing [603, 604]. In particular, an empirical\n",
      "evaluation of ToM conducted on two classic false-belief\n",
      "tasks speculates that LLMs may have ToM-like abilities\n",
      "since the model in the GPT-3.5 series achieves comparable\n",
      "performance with nine-year-old children in ToM task [603].\n",
      "In addition, another line of work has investigated applying\n",
      "LLMs into the software development domain, e.g., code\n",
      "suggestion [605], code summarization [606], and automated\n",
      "program repair [607]. To summarize, to assist humans by\n",
      "LLMs in real-world tasks has become a significant area of\n",
      "research. However, it also presents challenges. Ensuring the\n",
      "accuracy of LLM-generated content, addressing biases, and\n",
      "maintaining user privacy and data security are crucial con-\n",
      "siderations when applying LLMs to real-world scenarios.\n",
      "10 C ONCLUSION AND FUTURE DIRECTIONS\n",
      "In this survey, we have reviewed the recent progress of\n",
      "large language models (LLMs), and introduced the key\n",
      "concepts, findings, and techniques for understanding and\n",
      "utilizing LLMs. We focus on the large-sized models ( i.e.,\n",
      "having a size larger than 10B) while excluding the contents\n",
      "of early pre-trained language models ( e.g., BERT and GPT-\n",
      "2) that have been well covered in the existing literature. In\n",
      "particular, our survey has discussed four important aspects\n",
      "of LLMs, i.e., pre-training, adaptation tuning, utilization,\n",
      "and evaluation. For each aspect, we highlight the techniques\n",
      "or findings that are key to the success of LLMs. Furthermore,\n",
      "we also summarize the available resources for developing\n",
      "LLMs and discuss important implementation guidelines for\n",
      "reproducing LLMs. This survey tries to cover the most\n",
      "recent literature about LLMs and provides a good reference\n",
      "resource on this topic for both researchers and engineers.\n",
      "Next, we summarize the discussions of this survey, and\n",
      "introduce the challenges and future directions for LLMs, in\n",
      "the following aspects.\n",
      "Theory and Principle. To understand the underlying work-\n",
      "ing mechanism of LLMs, one of the greatest mysteries\n",
      "is how information is distributed, organized, and utilized\n",
      "through the very large, deep neural network. It is important\n",
      "to reveal the basic principles or elements that establish the\n",
      "foundation of the abilities of LLMs. In particular, scaling\n",
      "seems to play an important role in increasing the capacity\n",
      "of LLMs [31, 55, 59]. It has been shown that some emergent\n",
      "abilities would occur in an unexpected way (a sudden per-\n",
      "formance leap) when the parameter scale of language mod-\n",
      "els increases to a critical size ( e.g., 10B) [31, 33], typically in-\n",
      "cluding in-context learning, instruction following, and step-\n",
      "by-step reasoning. These emergent abilities are fascinating\n",
      "yet perplexing: when and how they are obtained by LLMs\n",
      "are not yet clear. Recent studies either conduct extensive\n",
      "experiments for investigating the effect of emergent abilities\n",
      "and the contributing factors to such abilities [307, 608, 609],\n",
      "or explain some specific abilities with existing theoretical\n",
      "frameworks [60, 317]. An insightful technical post also spe-\n",
      "cially discusses this topic [47], taking the GPT-series models\n",
      "as the target. However, more formal theories and principles\n",
      "to understand, characterize, and explain the abilities orbehaviors of LLMs are still missing. Since emergent abilities\n",
      "bear a close analogy to phase transitions in nature [31, 58],\n",
      "cross-discipline theories or principles ( e.g., whether LLMs\n",
      "can be considered as some kind of complex systems) might\n",
      "be useful to explain and understand the behaviors of LLMs.\n",
      "These fundamental questions are worth exploring for the\n",
      "research community, which are important for developing\n",
      "the next-generation LLMs.\n",
      "Model Architecture. Due to the scalability and effective-\n",
      "ness, Transformer, consisting of stacked multi-head self-\n",
      "attention layers, has become the de facto architecture for\n",
      "building LLMs. Various strategies have been proposed to\n",
      "improve the performance of this architecture, such as neural\n",
      "network configuration and scalable parallel training (see\n",
      "discussions in Section 4.2.2). To enhance the model capacity\n",
      "(e.g., the multi-turn conversation ability), existing LLMs\n",
      "typically maintain a long context window, e.g., GPT-4-32k\n",
      "has an extremely large context length of 32,768 tokens. Thus,\n",
      "a practical consideration is to reduce the time complexity\n",
      "(originally to be quadratic costs) incurred by the standard\n",
      "self-attention mechanism. It is important to investigate the\n",
      "effect of more efficient Transformer variants in building\n",
      "LLMs [610], e.g., sparse attention has been used in GPT-\n",
      "3 [55]. Besides, catastrophic forgetting has been a long-\n",
      "standing challenge for neural networks, which also has a\n",
      "negative impact on LLMs. When tuning LLMs with new\n",
      "data, the originally learned knowledge is likely to be dam-\n",
      "aged, e.g., fine-tuning a LLM according to some specific\n",
      "tasks will affect the general ability of LLMs. A similar case\n",
      "occurs when LLMs are aligned with human values (called\n",
      "alignment tax [61, 268]). Thus, it is necessary to consider\n",
      "extending existing architectures with more flexible mech-\n",
      "anisms or modules that can effectively support data update\n",
      "and task specialization.\n",
      "Model Training. In practice, it is very difficult to pre-\n",
      "train capable LLMs, due to the huge computation con-\n",
      "sumption and the sensitivity to data quality and training\n",
      "tricks [69, 83]. Thus, it becomes particularly important to\n",
      "develop more systemic, economical pre-training approaches\n",
      "for optimizing LLMs, considering the factors of model ef-\n",
      "fectiveness, efficiency optimization, and training stability.\n",
      "More model checking or performance diagnosis methods\n",
      "(e.g., predictable scaling in GPT-4 [46]) should be developed\n",
      "in order to detect early abnormal issues during training.\n",
      "Furthermore, it also calls for more flexible mechanisms of\n",
      "hardware support or resource schedule, so as to better\n",
      "organize and utilize the resources in a computing cluster.\n",
      "Since it is very costly to pre-train a LLM from scratch, it\n",
      "is important to design a suitable mechanisms for continu-\n",
      "ally pre-training or fine-tuning the LLM based on publicly\n",
      "available model checkpoints ( e.g., LLaMA [57] and Flan-\n",
      "T5 [64]). For this purpose, a number of technical issues\n",
      "have to be resolved, e.g., catastrophic forgetting and task\n",
      "specialization. However, to date, there still lack open-source\n",
      "model checkpoints for LLMs with complete pre-processing\n",
      "and training logs ( e.g., the scripts to prepare the pre-training\n",
      "data) for reproduction. We believe that it will be of great\n",
      "value to report more technical details in open-source models\n",
      "for the research of LLMs. Furthermore, it is also important\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2094fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
